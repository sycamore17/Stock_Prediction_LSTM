{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据切割数据集并保存\n",
    "TRAIN_WEIGHT=0.9\n",
    "SEQ_LEN=99\n",
    "LEARNING_RATE=0.00001\n",
    "BATCH_SIZE=16\n",
    "#train_size=int(TRAIN_WEIGHT*(data.shape[0]))\n",
    "train_path=\"stock_daily/stock_train.csv\"\n",
    "test_path=\"stock_daily/stock_test.csv\"\n",
    "#Train_data=data[:train_size+SEQ_LEN]\n",
    "#Test_data=data[train_size-SEQ_LEN:]\n",
    "#Train_data.to_csv(train_path,sep=',',index=False,header=False)\n",
    "#Test_data.to_csv(test_path,sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list=[]\n",
    "std_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成数据集类\n",
    "class Stock_Data(Dataset):\n",
    "    def __init__(self,train=True,transform=None):        \n",
    "        if train==True:\n",
    "            train_path=\"stock_daily/stock_train.csv\"\n",
    "            with open(train_path) as f:\n",
    "                self.data = np.loadtxt(f,delimiter = \",\")\n",
    "                #可以注释\n",
    "                #addi=np.zeros((self.data.shape[0],1))\n",
    "                #self.data=np.concatenate((self.data,addi),axis=1)\n",
    "                self.data=self.data[:,0:8]\n",
    "            for i in range(len(self.data[0])):\n",
    "                mean_list.append(np.mean(self.data[:,i]))\n",
    "                std_list.append(np.std(self.data[:,i]))\n",
    "                self.data[:,i]=(self.data[:,i]-np.mean(self.data[:,i]))/(np.std(self.data[:,i])+1e-8)\n",
    "            self.value=torch.rand(self.data.shape[0]-SEQ_LEN,SEQ_LEN,self.data.shape[1])\n",
    "            self.label=torch.rand(self.data.shape[0]-SEQ_LEN,1)\n",
    "            for i in range(self.data.shape[0]-SEQ_LEN):                  \n",
    "                self.value[i,:,:]=torch.from_numpy(self.data[i:i+SEQ_LEN,:].reshape(SEQ_LEN,self.data.shape[1]))    \n",
    "                self.label[i,:]=self.data[i+SEQ_LEN,0]\n",
    "            self.data=self.value\n",
    "        else:\n",
    "            test_path=\"stock_daily/stock_test.csv\"\n",
    "            with open(test_path) as f:\n",
    "                self.data = np.loadtxt(f,delimiter = \",\")\n",
    "                #addi=np.zeros((self.data.shape[0],1))\n",
    "                #self.data=np.concatenate((self.data,addi),axis=1)\n",
    "                self.data=self.data[:,0:8]\n",
    "            for i in range(len(self.data[0])):\n",
    "                self.data[:,i]=(self.data[:,i]-mean_list[i])/(std_list[i]+1e-8)\n",
    "            self.value=torch.rand(self.data.shape[0]-SEQ_LEN,SEQ_LEN,self.data.shape[1])\n",
    "            self.label=torch.rand(self.data.shape[0]-SEQ_LEN,1)\n",
    "            for i in range(self.data.shape[0]-SEQ_LEN):                  \n",
    "                self.value[i,:,:]=torch.from_numpy(self.data[i:i+SEQ_LEN,:].reshape(SEQ_LEN,self.data.shape[1]))    \n",
    "                self.label[i,:]=self.data[i+SEQ_LEN,0]\n",
    "            self.data=self.value\n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index],self.label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_train=Stock_Data(train=True)\n",
    "stock_test=Stock_Data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,dimension):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.lstm=nn.LSTM(input_size=dimension,hidden_size=128,num_layers=3,batch_first=True)\n",
    "        self.linear1=nn.Linear(in_features=128,out_features=16)\n",
    "        self.linear2=nn.Linear(16,1)\n",
    "    def forward(self,x):\n",
    "        out,_=self.lstm(x)\n",
    "        x=out[:,-1,:]        \n",
    "        x=self.linear1(x)\n",
    "        x=self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):    \n",
    "    model.train()\n",
    "    global loss_list\n",
    "    global iteration\n",
    "    dataloader=DataLoader(dataset=stock_train,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    for i,(data,label) in enumerate(dataloader):\n",
    "        iteration=iteration+1\n",
    "        data,label = data.to(device),label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model.forward(data)\n",
    "        loss=criterion(output,label)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        if i%20==0:\n",
    "            loss_list.append(loss.item())\n",
    "            print(\"epoch=\",epoch,\"iteration=\",iteration,\"loss=\",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    global accuracy_list\n",
    "    global predict_list\n",
    "    dataloader=DataLoader(dataset=stock_test,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    for i,(data,label) in enumerate(dataloader):\n",
    "        with torch.no_grad():            \n",
    "            data,label=data.to(device),label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predict=model.forward(data)\n",
    "            predict_list.append(predict)\n",
    "            loss=criterion(predict,label)\n",
    "            accuracy_fn=nn.MSELoss()\n",
    "            accuracy=accuracy_fn(predict,label)\n",
    "            accuracy_list.append(accuracy.item())\n",
    "    print(\"test_data MSELoss:(pred-real)/real=\",np.mean(accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_curve(loss_list):\n",
    "    x=np.linspace(1,len(loss_list),len(loss_list))\n",
    "    x=20*x\n",
    "    plt.plot(x,np.array(loss_list),label=\"train_loss\")\n",
    "    plt.ylabel(\"MSELoss\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.savefig(\"train_loss.png\",dpi=3000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_lines(predict_list):\n",
    "    real_list=[]\n",
    "    prediction_list=[]\n",
    "    dataloader=DataLoader(dataset=stock_test,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "    for i,(data,label) in enumerate(dataloader):\n",
    "        for idx in range(BATCH_SIZE):\n",
    "            real_list.append(np.array(label[idx]*std_list[0]+mean_list[0]))\n",
    "    for item in predict_list:\n",
    "        item=item.to(\"cpu\")\n",
    "        for idx in range(BATCH_SIZE):\n",
    "            prediction_list.append(np.array((item[idx]*std_list[0]+mean_list[0])))\n",
    "    x=np.linspace(1,len(real_list),len(real_list))\n",
    "    plt.plot(x,np.array(real_list),label=\"real\")\n",
    "    plt.plot(x,np.array(prediction_list),label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"600519_Pre.png\",dpi=3000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x维度实例化模型\n",
    "model=LSTM(dimension=8)\n",
    "model=model.to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 iteration= 1 loss= 1.4240838289260864\n",
      "epoch= 0 iteration= 21 loss= 0.5112948417663574\n",
      "epoch= 0 iteration= 41 loss= 0.23975816369056702\n",
      "epoch= 0 iteration= 61 loss= 0.3257552683353424\n",
      "epoch= 0 iteration= 81 loss= 1.496315836906433\n",
      "test_data MSELoss:(pred-real)/real= 1.5008195373747084\n",
      "epoch= 1 iteration= 94 loss= 1.3976850509643555\n",
      "epoch= 1 iteration= 114 loss= 0.5045064091682434\n",
      "epoch= 1 iteration= 134 loss= 0.2371928095817566\n",
      "epoch= 1 iteration= 154 loss= 0.3159105181694031\n",
      "epoch= 1 iteration= 174 loss= 1.465603232383728\n",
      "test_data MSELoss:(pred-real)/real= 1.4607597721947565\n",
      "epoch= 2 iteration= 187 loss= 1.3742033243179321\n",
      "epoch= 2 iteration= 207 loss= 0.4931754767894745\n",
      "epoch= 2 iteration= 227 loss= 0.23092830181121826\n",
      "epoch= 2 iteration= 247 loss= 0.30507445335388184\n",
      "epoch= 2 iteration= 267 loss= 1.421673059463501\n",
      "test_data MSELoss:(pred-real)/real= 1.3984482619497511\n",
      "epoch= 3 iteration= 280 loss= 1.331024408340454\n",
      "epoch= 3 iteration= 300 loss= 0.47058483958244324\n",
      "epoch= 3 iteration= 320 loss= 0.21751241385936737\n",
      "epoch= 3 iteration= 340 loss= 0.2882459759712219\n",
      "epoch= 3 iteration= 360 loss= 1.344132900238037\n",
      "test_data MSELoss:(pred-real)/real= 1.2818030714988708\n",
      "epoch= 4 iteration= 373 loss= 1.2479534149169922\n",
      "epoch= 4 iteration= 393 loss= 0.424599826335907\n",
      "epoch= 4 iteration= 413 loss= 0.18898862600326538\n",
      "epoch= 4 iteration= 433 loss= 0.25594788789749146\n",
      "epoch= 4 iteration= 453 loss= 1.180962324142456\n",
      "test_data MSELoss:(pred-real)/real= 1.0272999571429358\n",
      "epoch= 5 iteration= 466 loss= 1.0859465599060059\n",
      "epoch= 5 iteration= 486 loss= 0.33271169662475586\n",
      "epoch= 5 iteration= 506 loss= 0.13157343864440918\n",
      "epoch= 5 iteration= 526 loss= 0.1845240592956543\n",
      "epoch= 5 iteration= 546 loss= 0.8083158135414124\n",
      "test_data MSELoss:(pred-real)/real= 0.49934345814916825\n",
      "epoch= 6 iteration= 559 loss= 0.8169749975204468\n",
      "epoch= 6 iteration= 579 loss= 0.1902925670146942\n",
      "epoch= 6 iteration= 599 loss= 0.05078049749135971\n",
      "epoch= 6 iteration= 619 loss= 0.051604997366666794\n",
      "epoch= 6 iteration= 639 loss= 0.23093533515930176\n",
      "test_data MSELoss:(pred-real)/real= 0.043614411209192544\n",
      "epoch= 7 iteration= 652 loss= 0.5438253879547119\n",
      "epoch= 7 iteration= 672 loss= 0.0744088813662529\n",
      "epoch= 7 iteration= 692 loss= 0.004877091851085424\n",
      "epoch= 7 iteration= 712 loss= 0.01463999878615141\n",
      "epoch= 7 iteration= 732 loss= 0.022923389449715614\n",
      "test_data MSELoss:(pred-real)/real= 0.012142623710032139\n",
      "epoch= 8 iteration= 745 loss= 0.3623502254486084\n",
      "epoch= 8 iteration= 765 loss= 0.020808804780244827\n",
      "epoch= 8 iteration= 785 loss= 0.0027734446339309216\n",
      "epoch= 8 iteration= 805 loss= 0.05364399403333664\n",
      "epoch= 8 iteration= 825 loss= 0.015891999006271362\n",
      "test_data MSELoss:(pred-real)/real= 0.01606437134452992\n",
      "epoch= 9 iteration= 838 loss= 0.2441745549440384\n",
      "epoch= 9 iteration= 858 loss= 0.0034744429867714643\n",
      "epoch= 9 iteration= 878 loss= 0.015596538782119751\n",
      "epoch= 9 iteration= 898 loss= 0.051249630749225616\n",
      "epoch= 9 iteration= 918 loss= 0.0159057155251503\n",
      "test_data MSELoss:(pred-real)/real= 0.01573296774747885\n",
      "epoch= 10 iteration= 931 loss= 0.1790548712015152\n",
      "epoch= 10 iteration= 951 loss= 0.0029081625398248434\n",
      "epoch= 10 iteration= 971 loss= 0.025550665333867073\n",
      "epoch= 10 iteration= 991 loss= 0.045113351196050644\n",
      "epoch= 10 iteration= 1011 loss= 0.016002163290977478\n",
      "test_data MSELoss:(pred-real)/real= 0.01541937953637292\n",
      "epoch= 11 iteration= 1024 loss= 0.14790280163288116\n",
      "epoch= 11 iteration= 1044 loss= 0.005073606036603451\n",
      "epoch= 11 iteration= 1064 loss= 0.029383452609181404\n",
      "epoch= 11 iteration= 1084 loss= 0.03939128667116165\n",
      "epoch= 11 iteration= 1104 loss= 0.01607474312186241\n",
      "test_data MSELoss:(pred-real)/real= 0.015264139096770022\n",
      "epoch= 12 iteration= 1117 loss= 0.1311480551958084\n",
      "epoch= 12 iteration= 1137 loss= 0.0065982406958937645\n",
      "epoch= 12 iteration= 1157 loss= 0.030108030885457993\n",
      "epoch= 12 iteration= 1177 loss= 0.0344342440366745\n",
      "epoch= 12 iteration= 1197 loss= 0.016129139810800552\n",
      "test_data MSELoss:(pred-real)/real= 0.01517140955871178\n",
      "epoch= 13 iteration= 1210 loss= 0.11948103457689285\n",
      "epoch= 13 iteration= 1230 loss= 0.007668586913496256\n",
      "epoch= 13 iteration= 1250 loss= 0.029719768092036247\n",
      "epoch= 13 iteration= 1270 loss= 0.030167710036039352\n",
      "epoch= 13 iteration= 1290 loss= 0.0161815844476223\n",
      "test_data MSELoss:(pred-real)/real= 0.01509141230619409\n",
      "epoch= 14 iteration= 1303 loss= 0.10958439111709595\n",
      "epoch= 14 iteration= 1323 loss= 0.008587460964918137\n",
      "epoch= 14 iteration= 1343 loss= 0.028964750468730927\n",
      "epoch= 14 iteration= 1363 loss= 0.026443488895893097\n",
      "epoch= 14 iteration= 1383 loss= 0.016238655894994736\n",
      "test_data MSELoss:(pred-real)/real= 0.01500391801043103\n",
      "epoch= 15 iteration= 1396 loss= 0.10047502815723419\n",
      "epoch= 15 iteration= 1416 loss= 0.009484797716140747\n",
      "epoch= 15 iteration= 1436 loss= 0.028047583997249603\n",
      "epoch= 15 iteration= 1456 loss= 0.023184247314929962\n",
      "epoch= 15 iteration= 1476 loss= 0.016305046156048775\n",
      "test_data MSELoss:(pred-real)/real= 0.014904880773327831\n",
      "epoch= 16 iteration= 1489 loss= 0.09182719141244888\n",
      "epoch= 16 iteration= 1509 loss= 0.010396108962595463\n",
      "epoch= 16 iteration= 1529 loss= 0.02705296128988266\n",
      "epoch= 16 iteration= 1549 loss= 0.02032594010233879\n",
      "epoch= 16 iteration= 1569 loss= 0.016379643231630325\n",
      "test_data MSELoss:(pred-real)/real= 0.014791066603114208\n",
      "epoch= 17 iteration= 1582 loss= 0.08359204232692719\n",
      "epoch= 17 iteration= 1602 loss= 0.011332976631820202\n",
      "epoch= 17 iteration= 1622 loss= 0.025996055454015732\n",
      "epoch= 17 iteration= 1642 loss= 0.017799459397792816\n",
      "epoch= 17 iteration= 1662 loss= 0.016466105356812477\n",
      "test_data MSELoss:(pred-real)/real= 0.014664627036028024\n",
      "epoch= 18 iteration= 1675 loss= 0.07569047808647156\n",
      "epoch= 18 iteration= 1695 loss= 0.012271564453840256\n",
      "epoch= 18 iteration= 1715 loss= 0.024886881932616234\n",
      "epoch= 18 iteration= 1735 loss= 0.015577882528305054\n",
      "epoch= 18 iteration= 1755 loss= 0.016559571027755737\n",
      "test_data MSELoss:(pred-real)/real= 0.0145239164121449\n",
      "epoch= 19 iteration= 1768 loss= 0.06818413734436035\n",
      "epoch= 19 iteration= 1788 loss= 0.013210729695856571\n",
      "epoch= 19 iteration= 1808 loss= 0.02374446950852871\n",
      "epoch= 19 iteration= 1828 loss= 0.01361791417002678\n",
      "epoch= 19 iteration= 1848 loss= 0.01666579395532608\n",
      "test_data MSELoss:(pred-real)/real= 0.014376264498827772\n",
      "epoch= 20 iteration= 1861 loss= 0.06107452139258385\n",
      "epoch= 20 iteration= 1881 loss= 0.014121735468506813\n",
      "epoch= 20 iteration= 1901 loss= 0.02256772667169571\n",
      "epoch= 20 iteration= 1921 loss= 0.011885472573339939\n",
      "epoch= 20 iteration= 1941 loss= 0.016779456287622452\n",
      "test_data MSELoss:(pred-real)/real= 0.014221902775009058\n",
      "epoch= 21 iteration= 1954 loss= 0.054387032985687256\n",
      "epoch= 21 iteration= 1974 loss= 0.01499328576028347\n",
      "epoch= 21 iteration= 1994 loss= 0.021376727148890495\n",
      "epoch= 21 iteration= 2014 loss= 0.010361902415752411\n",
      "epoch= 21 iteration= 2034 loss= 0.016906894743442535\n",
      "test_data MSELoss:(pred-real)/real= 0.014064253139723506\n",
      "epoch= 22 iteration= 2047 loss= 0.04813844710588455\n",
      "epoch= 22 iteration= 2067 loss= 0.015810176730155945\n",
      "epoch= 22 iteration= 2087 loss= 0.020179659128189087\n",
      "epoch= 22 iteration= 2107 loss= 0.009014027193188667\n",
      "epoch= 22 iteration= 2127 loss= 0.01704576425254345\n",
      "test_data MSELoss:(pred-real)/real= 0.013901163791565018\n",
      "epoch= 23 iteration= 2140 loss= 0.042374081909656525\n",
      "epoch= 23 iteration= 2160 loss= 0.01653829962015152\n",
      "epoch= 23 iteration= 2180 loss= 0.018985889852046967\n",
      "epoch= 23 iteration= 2200 loss= 0.007829605601727962\n",
      "epoch= 23 iteration= 2220 loss= 0.0172039233148098\n",
      "test_data MSELoss:(pred-real)/real= 0.013740801200684574\n",
      "epoch= 24 iteration= 2233 loss= 0.03706670552492142\n",
      "epoch= 24 iteration= 2253 loss= 0.01718110777437687\n",
      "epoch= 24 iteration= 2273 loss= 0.017804879695177078\n",
      "epoch= 24 iteration= 2293 loss= 0.0067924452014267445\n",
      "epoch= 24 iteration= 2313 loss= 0.01737365871667862\n",
      "test_data MSELoss:(pred-real)/real= 0.013580189746183654\n",
      "epoch= 25 iteration= 2326 loss= 0.03227934241294861\n",
      "epoch= 25 iteration= 2346 loss= 0.01770312711596489\n",
      "epoch= 25 iteration= 2366 loss= 0.016653310507535934\n",
      "epoch= 25 iteration= 2386 loss= 0.005881408229470253\n",
      "epoch= 25 iteration= 2406 loss= 0.017557773739099503\n",
      "test_data MSELoss:(pred-real)/real= 0.0134252754247023\n",
      "epoch= 26 iteration= 2419 loss= 0.027968518435955048\n",
      "epoch= 26 iteration= 2439 loss= 0.01811078190803528\n",
      "epoch= 26 iteration= 2459 loss= 0.015536020509898663\n",
      "epoch= 26 iteration= 2479 loss= 0.0050916653126478195\n",
      "epoch= 26 iteration= 2499 loss= 0.017766546458005905\n",
      "test_data MSELoss:(pred-real)/real= 0.013274095952510834\n",
      "epoch= 27 iteration= 2512 loss= 0.02413824573159218\n",
      "epoch= 27 iteration= 2532 loss= 0.01838883012533188\n",
      "epoch= 27 iteration= 2552 loss= 0.014462977647781372\n",
      "epoch= 27 iteration= 2572 loss= 0.004410259425640106\n",
      "epoch= 27 iteration= 2592 loss= 0.0179951973259449\n",
      "test_data MSELoss:(pred-real)/real= 0.013131581583163805\n",
      "epoch= 28 iteration= 2605 loss= 0.020765207707881927\n",
      "epoch= 28 iteration= 2625 loss= 0.018551062792539597\n",
      "epoch= 28 iteration= 2645 loss= 0.013446569442749023\n",
      "epoch= 28 iteration= 2665 loss= 0.003825215622782707\n",
      "epoch= 28 iteration= 2685 loss= 0.01824934594333172\n",
      "test_data MSELoss:(pred-real)/real= 0.013000167763998939\n",
      "epoch= 29 iteration= 2698 loss= 0.017821475863456726\n",
      "epoch= 29 iteration= 2718 loss= 0.018575023859739304\n",
      "epoch= 29 iteration= 2738 loss= 0.012490508146584034\n",
      "epoch= 29 iteration= 2758 loss= 0.0033348198048770428\n",
      "epoch= 29 iteration= 2778 loss= 0.01851927489042282\n",
      "test_data MSELoss:(pred-real)/real= 0.012874104030844238\n",
      "epoch= 30 iteration= 2791 loss= 0.015279685147106647\n",
      "epoch= 30 iteration= 2811 loss= 0.01850305125117302\n",
      "epoch= 30 iteration= 2831 loss= 0.011604719795286655\n",
      "epoch= 30 iteration= 2851 loss= 0.002917110687121749\n",
      "epoch= 30 iteration= 2871 loss= 0.018813101574778557\n",
      "test_data MSELoss:(pred-real)/real= 0.012767988334720334\n",
      "epoch= 31 iteration= 2884 loss= 0.01308867335319519\n",
      "epoch= 31 iteration= 2904 loss= 0.01832590438425541\n",
      "epoch= 31 iteration= 2924 loss= 0.010774361900985241\n",
      "epoch= 31 iteration= 2944 loss= 0.0025738528929650784\n",
      "epoch= 31 iteration= 2964 loss= 0.019127560779452324\n",
      "test_data MSELoss:(pred-real)/real= 0.012666016752417717\n",
      "epoch= 32 iteration= 2977 loss= 0.011225350201129913\n",
      "epoch= 32 iteration= 2997 loss= 0.018074695020914078\n",
      "epoch= 32 iteration= 3017 loss= 0.01001985277980566\n",
      "epoch= 32 iteration= 3037 loss= 0.0022931476123631\n",
      "epoch= 32 iteration= 3057 loss= 0.019463829696178436\n",
      "test_data MSELoss:(pred-real)/real= 0.012577889034421079\n",
      "epoch= 33 iteration= 3070 loss= 0.00964131485670805\n",
      "epoch= 33 iteration= 3090 loss= 0.017744755372405052\n",
      "epoch= 33 iteration= 3110 loss= 0.009329397231340408\n",
      "epoch= 33 iteration= 3130 loss= 0.002068512374535203\n",
      "epoch= 33 iteration= 3150 loss= 0.019823575392365456\n",
      "test_data MSELoss:(pred-real)/real= 0.012495123936484257\n",
      "epoch= 34 iteration= 3163 loss= 0.008295852690935135\n",
      "epoch= 34 iteration= 3183 loss= 0.017361987382173538\n",
      "epoch= 34 iteration= 3203 loss= 0.00869789719581604\n",
      "epoch= 34 iteration= 3223 loss= 0.001891233492642641\n",
      "epoch= 34 iteration= 3243 loss= 0.020192164927721024\n",
      "test_data MSELoss:(pred-real)/real= 0.0124320764767213\n",
      "epoch= 35 iteration= 3256 loss= 0.007163389585912228\n",
      "epoch= 35 iteration= 3276 loss= 0.016946246847510338\n",
      "epoch= 35 iteration= 3296 loss= 0.008123614825308323\n",
      "epoch= 35 iteration= 3316 loss= 0.0017536748200654984\n",
      "epoch= 35 iteration= 3336 loss= 0.020574714988470078\n",
      "test_data MSELoss:(pred-real)/real= 0.012372950868060192\n",
      "epoch= 36 iteration= 3349 loss= 0.006201252341270447\n",
      "epoch= 36 iteration= 3369 loss= 0.016505517065525055\n",
      "epoch= 36 iteration= 3389 loss= 0.007603173144161701\n",
      "epoch= 36 iteration= 3409 loss= 0.0016513370210304856\n",
      "epoch= 36 iteration= 3429 loss= 0.02097858116030693\n",
      "test_data MSELoss:(pred-real)/real= 0.012325577476682762\n",
      "epoch= 37 iteration= 3442 loss= 0.00539238378405571\n",
      "epoch= 37 iteration= 3462 loss= 0.01604071632027626\n",
      "epoch= 37 iteration= 3482 loss= 0.0071313651278615\n",
      "epoch= 37 iteration= 3502 loss= 0.0015769234159961343\n",
      "epoch= 37 iteration= 3522 loss= 0.021388914436101913\n",
      "test_data MSELoss:(pred-real)/real= 0.012287576333619654\n",
      "epoch= 38 iteration= 3535 loss= 0.004709524102509022\n",
      "epoch= 38 iteration= 3555 loss= 0.015571855008602142\n",
      "epoch= 38 iteration= 3575 loss= 0.0067025162279605865\n",
      "epoch= 38 iteration= 3595 loss= 0.0015257908962666988\n",
      "epoch= 38 iteration= 3615 loss= 0.02179430052638054\n",
      "test_data MSELoss:(pred-real)/real= 0.012250994383874867\n",
      "epoch= 39 iteration= 3628 loss= 0.004128097090870142\n",
      "epoch= 39 iteration= 3648 loss= 0.015099183656275272\n",
      "epoch= 39 iteration= 3668 loss= 0.006316668353974819\n",
      "epoch= 39 iteration= 3688 loss= 0.001491420203819871\n",
      "epoch= 39 iteration= 3708 loss= 0.02221919223666191\n",
      "test_data MSELoss:(pred-real)/real= 0.012231525305348137\n",
      "epoch= 40 iteration= 3721 loss= 0.0036325068213045597\n",
      "epoch= 40 iteration= 3741 loss= 0.014632566832005978\n",
      "epoch= 40 iteration= 3761 loss= 0.005963795818388462\n",
      "epoch= 40 iteration= 3781 loss= 0.0014717783778905869\n",
      "epoch= 40 iteration= 3801 loss= 0.02263632044196129\n",
      "test_data MSELoss:(pred-real)/real= 0.012212363007064495\n",
      "epoch= 41 iteration= 3814 loss= 0.0032152661588042974\n",
      "epoch= 41 iteration= 3834 loss= 0.014165784232318401\n",
      "epoch= 41 iteration= 3854 loss= 0.005644194781780243\n",
      "epoch= 41 iteration= 3874 loss= 0.0014623819151893258\n",
      "epoch= 41 iteration= 3894 loss= 0.023048507049679756\n",
      "test_data MSELoss:(pred-real)/real= 0.012195863838618001\n",
      "epoch= 42 iteration= 3907 loss= 0.002855392638593912\n",
      "epoch= 42 iteration= 3927 loss= 0.01372123509645462\n",
      "epoch= 42 iteration= 3947 loss= 0.005351565778255463\n",
      "epoch= 42 iteration= 3967 loss= 0.0014603917952626944\n",
      "epoch= 42 iteration= 3987 loss= 0.023466167971491814\n",
      "test_data MSELoss:(pred-real)/real= 0.012189596723247733\n",
      "epoch= 43 iteration= 4000 loss= 0.002549929078668356\n",
      "epoch= 43 iteration= 4020 loss= 0.013286339119076729\n",
      "epoch= 43 iteration= 4040 loss= 0.005084157921373844\n",
      "epoch= 43 iteration= 4060 loss= 0.001463472144678235\n",
      "epoch= 43 iteration= 4080 loss= 0.023872580379247665\n",
      "test_data MSELoss:(pred-real)/real= 0.012187826718824605\n",
      "epoch= 44 iteration= 4093 loss= 0.002283881651237607\n",
      "epoch= 44 iteration= 4113 loss= 0.012870212085545063\n",
      "epoch= 44 iteration= 4133 loss= 0.004837591666728258\n",
      "epoch= 44 iteration= 4153 loss= 0.0014698037412017584\n",
      "epoch= 44 iteration= 4173 loss= 0.024288784712553024\n",
      "test_data MSELoss:(pred-real)/real= 0.01218566180775977\n",
      "epoch= 45 iteration= 4186 loss= 0.002058360492810607\n",
      "epoch= 45 iteration= 4206 loss= 0.01246580109000206\n",
      "epoch= 45 iteration= 4226 loss= 0.004616346210241318\n",
      "epoch= 45 iteration= 4246 loss= 0.0014781667850911617\n",
      "epoch= 45 iteration= 4266 loss= 0.024677865207195282\n",
      "test_data MSELoss:(pred-real)/real= 0.012183195898412831\n",
      "epoch= 46 iteration= 4279 loss= 0.0018623664509505033\n",
      "epoch= 46 iteration= 4299 loss= 0.01207437552511692\n",
      "epoch= 46 iteration= 4319 loss= 0.004408814944326878\n",
      "epoch= 46 iteration= 4339 loss= 0.0014879818772897124\n",
      "epoch= 46 iteration= 4359 loss= 0.025066498667001724\n",
      "test_data MSELoss:(pred-real)/real= 0.012194905869869722\n",
      "epoch= 47 iteration= 4372 loss= 0.0016899924958124757\n",
      "epoch= 47 iteration= 4392 loss= 0.01170499436557293\n",
      "epoch= 47 iteration= 4412 loss= 0.004217772278934717\n",
      "epoch= 47 iteration= 4432 loss= 0.0014974195510149002\n",
      "epoch= 47 iteration= 4452 loss= 0.025443915277719498\n",
      "test_data MSELoss:(pred-real)/real= 0.01219657831825316\n",
      "epoch= 48 iteration= 4465 loss= 0.0015431581996381283\n",
      "epoch= 48 iteration= 4485 loss= 0.011350632645189762\n",
      "epoch= 48 iteration= 4505 loss= 0.004046231508255005\n",
      "epoch= 48 iteration= 4525 loss= 0.0015062297461554408\n",
      "epoch= 48 iteration= 4545 loss= 0.025810083374381065\n",
      "test_data MSELoss:(pred-real)/real= 0.012202402757894661\n",
      "epoch= 49 iteration= 4558 loss= 0.0014118164544925094\n",
      "epoch= 49 iteration= 4578 loss= 0.01101535651832819\n",
      "epoch= 49 iteration= 4598 loss= 0.00388488732278347\n",
      "epoch= 49 iteration= 4618 loss= 0.0015149376122280955\n",
      "epoch= 49 iteration= 4638 loss= 0.02615504153072834\n",
      "test_data MSELoss:(pred-real)/real= 0.012204482168373134\n",
      "epoch= 50 iteration= 4651 loss= 0.0012998934835195541\n",
      "epoch= 50 iteration= 4671 loss= 0.010691707953810692\n",
      "epoch= 50 iteration= 4691 loss= 0.0037368121556937695\n",
      "epoch= 50 iteration= 4711 loss= 0.0015218890039250255\n",
      "epoch= 50 iteration= 4731 loss= 0.026497414335608482\n",
      "test_data MSELoss:(pred-real)/real= 0.012213490644676818\n",
      "epoch= 51 iteration= 4744 loss= 0.0012004332384094596\n",
      "epoch= 51 iteration= 4764 loss= 0.010388633236289024\n",
      "epoch= 51 iteration= 4784 loss= 0.003599316580221057\n",
      "epoch= 51 iteration= 4804 loss= 0.0015278981300070882\n",
      "epoch= 51 iteration= 4824 loss= 0.026825767010450363\n",
      "test_data MSELoss:(pred-real)/real= 0.012223511688514717\n",
      "epoch= 52 iteration= 4837 loss= 0.001115099643357098\n",
      "epoch= 52 iteration= 4857 loss= 0.010098621249198914\n",
      "epoch= 52 iteration= 4877 loss= 0.0034749405458569527\n",
      "epoch= 52 iteration= 4897 loss= 0.0015318356454372406\n",
      "epoch= 52 iteration= 4917 loss= 0.02714010700583458\n",
      "test_data MSELoss:(pred-real)/real= 0.012221361135339571\n",
      "epoch= 53 iteration= 4930 loss= 0.0010402746265754104\n",
      "epoch= 53 iteration= 4950 loss= 0.009823650121688843\n",
      "epoch= 53 iteration= 4970 loss= 0.003357083071023226\n",
      "epoch= 53 iteration= 4990 loss= 0.0015355362556874752\n",
      "epoch= 53 iteration= 5010 loss= 0.027437496930360794\n",
      "test_data MSELoss:(pred-real)/real= 0.012225704966112971\n",
      "epoch= 54 iteration= 5023 loss= 0.0009741622488945723\n",
      "epoch= 54 iteration= 5043 loss= 0.00956687331199646\n",
      "epoch= 54 iteration= 5063 loss= 0.003248658962547779\n",
      "epoch= 54 iteration= 5083 loss= 0.001538407290354371\n",
      "epoch= 54 iteration= 5103 loss= 0.027723580598831177\n",
      "test_data MSELoss:(pred-real)/real= 0.012233234230532415\n",
      "epoch= 55 iteration= 5116 loss= 0.0009120756294578314\n",
      "epoch= 55 iteration= 5136 loss= 0.009320246055722237\n",
      "epoch= 55 iteration= 5156 loss= 0.003145641880109906\n",
      "epoch= 55 iteration= 5176 loss= 0.0015389671316370368\n",
      "epoch= 55 iteration= 5196 loss= 0.02799190767109394\n",
      "test_data MSELoss:(pred-real)/real= 0.012235180190246966\n",
      "epoch= 56 iteration= 5209 loss= 0.0008604618487879634\n",
      "epoch= 56 iteration= 5229 loss= 0.009090314619243145\n",
      "epoch= 56 iteration= 5249 loss= 0.0030539645813405514\n",
      "epoch= 56 iteration= 5269 loss= 0.0015387055464088917\n",
      "epoch= 56 iteration= 5289 loss= 0.028248541057109833\n",
      "test_data MSELoss:(pred-real)/real= 0.012235270062875416\n",
      "epoch= 57 iteration= 5302 loss= 0.0008145468891598284\n",
      "epoch= 57 iteration= 5322 loss= 0.0088710468262434\n",
      "epoch= 57 iteration= 5342 loss= 0.0029685087502002716\n",
      "epoch= 57 iteration= 5362 loss= 0.0015387153252959251\n",
      "epoch= 57 iteration= 5382 loss= 0.028500188142061234\n",
      "test_data MSELoss:(pred-real)/real= 0.012232630125557383\n",
      "epoch= 58 iteration= 5395 loss= 0.0007742773159407079\n",
      "epoch= 58 iteration= 5415 loss= 0.008668489754199982\n",
      "epoch= 58 iteration= 5435 loss= 0.0028878154698759317\n",
      "epoch= 58 iteration= 5455 loss= 0.0015363728161901236\n",
      "epoch= 58 iteration= 5475 loss= 0.028724126517772675\n",
      "test_data MSELoss:(pred-real)/real= 0.01222152746696439\n",
      "epoch= 59 iteration= 5488 loss= 0.0007383697666227818\n",
      "epoch= 59 iteration= 5508 loss= 0.008473999798297882\n",
      "epoch= 59 iteration= 5528 loss= 0.0028128393460065126\n",
      "epoch= 59 iteration= 5548 loss= 0.0015344839775934815\n",
      "epoch= 59 iteration= 5568 loss= 0.028946738690137863\n",
      "test_data MSELoss:(pred-real)/real= 0.012215354272888767\n",
      "epoch= 60 iteration= 5581 loss= 0.0007061331998556852\n",
      "epoch= 60 iteration= 5601 loss= 0.008290650323033333\n",
      "epoch= 60 iteration= 5621 loss= 0.0027450062334537506\n",
      "epoch= 60 iteration= 5641 loss= 0.0015317806974053383\n",
      "epoch= 60 iteration= 5661 loss= 0.029150355607271194\n",
      "test_data MSELoss:(pred-real)/real= 0.012207081024017599\n",
      "epoch= 61 iteration= 5674 loss= 0.0006772822816856205\n",
      "epoch= 61 iteration= 5694 loss= 0.008118320256471634\n",
      "epoch= 61 iteration= 5714 loss= 0.002679409459233284\n",
      "epoch= 61 iteration= 5734 loss= 0.0015289313159883022\n",
      "epoch= 61 iteration= 5754 loss= 0.029338883236050606\n",
      "test_data MSELoss:(pred-real)/real= 0.012190898595791724\n",
      "epoch= 62 iteration= 5767 loss= 0.0006520069437101483\n",
      "epoch= 62 iteration= 5787 loss= 0.007964219897985458\n",
      "epoch= 62 iteration= 5807 loss= 0.0026181256398558617\n",
      "epoch= 62 iteration= 5827 loss= 0.0015252202283591032\n",
      "epoch= 62 iteration= 5847 loss= 0.029526952654123306\n",
      "test_data MSELoss:(pred-real)/real= 0.01218092587724742\n",
      "epoch= 63 iteration= 5860 loss= 0.0006292507750913501\n",
      "epoch= 63 iteration= 5880 loss= 0.007813848555088043\n",
      "epoch= 63 iteration= 5900 loss= 0.0025620092637836933\n",
      "epoch= 63 iteration= 5920 loss= 0.0015205438248813152\n",
      "epoch= 63 iteration= 5940 loss= 0.02968909591436386\n",
      "test_data MSELoss:(pred-real)/real= 0.012159127897272507\n",
      "epoch= 64 iteration= 5953 loss= 0.0006092338007874787\n",
      "epoch= 64 iteration= 5973 loss= 0.007670343853533268\n",
      "epoch= 64 iteration= 5993 loss= 0.002510735532268882\n",
      "epoch= 64 iteration= 6013 loss= 0.0015168574173003435\n",
      "epoch= 64 iteration= 6033 loss= 0.029853418469429016\n",
      "test_data MSELoss:(pred-real)/real= 0.012143415774011778\n",
      "epoch= 65 iteration= 6046 loss= 0.0005910849431529641\n",
      "epoch= 65 iteration= 6066 loss= 0.007539879065006971\n",
      "epoch= 65 iteration= 6086 loss= 0.0024644003715366125\n",
      "epoch= 65 iteration= 6106 loss= 0.0015129420207813382\n",
      "epoch= 65 iteration= 6126 loss= 0.02998913638293743\n",
      "test_data MSELoss:(pred-real)/real= 0.012117950116387673\n",
      "epoch= 66 iteration= 6139 loss= 0.000575129990465939\n",
      "epoch= 66 iteration= 6159 loss= 0.0074128881096839905\n",
      "epoch= 66 iteration= 6179 loss= 0.0024198698811233044\n",
      "epoch= 66 iteration= 6199 loss= 0.0015089173102751374\n",
      "epoch= 66 iteration= 6219 loss= 0.030135512351989746\n",
      "test_data MSELoss:(pred-real)/real= 0.0120926339748419\n",
      "epoch= 67 iteration= 6232 loss= 0.0005601091543212533\n",
      "epoch= 67 iteration= 6252 loss= 0.007293268106877804\n",
      "epoch= 67 iteration= 6272 loss= 0.0023748420644551516\n",
      "epoch= 67 iteration= 6292 loss= 0.0015046182088553905\n",
      "epoch= 67 iteration= 6312 loss= 0.030259544029831886\n",
      "test_data MSELoss:(pred-real)/real= 0.012071728965060579\n",
      "epoch= 68 iteration= 6325 loss= 0.0005460268585011363\n",
      "epoch= 68 iteration= 6345 loss= 0.0071798223070800304\n",
      "epoch= 68 iteration= 6365 loss= 0.0023389672860503197\n",
      "epoch= 68 iteration= 6385 loss= 0.0015000358689576387\n",
      "epoch= 68 iteration= 6405 loss= 0.030367989093065262\n",
      "test_data MSELoss:(pred-real)/real= 0.012040698693858253\n",
      "epoch= 69 iteration= 6418 loss= 0.0005350052379071712\n",
      "epoch= 69 iteration= 6438 loss= 0.007073987275362015\n",
      "epoch= 69 iteration= 6458 loss= 0.0023012319579720497\n",
      "epoch= 69 iteration= 6478 loss= 0.00149536260869354\n",
      "epoch= 69 iteration= 6498 loss= 0.03048229217529297\n",
      "test_data MSELoss:(pred-real)/real= 0.012008912111115124\n",
      "epoch= 70 iteration= 6511 loss= 0.0005246333894319832\n",
      "epoch= 70 iteration= 6531 loss= 0.006973623298108578\n",
      "epoch= 70 iteration= 6551 loss= 0.0022667841985821724\n",
      "epoch= 70 iteration= 6571 loss= 0.0014905973803251982\n",
      "epoch= 70 iteration= 6591 loss= 0.030572516843676567\n",
      "test_data MSELoss:(pred-real)/real= 0.01197718962147418\n",
      "epoch= 71 iteration= 6604 loss= 0.0005144219612702727\n",
      "epoch= 71 iteration= 6624 loss= 0.0068917833268642426\n",
      "epoch= 71 iteration= 6644 loss= 0.002236511092633009\n",
      "epoch= 71 iteration= 6664 loss= 0.0014865403063595295\n",
      "epoch= 71 iteration= 6684 loss= 0.03066650778055191\n",
      "test_data MSELoss:(pred-real)/real= 0.011945966799329553\n",
      "epoch= 72 iteration= 6697 loss= 0.0005055275396443903\n",
      "epoch= 72 iteration= 6717 loss= 0.006799312774091959\n",
      "epoch= 72 iteration= 6737 loss= 0.0022054719738662243\n",
      "epoch= 72 iteration= 6757 loss= 0.0014817630872130394\n",
      "epoch= 72 iteration= 6777 loss= 0.03074164129793644\n",
      "test_data MSELoss:(pred-real)/real= 0.01190378819592297\n",
      "epoch= 73 iteration= 6790 loss= 0.000497575500048697\n",
      "epoch= 73 iteration= 6810 loss= 0.0067193880677223206\n",
      "epoch= 73 iteration= 6830 loss= 0.002179298549890518\n",
      "epoch= 73 iteration= 6850 loss= 0.001477178419008851\n",
      "epoch= 73 iteration= 6870 loss= 0.03080694004893303\n",
      "test_data MSELoss:(pred-real)/real= 0.011860403775547942\n",
      "epoch= 74 iteration= 6883 loss= 0.0004909688723273575\n",
      "epoch= 74 iteration= 6903 loss= 0.006641101557761431\n",
      "epoch= 74 iteration= 6923 loss= 0.0021530441008508205\n",
      "epoch= 74 iteration= 6943 loss= 0.0014735088916495442\n",
      "epoch= 74 iteration= 6963 loss= 0.030882520601153374\n",
      "test_data MSELoss:(pred-real)/real= 0.011823549284599721\n",
      "epoch= 75 iteration= 6976 loss= 0.0004834296414628625\n",
      "epoch= 75 iteration= 6996 loss= 0.006570293568074703\n",
      "epoch= 75 iteration= 7016 loss= 0.0021300637163221836\n",
      "epoch= 75 iteration= 7036 loss= 0.0014694745186716318\n",
      "epoch= 75 iteration= 7056 loss= 0.030934862792491913\n",
      "test_data MSELoss:(pred-real)/real= 0.011789182433858514\n",
      "epoch= 76 iteration= 7069 loss= 0.0004770877421833575\n",
      "epoch= 76 iteration= 7089 loss= 0.00650468235835433\n",
      "epoch= 76 iteration= 7109 loss= 0.0021057534031569958\n",
      "epoch= 76 iteration= 7129 loss= 0.0014655847335234284\n",
      "epoch= 76 iteration= 7149 loss= 0.030985895544290543\n",
      "test_data MSELoss:(pred-real)/real= 0.011746416476348208\n",
      "epoch= 77 iteration= 7162 loss= 0.00047172963968478143\n",
      "epoch= 77 iteration= 7182 loss= 0.006439644377678633\n",
      "epoch= 77 iteration= 7202 loss= 0.0020868151914328337\n",
      "epoch= 77 iteration= 7222 loss= 0.0014611210208386183\n",
      "epoch= 77 iteration= 7242 loss= 0.031034961342811584\n",
      "test_data MSELoss:(pred-real)/real= 0.01169305193858842\n",
      "epoch= 78 iteration= 7255 loss= 0.00046662744716741145\n",
      "epoch= 78 iteration= 7275 loss= 0.006376292556524277\n",
      "epoch= 78 iteration= 7295 loss= 0.0020658208522945642\n",
      "epoch= 78 iteration= 7315 loss= 0.0014570606872439384\n",
      "epoch= 78 iteration= 7335 loss= 0.031070370227098465\n",
      "test_data MSELoss:(pred-real)/real= 0.011649701369201971\n",
      "epoch= 79 iteration= 7348 loss= 0.00046187511179596186\n",
      "epoch= 79 iteration= 7368 loss= 0.006325365975499153\n",
      "epoch= 79 iteration= 7388 loss= 0.0020504817366600037\n",
      "epoch= 79 iteration= 7408 loss= 0.001453530159778893\n",
      "epoch= 79 iteration= 7428 loss= 0.03109484165906906\n",
      "test_data MSELoss:(pred-real)/real= 0.01159884498661591\n",
      "epoch= 80 iteration= 7441 loss= 0.00045753311133012176\n",
      "epoch= 80 iteration= 7461 loss= 0.006271373946219683\n",
      "epoch= 80 iteration= 7481 loss= 0.002032014075666666\n",
      "epoch= 80 iteration= 7501 loss= 0.0014497536467388272\n",
      "epoch= 80 iteration= 7521 loss= 0.031127985566854477\n",
      "test_data MSELoss:(pred-real)/real= 0.011555641941312287\n",
      "epoch= 81 iteration= 7534 loss= 0.0004522739618550986\n",
      "epoch= 81 iteration= 7554 loss= 0.006216352805495262\n",
      "epoch= 81 iteration= 7574 loss= 0.0020155333913862705\n",
      "epoch= 81 iteration= 7594 loss= 0.0014461148530244827\n",
      "epoch= 81 iteration= 7614 loss= 0.031153611838817596\n",
      "test_data MSELoss:(pred-real)/real= 0.01150696502170629\n",
      "epoch= 82 iteration= 7627 loss= 0.0004492098814807832\n",
      "epoch= 82 iteration= 7647 loss= 0.006174875423312187\n",
      "epoch= 82 iteration= 7667 loss= 0.0020006741397082806\n",
      "epoch= 82 iteration= 7687 loss= 0.0014423530083149672\n",
      "epoch= 82 iteration= 7707 loss= 0.031162474304437637\n",
      "test_data MSELoss:(pred-real)/real= 0.011456673641482161\n",
      "epoch= 83 iteration= 7720 loss= 0.00044533112668432295\n",
      "epoch= 83 iteration= 7740 loss= 0.006129204761236906\n",
      "epoch= 83 iteration= 7760 loss= 0.0019862703047692776\n",
      "epoch= 83 iteration= 7780 loss= 0.0014391979202628136\n",
      "epoch= 83 iteration= 7800 loss= 0.03117574006319046\n",
      "test_data MSELoss:(pred-real)/real= 0.011398891025843719\n",
      "epoch= 84 iteration= 7813 loss= 0.0004419518227223307\n",
      "epoch= 84 iteration= 7833 loss= 0.00608697859570384\n",
      "epoch= 84 iteration= 7853 loss= 0.0019742005970329046\n",
      "epoch= 84 iteration= 7873 loss= 0.0014354686718434095\n",
      "epoch= 84 iteration= 7893 loss= 0.031176012009382248\n",
      "test_data MSELoss:(pred-real)/real= 0.01134844530477292\n",
      "epoch= 85 iteration= 7906 loss= 0.0004389651876408607\n",
      "epoch= 85 iteration= 7926 loss= 0.00604194076731801\n",
      "epoch= 85 iteration= 7946 loss= 0.001961124362424016\n",
      "epoch= 85 iteration= 7966 loss= 0.001432366669178009\n",
      "epoch= 85 iteration= 7986 loss= 0.031175270676612854\n",
      "test_data MSELoss:(pred-real)/real= 0.01129183729386164\n",
      "epoch= 86 iteration= 7999 loss= 0.0004364521009847522\n",
      "epoch= 86 iteration= 8019 loss= 0.006008195225149393\n",
      "epoch= 86 iteration= 8039 loss= 0.0019492371939122677\n",
      "epoch= 86 iteration= 8059 loss= 0.0014285794459283352\n",
      "epoch= 86 iteration= 8079 loss= 0.031181219965219498\n",
      "test_data MSELoss:(pred-real)/real= 0.011236877710972395\n",
      "epoch= 87 iteration= 8092 loss= 0.00043383697629906237\n",
      "epoch= 87 iteration= 8112 loss= 0.005973629653453827\n",
      "epoch= 87 iteration= 8132 loss= 0.0019381981110200286\n",
      "epoch= 87 iteration= 8152 loss= 0.0014260744210332632\n",
      "epoch= 87 iteration= 8172 loss= 0.0311734601855278\n",
      "test_data MSELoss:(pred-real)/real= 0.011184751948652169\n",
      "epoch= 88 iteration= 8185 loss= 0.0004312417877372354\n",
      "epoch= 88 iteration= 8205 loss= 0.005936122499406338\n",
      "epoch= 88 iteration= 8225 loss= 0.0019277743995189667\n",
      "epoch= 88 iteration= 8245 loss= 0.001422687666490674\n",
      "epoch= 88 iteration= 8265 loss= 0.03116774372756481\n",
      "test_data MSELoss:(pred-real)/real= 0.011133842196108567\n",
      "epoch= 89 iteration= 8278 loss= 0.00042845041025429964\n",
      "epoch= 89 iteration= 8298 loss= 0.005900206044316292\n",
      "epoch= 89 iteration= 8318 loss= 0.0019177504582330585\n",
      "epoch= 89 iteration= 8338 loss= 0.0014202588936313987\n",
      "epoch= 89 iteration= 8358 loss= 0.03114994987845421\n",
      "test_data MSELoss:(pred-real)/real= 0.011070007316043807\n",
      "epoch= 90 iteration= 8371 loss= 0.00042630036477930844\n",
      "epoch= 90 iteration= 8391 loss= 0.005875523202121258\n",
      "epoch= 90 iteration= 8411 loss= 0.001908212318085134\n",
      "epoch= 90 iteration= 8431 loss= 0.0014178073033690453\n",
      "epoch= 90 iteration= 8451 loss= 0.03113483265042305\n",
      "test_data MSELoss:(pred-real)/real= 0.011020777902255455\n",
      "epoch= 91 iteration= 8464 loss= 0.0004240113194100559\n",
      "epoch= 91 iteration= 8484 loss= 0.005849569570273161\n",
      "epoch= 91 iteration= 8504 loss= 0.0019004635978490114\n",
      "epoch= 91 iteration= 8524 loss= 0.0014141574501991272\n",
      "epoch= 91 iteration= 8544 loss= 0.031119678169488907\n",
      "test_data MSELoss:(pred-real)/real= 0.010966517768489817\n",
      "epoch= 92 iteration= 8557 loss= 0.0004218409594614059\n",
      "epoch= 92 iteration= 8577 loss= 0.005821159575134516\n",
      "epoch= 92 iteration= 8597 loss= 0.0018919743597507477\n",
      "epoch= 92 iteration= 8617 loss= 0.0014116506790742278\n",
      "epoch= 92 iteration= 8637 loss= 0.03109475038945675\n",
      "test_data MSELoss:(pred-real)/real= 0.010904493327769969\n",
      "epoch= 93 iteration= 8650 loss= 0.0004201920819468796\n",
      "epoch= 93 iteration= 8670 loss= 0.005792650394141674\n",
      "epoch= 93 iteration= 8690 loss= 0.0018837221432477236\n",
      "epoch= 93 iteration= 8710 loss= 0.0014083432033658028\n",
      "epoch= 93 iteration= 8730 loss= 0.031075678765773773\n",
      "test_data MSELoss:(pred-real)/real= 0.010848527075722814\n",
      "epoch= 94 iteration= 8743 loss= 0.00041781453182920814\n",
      "epoch= 94 iteration= 8763 loss= 0.005770585499703884\n",
      "epoch= 94 iteration= 8783 loss= 0.0018756797071546316\n",
      "epoch= 94 iteration= 8803 loss= 0.00140609429217875\n",
      "epoch= 94 iteration= 8823 loss= 0.031041406095027924\n",
      "test_data MSELoss:(pred-real)/real= 0.01078830817196932\n",
      "epoch= 95 iteration= 8836 loss= 0.00041634897934272885\n",
      "epoch= 95 iteration= 8856 loss= 0.005746590439230204\n",
      "epoch= 95 iteration= 8876 loss= 0.0018688173731788993\n",
      "epoch= 95 iteration= 8896 loss= 0.0014035950880497694\n",
      "epoch= 95 iteration= 8916 loss= 0.031017910689115524\n",
      "test_data MSELoss:(pred-real)/real= 0.01072924432810396\n",
      "epoch= 96 iteration= 8929 loss= 0.00041432646685279906\n",
      "epoch= 96 iteration= 8949 loss= 0.00572142843157053\n",
      "epoch= 96 iteration= 8969 loss= 0.0018626435194164515\n",
      "epoch= 96 iteration= 8989 loss= 0.001400989480316639\n",
      "epoch= 96 iteration= 9009 loss= 0.030979834496974945\n",
      "test_data MSELoss:(pred-real)/real= 0.010672863818601601\n",
      "epoch= 97 iteration= 9022 loss= 0.00041296525159850717\n",
      "epoch= 97 iteration= 9042 loss= 0.005701096728444099\n",
      "epoch= 97 iteration= 9062 loss= 0.0018546427600085735\n",
      "epoch= 97 iteration= 9082 loss= 0.0013983300887048244\n",
      "epoch= 97 iteration= 9102 loss= 0.03093932755291462\n",
      "test_data MSELoss:(pred-real)/real= 0.01061752838237832\n",
      "epoch= 98 iteration= 9115 loss= 0.0004112716414965689\n",
      "epoch= 98 iteration= 9135 loss= 0.005676461383700371\n",
      "epoch= 98 iteration= 9155 loss= 0.0018481831066310406\n",
      "epoch= 98 iteration= 9175 loss= 0.0013960945652797818\n",
      "epoch= 98 iteration= 9195 loss= 0.03090507909655571\n",
      "test_data MSELoss:(pred-real)/real= 0.010554895529316531\n",
      "epoch= 99 iteration= 9208 loss= 0.00040960172191262245\n",
      "epoch= 99 iteration= 9228 loss= 0.0056610554456710815\n",
      "epoch= 99 iteration= 9248 loss= 0.001842351513914764\n",
      "epoch= 99 iteration= 9268 loss= 0.001393312937580049\n",
      "epoch= 99 iteration= 9288 loss= 0.030870243906974792\n",
      "test_data MSELoss:(pred-real)/real= 0.010499502325223552\n",
      "epoch= 100 iteration= 9301 loss= 0.0004084184765815735\n",
      "epoch= 100 iteration= 9321 loss= 0.005638108588755131\n",
      "epoch= 100 iteration= 9341 loss= 0.0018376997904852033\n",
      "epoch= 100 iteration= 9361 loss= 0.0013915920862928033\n",
      "epoch= 100 iteration= 9381 loss= 0.030830305069684982\n",
      "test_data MSELoss:(pred-real)/real= 0.010439625307400193\n",
      "epoch= 101 iteration= 9394 loss= 0.0004069422429893166\n",
      "epoch= 101 iteration= 9414 loss= 0.0056198821403086185\n",
      "epoch= 101 iteration= 9434 loss= 0.001830315450206399\n",
      "epoch= 101 iteration= 9454 loss= 0.0013891084818169475\n",
      "epoch= 101 iteration= 9474 loss= 0.03078523650765419\n",
      "test_data MSELoss:(pred-real)/real= 0.010384358970137933\n",
      "epoch= 102 iteration= 9487 loss= 0.0004049880662932992\n",
      "epoch= 102 iteration= 9507 loss= 0.005607405677437782\n",
      "epoch= 102 iteration= 9527 loss= 0.0018239899072796106\n",
      "epoch= 102 iteration= 9547 loss= 0.0013869217364117503\n",
      "epoch= 102 iteration= 9567 loss= 0.03074733354151249\n",
      "test_data MSELoss:(pred-real)/real= 0.01032456098538306\n",
      "epoch= 103 iteration= 9580 loss= 0.00040423826430924237\n",
      "epoch= 103 iteration= 9600 loss= 0.005589171312749386\n",
      "epoch= 103 iteration= 9620 loss= 0.0018197346944361925\n",
      "epoch= 103 iteration= 9640 loss= 0.001384552801027894\n",
      "epoch= 103 iteration= 9660 loss= 0.030706878751516342\n",
      "test_data MSELoss:(pred-real)/real= 0.010262374403989978\n",
      "epoch= 104 iteration= 9673 loss= 0.0004028304247185588\n",
      "epoch= 104 iteration= 9693 loss= 0.005572170950472355\n",
      "epoch= 104 iteration= 9713 loss= 0.0018140891334041953\n",
      "epoch= 104 iteration= 9733 loss= 0.0013825335772708058\n",
      "epoch= 104 iteration= 9753 loss= 0.030652161687612534\n",
      "test_data MSELoss:(pred-real)/real= 0.010204549969380928\n",
      "epoch= 105 iteration= 9766 loss= 0.0004014723817817867\n",
      "epoch= 105 iteration= 9786 loss= 0.005554869305342436\n",
      "epoch= 105 iteration= 9806 loss= 0.0018103955080732703\n",
      "epoch= 105 iteration= 9826 loss= 0.001380121917463839\n",
      "epoch= 105 iteration= 9846 loss= 0.030600545927882195\n",
      "test_data MSELoss:(pred-real)/real= 0.010148837438060178\n",
      "epoch= 106 iteration= 9859 loss= 0.0004000500775873661\n",
      "epoch= 106 iteration= 9879 loss= 0.005537730176001787\n",
      "epoch= 106 iteration= 9899 loss= 0.0018059255089610815\n",
      "epoch= 106 iteration= 9919 loss= 0.0013781397137790918\n",
      "epoch= 106 iteration= 9939 loss= 0.03054874576628208\n",
      "test_data MSELoss:(pred-real)/real= 0.010089248134237196\n",
      "epoch= 107 iteration= 9952 loss= 0.0003987347590737045\n",
      "epoch= 107 iteration= 9972 loss= 0.005521368235349655\n",
      "epoch= 107 iteration= 9992 loss= 0.0018015436362475157\n",
      "epoch= 107 iteration= 10012 loss= 0.0013756122207269073\n",
      "epoch= 107 iteration= 10032 loss= 0.03051065281033516\n",
      "test_data MSELoss:(pred-real)/real= 0.010031901183538139\n",
      "epoch= 108 iteration= 10045 loss= 0.0003976643201895058\n",
      "epoch= 108 iteration= 10065 loss= 0.0055109187960624695\n",
      "epoch= 108 iteration= 10085 loss= 0.001797175733372569\n",
      "epoch= 108 iteration= 10105 loss= 0.001373520353808999\n",
      "epoch= 108 iteration= 10125 loss= 0.03045712783932686\n",
      "test_data MSELoss:(pred-real)/real= 0.009975081904687814\n",
      "epoch= 109 iteration= 10138 loss= 0.0003971075057052076\n",
      "epoch= 109 iteration= 10158 loss= 0.005496507976204157\n",
      "epoch= 109 iteration= 10178 loss= 0.0017917535733431578\n",
      "epoch= 109 iteration= 10198 loss= 0.0013719273265451193\n",
      "epoch= 109 iteration= 10218 loss= 0.03040275350213051\n",
      "test_data MSELoss:(pred-real)/real= 0.009918579779979255\n",
      "epoch= 110 iteration= 10231 loss= 0.0003954730345867574\n",
      "epoch= 110 iteration= 10251 loss= 0.005478959530591965\n",
      "epoch= 110 iteration= 10271 loss= 0.0017852011369541287\n",
      "epoch= 110 iteration= 10291 loss= 0.0013696487294510007\n",
      "epoch= 110 iteration= 10311 loss= 0.030346568673849106\n",
      "test_data MSELoss:(pred-real)/real= 0.009858846353987852\n",
      "epoch= 111 iteration= 10324 loss= 0.0003946132492274046\n",
      "epoch= 111 iteration= 10344 loss= 0.005470313131809235\n",
      "epoch= 111 iteration= 10364 loss= 0.0017815129831433296\n",
      "epoch= 111 iteration= 10384 loss= 0.0013678864343091846\n",
      "epoch= 111 iteration= 10404 loss= 0.0302882120013237\n",
      "test_data MSELoss:(pred-real)/real= 0.009801677751561834\n",
      "epoch= 112 iteration= 10417 loss= 0.0003933830885216594\n",
      "epoch= 112 iteration= 10437 loss= 0.0054583074524998665\n",
      "epoch= 112 iteration= 10457 loss= 0.0017788251861929893\n",
      "epoch= 112 iteration= 10477 loss= 0.0013661680277436972\n",
      "epoch= 112 iteration= 10497 loss= 0.030241381376981735\n",
      "test_data MSELoss:(pred-real)/real= 0.009744556323211227\n",
      "epoch= 113 iteration= 10510 loss= 0.00039259050390683115\n",
      "epoch= 113 iteration= 10530 loss= 0.005442420952022076\n",
      "epoch= 113 iteration= 10550 loss= 0.0017740033799782395\n",
      "epoch= 113 iteration= 10570 loss= 0.0013642082922160625\n",
      "epoch= 113 iteration= 10590 loss= 0.030182063579559326\n",
      "test_data MSELoss:(pred-real)/real= 0.009690590991845561\n",
      "epoch= 114 iteration= 10603 loss= 0.0003913975087925792\n",
      "epoch= 114 iteration= 10623 loss= 0.005432024598121643\n",
      "epoch= 114 iteration= 10643 loss= 0.001770837465301156\n",
      "epoch= 114 iteration= 10663 loss= 0.0013624259736388922\n",
      "epoch= 114 iteration= 10683 loss= 0.030123025178909302\n",
      "test_data MSELoss:(pred-real)/real= 0.009634188065926233\n",
      "epoch= 115 iteration= 10696 loss= 0.0003903049509972334\n",
      "epoch= 115 iteration= 10716 loss= 0.005419705994427204\n",
      "epoch= 115 iteration= 10736 loss= 0.0017658155411481857\n",
      "epoch= 115 iteration= 10756 loss= 0.001360529800876975\n",
      "epoch= 115 iteration= 10776 loss= 0.030059028416872025\n",
      "test_data MSELoss:(pred-real)/real= 0.009578919102851715\n",
      "epoch= 116 iteration= 10789 loss= 0.00038944385596551\n",
      "epoch= 116 iteration= 10809 loss= 0.005404482129961252\n",
      "epoch= 116 iteration= 10829 loss= 0.0017608056077733636\n",
      "epoch= 116 iteration= 10849 loss= 0.0013585244305431843\n",
      "epoch= 116 iteration= 10869 loss= 0.03000667318701744\n",
      "test_data MSELoss:(pred-real)/real= 0.009524006655232774\n",
      "epoch= 117 iteration= 10882 loss= 0.00038808275712653995\n",
      "epoch= 117 iteration= 10902 loss= 0.005392919294536114\n",
      "epoch= 117 iteration= 10922 loss= 0.0017576438840478659\n",
      "epoch= 117 iteration= 10942 loss= 0.0013562231324613094\n",
      "epoch= 117 iteration= 10962 loss= 0.029938245192170143\n",
      "test_data MSELoss:(pred-real)/real= 0.009462812462718122\n",
      "epoch= 118 iteration= 10975 loss= 0.0003877276321873069\n",
      "epoch= 118 iteration= 10995 loss= 0.005384223535656929\n",
      "epoch= 118 iteration= 11015 loss= 0.0017531143967062235\n",
      "epoch= 118 iteration= 11035 loss= 0.0013547752751037478\n",
      "epoch= 118 iteration= 11055 loss= 0.029881441965699196\n",
      "test_data MSELoss:(pred-real)/real= 0.009406779582301775\n",
      "epoch= 119 iteration= 11068 loss= 0.00038670087815262377\n",
      "epoch= 119 iteration= 11088 loss= 0.005370599217712879\n",
      "epoch= 119 iteration= 11108 loss= 0.0017484398558735847\n",
      "epoch= 119 iteration= 11128 loss= 0.0013531774748116732\n",
      "epoch= 119 iteration= 11148 loss= 0.029819605872035027\n",
      "test_data MSELoss:(pred-real)/real= 0.009351882876621352\n",
      "epoch= 120 iteration= 11161 loss= 0.0003861496224999428\n",
      "epoch= 120 iteration= 11181 loss= 0.00536144245415926\n",
      "epoch= 120 iteration= 11201 loss= 0.0017460425151512027\n",
      "epoch= 120 iteration= 11221 loss= 0.001351906917989254\n",
      "epoch= 120 iteration= 11241 loss= 0.02976459078490734\n",
      "test_data MSELoss:(pred-real)/real= 0.009298828184708126\n",
      "epoch= 121 iteration= 11254 loss= 0.000385054387152195\n",
      "epoch= 121 iteration= 11274 loss= 0.00535462936386466\n",
      "epoch= 121 iteration= 11294 loss= 0.0017413684399798512\n",
      "epoch= 121 iteration= 11314 loss= 0.0013496032916009426\n",
      "epoch= 121 iteration= 11334 loss= 0.029692618176341057\n",
      "test_data MSELoss:(pred-real)/real= 0.009237978400455581\n",
      "epoch= 122 iteration= 11347 loss= 0.0003839553683064878\n",
      "epoch= 122 iteration= 11367 loss= 0.005343685857951641\n",
      "epoch= 122 iteration= 11387 loss= 0.0017381554935127497\n",
      "epoch= 122 iteration= 11407 loss= 0.0013481357600539923\n",
      "epoch= 122 iteration= 11427 loss= 0.02962973527610302\n",
      "test_data MSELoss:(pred-real)/real= 0.009184436549225615\n",
      "epoch= 123 iteration= 11440 loss= 0.00038337684236466885\n",
      "epoch= 123 iteration= 11460 loss= 0.005330290645360947\n",
      "epoch= 123 iteration= 11480 loss= 0.0017321980558335781\n",
      "epoch= 123 iteration= 11500 loss= 0.001346540986560285\n",
      "epoch= 123 iteration= 11520 loss= 0.02956809103488922\n",
      "test_data MSELoss:(pred-real)/real= 0.009131393783415357\n",
      "epoch= 124 iteration= 11533 loss= 0.00038223754381760955\n",
      "epoch= 124 iteration= 11553 loss= 0.0053213126957416534\n",
      "epoch= 124 iteration= 11573 loss= 0.0017286029178649187\n",
      "epoch= 124 iteration= 11593 loss= 0.0013446330558508635\n",
      "epoch= 124 iteration= 11613 loss= 0.029503606259822845\n",
      "test_data MSELoss:(pred-real)/real= 0.00907489101195501\n",
      "epoch= 125 iteration= 11626 loss= 0.00038176082307472825\n",
      "epoch= 125 iteration= 11646 loss= 0.005310166161507368\n",
      "epoch= 125 iteration= 11666 loss= 0.0017249803058803082\n",
      "epoch= 125 iteration= 11686 loss= 0.001343035139143467\n",
      "epoch= 125 iteration= 11706 loss= 0.029442032799124718\n",
      "test_data MSELoss:(pred-real)/real= 0.00902507434754322\n",
      "epoch= 126 iteration= 11719 loss= 0.0003811304341070354\n",
      "epoch= 126 iteration= 11739 loss= 0.005300613585859537\n",
      "epoch= 126 iteration= 11759 loss= 0.0017202754970639944\n",
      "epoch= 126 iteration= 11779 loss= 0.0013418737798929214\n",
      "epoch= 126 iteration= 11799 loss= 0.029372956603765488\n",
      "test_data MSELoss:(pred-real)/real= 0.008970662369392812\n",
      "epoch= 127 iteration= 11812 loss= 0.00037971921847201884\n",
      "epoch= 127 iteration= 11832 loss= 0.005292190238833427\n",
      "epoch= 127 iteration= 11852 loss= 0.0017173564992845058\n",
      "epoch= 127 iteration= 11872 loss= 0.0013397784205153584\n",
      "epoch= 127 iteration= 11892 loss= 0.029309548437595367\n",
      "test_data MSELoss:(pred-real)/real= 0.008916730591509905\n",
      "epoch= 128 iteration= 11905 loss= 0.0003789444745052606\n",
      "epoch= 128 iteration= 11925 loss= 0.005280196666717529\n",
      "epoch= 128 iteration= 11945 loss= 0.0017137611284852028\n",
      "epoch= 128 iteration= 11965 loss= 0.0013388041406869888\n",
      "epoch= 128 iteration= 11985 loss= 0.02923864871263504\n",
      "test_data MSELoss:(pred-real)/real= 0.008864232235484652\n",
      "epoch= 129 iteration= 11998 loss= 0.0003788252070080489\n",
      "epoch= 129 iteration= 12018 loss= 0.005271690897643566\n",
      "epoch= 129 iteration= 12038 loss= 0.0017092712223529816\n",
      "epoch= 129 iteration= 12058 loss= 0.0013370165834203362\n",
      "epoch= 129 iteration= 12078 loss= 0.02917787805199623\n",
      "test_data MSELoss:(pred-real)/real= 0.008812158847124212\n",
      "epoch= 130 iteration= 12091 loss= 0.00037820107536390424\n",
      "epoch= 130 iteration= 12111 loss= 0.005258506163954735\n",
      "epoch= 130 iteration= 12131 loss= 0.0017060482641682029\n",
      "epoch= 130 iteration= 12151 loss= 0.00133555568754673\n",
      "epoch= 130 iteration= 12171 loss= 0.029103660956025124\n",
      "test_data MSELoss:(pred-real)/real= 0.008759750842323734\n",
      "epoch= 131 iteration= 12184 loss= 0.0003773729840759188\n",
      "epoch= 131 iteration= 12204 loss= 0.005251612514257431\n",
      "epoch= 131 iteration= 12224 loss= 0.0017028256552293897\n",
      "epoch= 131 iteration= 12244 loss= 0.001334089320152998\n",
      "epoch= 131 iteration= 12264 loss= 0.029036695137619972\n",
      "test_data MSELoss:(pred-real)/real= 0.008703529718331993\n",
      "epoch= 132 iteration= 12277 loss= 0.0003761611005757004\n",
      "epoch= 132 iteration= 12297 loss= 0.005238474812358618\n",
      "epoch= 132 iteration= 12317 loss= 0.0016972828889265656\n",
      "epoch= 132 iteration= 12337 loss= 0.0013327059568837285\n",
      "epoch= 132 iteration= 12357 loss= 0.028970200568437576\n",
      "test_data MSELoss:(pred-real)/real= 0.008656261391782513\n",
      "epoch= 133 iteration= 12370 loss= 0.0003756017831619829\n",
      "epoch= 133 iteration= 12390 loss= 0.005228783003985882\n",
      "epoch= 133 iteration= 12410 loss= 0.0016930554993450642\n",
      "epoch= 133 iteration= 12430 loss= 0.0013311635702848434\n",
      "epoch= 133 iteration= 12450 loss= 0.028901370242238045\n",
      "test_data MSELoss:(pred-real)/real= 0.008602355994905034\n",
      "epoch= 134 iteration= 12463 loss= 0.0003752073389478028\n",
      "epoch= 134 iteration= 12483 loss= 0.0052192797884345055\n",
      "epoch= 134 iteration= 12503 loss= 0.00168847746681422\n",
      "epoch= 134 iteration= 12523 loss= 0.0013295349199324846\n",
      "epoch= 134 iteration= 12543 loss= 0.028831012547016144\n",
      "test_data MSELoss:(pred-real)/real= 0.00854967579167957\n",
      "epoch= 135 iteration= 12556 loss= 0.0003746317815966904\n",
      "epoch= 135 iteration= 12576 loss= 0.005211149342358112\n",
      "epoch= 135 iteration= 12596 loss= 0.0016867849044501781\n",
      "epoch= 135 iteration= 12616 loss= 0.001327952486462891\n",
      "epoch= 135 iteration= 12636 loss= 0.02875831536948681\n",
      "test_data MSELoss:(pred-real)/real= 0.008501117673909498\n",
      "epoch= 136 iteration= 12649 loss= 0.00037383887683972716\n",
      "epoch= 136 iteration= 12669 loss= 0.005205498542636633\n",
      "epoch= 136 iteration= 12689 loss= 0.0016822414472699165\n",
      "epoch= 136 iteration= 12709 loss= 0.0013266036985442042\n",
      "epoch= 136 iteration= 12729 loss= 0.028696736320853233\n",
      "test_data MSELoss:(pred-real)/real= 0.008447291859839525\n",
      "epoch= 137 iteration= 12742 loss= 0.0003732755722012371\n",
      "epoch= 137 iteration= 12762 loss= 0.00519523024559021\n",
      "epoch= 137 iteration= 12782 loss= 0.0016783797182142735\n",
      "epoch= 137 iteration= 12802 loss= 0.0013249440817162395\n",
      "epoch= 137 iteration= 12822 loss= 0.028619961813092232\n",
      "test_data MSELoss:(pred-real)/real= 0.008395896254417798\n",
      "epoch= 138 iteration= 12835 loss= 0.0003724779235199094\n",
      "epoch= 138 iteration= 12855 loss= 0.0051878150552511215\n",
      "epoch= 138 iteration= 12875 loss= 0.0016737468540668488\n",
      "epoch= 138 iteration= 12895 loss= 0.0013239955296739936\n",
      "epoch= 138 iteration= 12915 loss= 0.028549958020448685\n",
      "test_data MSELoss:(pred-real)/real= 0.00834484214687513\n",
      "epoch= 139 iteration= 12928 loss= 0.00037103256909176707\n",
      "epoch= 139 iteration= 12948 loss= 0.005181456450372934\n",
      "epoch= 139 iteration= 12968 loss= 0.0016712959622964263\n",
      "epoch= 139 iteration= 12988 loss= 0.0013221253175288439\n",
      "epoch= 139 iteration= 13008 loss= 0.028481297194957733\n",
      "test_data MSELoss:(pred-real)/real= 0.008296594120717296\n",
      "epoch= 140 iteration= 13021 loss= 0.00037113740108907223\n",
      "epoch= 140 iteration= 13041 loss= 0.0051713380962610245\n",
      "epoch= 140 iteration= 13061 loss= 0.0016667710151523352\n",
      "epoch= 140 iteration= 13081 loss= 0.0013209242606535554\n",
      "epoch= 140 iteration= 13101 loss= 0.0284020584076643\n",
      "test_data MSELoss:(pred-real)/real= 0.008243449452695333\n",
      "epoch= 141 iteration= 13114 loss= 0.00037043009069748223\n",
      "epoch= 141 iteration= 13134 loss= 0.00516018969938159\n",
      "epoch= 141 iteration= 13154 loss= 0.0016630829777568579\n",
      "epoch= 141 iteration= 13174 loss= 0.0013199325185269117\n",
      "epoch= 141 iteration= 13194 loss= 0.02833828702569008\n",
      "test_data MSELoss:(pred-real)/real= 0.008196941298794828\n",
      "epoch= 142 iteration= 13207 loss= 0.0003702320682350546\n",
      "epoch= 142 iteration= 13227 loss= 0.0051498739048838615\n",
      "epoch= 142 iteration= 13247 loss= 0.0016585030825808644\n",
      "epoch= 142 iteration= 13267 loss= 0.0013181869871914387\n",
      "epoch= 142 iteration= 13287 loss= 0.028262855485081673\n",
      "test_data MSELoss:(pred-real)/real= 0.008149304550089356\n",
      "epoch= 143 iteration= 13300 loss= 0.000369736400898546\n",
      "epoch= 143 iteration= 13320 loss= 0.005140333436429501\n",
      "epoch= 143 iteration= 13340 loss= 0.0016547560226172209\n",
      "epoch= 143 iteration= 13360 loss= 0.001317250425927341\n",
      "epoch= 143 iteration= 13380 loss= 0.028189681470394135\n",
      "test_data MSELoss:(pred-real)/real= 0.00810141142250763\n",
      "epoch= 144 iteration= 13393 loss= 0.00036927248584106565\n",
      "epoch= 144 iteration= 13413 loss= 0.005131126847118139\n",
      "epoch= 144 iteration= 13433 loss= 0.0016507868422195315\n",
      "epoch= 144 iteration= 13453 loss= 0.0013157879002392292\n",
      "epoch= 144 iteration= 13473 loss= 0.02812025137245655\n",
      "test_data MSELoss:(pred-real)/real= 0.008051931521751814\n",
      "epoch= 145 iteration= 13486 loss= 0.0003687030402943492\n",
      "epoch= 145 iteration= 13506 loss= 0.0051271021366119385\n",
      "epoch= 145 iteration= 13526 loss= 0.0016476701712235808\n",
      "epoch= 145 iteration= 13546 loss= 0.001314337132498622\n",
      "epoch= 145 iteration= 13566 loss= 0.0280479546636343\n",
      "test_data MSELoss:(pred-real)/real= 0.008002593473065645\n",
      "epoch= 146 iteration= 13579 loss= 0.0003679284709505737\n",
      "epoch= 146 iteration= 13599 loss= 0.005117851309478283\n",
      "epoch= 146 iteration= 13619 loss= 0.0016429155366495252\n",
      "epoch= 146 iteration= 13639 loss= 0.0013130850857123733\n",
      "epoch= 146 iteration= 13659 loss= 0.027974631637334824\n",
      "test_data MSELoss:(pred-real)/real= 0.007958044504953755\n",
      "epoch= 147 iteration= 13672 loss= 0.0003674458712339401\n",
      "epoch= 147 iteration= 13692 loss= 0.005105788819491863\n",
      "epoch= 147 iteration= 13712 loss= 0.0016379276057705283\n",
      "epoch= 147 iteration= 13732 loss= 0.001311543514020741\n",
      "epoch= 147 iteration= 13752 loss= 0.02790021151304245\n",
      "test_data MSELoss:(pred-real)/real= 0.007909096551075991\n",
      "epoch= 148 iteration= 13765 loss= 0.0003668519202619791\n",
      "epoch= 148 iteration= 13785 loss= 0.00509406765922904\n",
      "epoch= 148 iteration= 13805 loss= 0.001635147724300623\n",
      "epoch= 148 iteration= 13825 loss= 0.0013101694639772177\n",
      "epoch= 148 iteration= 13845 loss= 0.02782623842358589\n",
      "test_data MSELoss:(pred-real)/real= 0.007865242233189443\n",
      "epoch= 149 iteration= 13858 loss= 0.00036677683237940073\n",
      "epoch= 149 iteration= 13878 loss= 0.005093937274068594\n",
      "epoch= 149 iteration= 13898 loss= 0.0016311443177983165\n",
      "epoch= 149 iteration= 13918 loss= 0.0013092660810798407\n",
      "epoch= 149 iteration= 13938 loss= 0.02775561809539795\n",
      "test_data MSELoss:(pred-real)/real= 0.007817051670927968\n",
      "epoch= 150 iteration= 13951 loss= 0.0003662209492176771\n",
      "epoch= 150 iteration= 13971 loss= 0.005079399794340134\n",
      "epoch= 150 iteration= 13991 loss= 0.001627543242648244\n",
      "epoch= 150 iteration= 14011 loss= 0.0013075119350105524\n",
      "epoch= 150 iteration= 14031 loss= 0.02767852693796158\n",
      "test_data MSELoss:(pred-real)/real= 0.0077655801821189625\n",
      "epoch= 151 iteration= 14044 loss= 0.0003652327286545187\n",
      "epoch= 151 iteration= 14064 loss= 0.0050712814554572105\n",
      "epoch= 151 iteration= 14084 loss= 0.0016233237693086267\n",
      "epoch= 151 iteration= 14104 loss= 0.0013068990083411336\n",
      "epoch= 151 iteration= 14124 loss= 0.027599291875958443\n",
      "test_data MSELoss:(pred-real)/real= 0.007721029370764477\n",
      "epoch= 152 iteration= 14137 loss= 0.0003646539989858866\n",
      "epoch= 152 iteration= 14157 loss= 0.005066778045147657\n",
      "epoch= 152 iteration= 14177 loss= 0.0016200884710997343\n",
      "epoch= 152 iteration= 14197 loss= 0.001305626705288887\n",
      "epoch= 152 iteration= 14217 loss= 0.027527205646038055\n",
      "test_data MSELoss:(pred-real)/real= 0.007672785289792551\n",
      "epoch= 153 iteration= 14230 loss= 0.00036395914503373206\n",
      "epoch= 153 iteration= 14250 loss= 0.005060605239123106\n",
      "epoch= 153 iteration= 14270 loss= 0.0016139827203005552\n",
      "epoch= 153 iteration= 14290 loss= 0.001304310979321599\n",
      "epoch= 153 iteration= 14310 loss= 0.0274517722427845\n",
      "test_data MSELoss:(pred-real)/real= 0.007625466303175522\n",
      "epoch= 154 iteration= 14323 loss= 0.0003634811728261411\n",
      "epoch= 154 iteration= 14343 loss= 0.005051657557487488\n",
      "epoch= 154 iteration= 14363 loss= 0.0016114100581035018\n",
      "epoch= 154 iteration= 14383 loss= 0.0013030651025474072\n",
      "epoch= 154 iteration= 14403 loss= 0.027368947863578796\n",
      "test_data MSELoss:(pred-real)/real= 0.0075795237595836324\n",
      "epoch= 155 iteration= 14416 loss= 0.0003630885621532798\n",
      "epoch= 155 iteration= 14436 loss= 0.005042201839387417\n",
      "epoch= 155 iteration= 14456 loss= 0.0016069883713498712\n",
      "epoch= 155 iteration= 14476 loss= 0.0013017444871366024\n",
      "epoch= 155 iteration= 14496 loss= 0.02729722484946251\n",
      "test_data MSELoss:(pred-real)/real= 0.007533548613234113\n",
      "epoch= 156 iteration= 14509 loss= 0.0003626802936196327\n",
      "epoch= 156 iteration= 14529 loss= 0.005031784996390343\n",
      "epoch= 156 iteration= 14549 loss= 0.0016018981114029884\n",
      "epoch= 156 iteration= 14569 loss= 0.0013007486704736948\n",
      "epoch= 156 iteration= 14589 loss= 0.027216538786888123\n",
      "test_data MSELoss:(pred-real)/real= 0.007491627973245664\n",
      "epoch= 157 iteration= 14602 loss= 0.00036248876131139696\n",
      "epoch= 157 iteration= 14622 loss= 0.0050199758261442184\n",
      "epoch= 157 iteration= 14642 loss= 0.0015985851641744375\n",
      "epoch= 157 iteration= 14662 loss= 0.0012992378324270248\n",
      "epoch= 157 iteration= 14682 loss= 0.02713901922106743\n",
      "test_data MSELoss:(pred-real)/real= 0.007446174367537929\n",
      "epoch= 158 iteration= 14695 loss= 0.0003620443749241531\n",
      "epoch= 158 iteration= 14715 loss= 0.005016798619180918\n",
      "epoch= 158 iteration= 14735 loss= 0.00159368384629488\n",
      "epoch= 158 iteration= 14755 loss= 0.001298092189244926\n",
      "epoch= 158 iteration= 14775 loss= 0.02706589549779892\n",
      "test_data MSELoss:(pred-real)/real= 0.007399883832679027\n",
      "epoch= 159 iteration= 14788 loss= 0.00036100411671213806\n",
      "epoch= 159 iteration= 14808 loss= 0.005011478438973427\n",
      "epoch= 159 iteration= 14828 loss= 0.001589402207173407\n",
      "epoch= 159 iteration= 14848 loss= 0.0012973528355360031\n",
      "epoch= 159 iteration= 14868 loss= 0.026993561536073685\n",
      "test_data MSELoss:(pred-real)/real= 0.007357613299973309\n",
      "epoch= 160 iteration= 14881 loss= 0.00035988641320727766\n",
      "epoch= 160 iteration= 14901 loss= 0.0050011589191854\n",
      "epoch= 160 iteration= 14921 loss= 0.0015862437430769205\n",
      "epoch= 160 iteration= 14941 loss= 0.0012961116153746843\n",
      "epoch= 160 iteration= 14961 loss= 0.026910699903964996\n",
      "test_data MSELoss:(pred-real)/real= 0.0073094513500109315\n",
      "epoch= 161 iteration= 14974 loss= 0.0003595294547267258\n",
      "epoch= 161 iteration= 14994 loss= 0.004990728106349707\n",
      "epoch= 161 iteration= 15014 loss= 0.0015834826044738293\n",
      "epoch= 161 iteration= 15034 loss= 0.0012948152143508196\n",
      "epoch= 161 iteration= 15054 loss= 0.026831626892089844\n",
      "test_data MSELoss:(pred-real)/real= 0.007266681654275292\n",
      "epoch= 162 iteration= 15067 loss= 0.00035976566141471267\n",
      "epoch= 162 iteration= 15087 loss= 0.0049846782349050045\n",
      "epoch= 162 iteration= 15107 loss= 0.0015777291264384985\n",
      "epoch= 162 iteration= 15127 loss= 0.0012940670130774379\n",
      "epoch= 162 iteration= 15147 loss= 0.026757478713989258\n",
      "test_data MSELoss:(pred-real)/real= 0.007225664959858275\n",
      "epoch= 163 iteration= 15160 loss= 0.00035907793790102005\n",
      "epoch= 163 iteration= 15180 loss= 0.0049737486988306046\n",
      "epoch= 163 iteration= 15200 loss= 0.0015741157112643123\n",
      "epoch= 163 iteration= 15220 loss= 0.00129211216699332\n",
      "epoch= 163 iteration= 15240 loss= 0.026673389598727226\n",
      "test_data MSELoss:(pred-real)/real= 0.007179271429777145\n",
      "epoch= 164 iteration= 15253 loss= 0.00035773508716374636\n",
      "epoch= 164 iteration= 15273 loss= 0.004967588931322098\n",
      "epoch= 164 iteration= 15293 loss= 0.0015699166106060147\n",
      "epoch= 164 iteration= 15313 loss= 0.001290891319513321\n",
      "epoch= 164 iteration= 15333 loss= 0.026592258363962173\n",
      "test_data MSELoss:(pred-real)/real= 0.007133420433900837\n",
      "epoch= 165 iteration= 15346 loss= 0.0003578919277060777\n",
      "epoch= 165 iteration= 15366 loss= 0.004963306710124016\n",
      "epoch= 165 iteration= 15386 loss= 0.0015668270643800497\n",
      "epoch= 165 iteration= 15406 loss= 0.0012903081951662898\n",
      "epoch= 165 iteration= 15426 loss= 0.026505926623940468\n",
      "test_data MSELoss:(pred-real)/real= 0.007091010791353053\n",
      "epoch= 166 iteration= 15439 loss= 0.00035664779716171324\n",
      "epoch= 166 iteration= 15459 loss= 0.004954822361469269\n",
      "epoch= 166 iteration= 15479 loss= 0.0015613720752298832\n",
      "epoch= 166 iteration= 15499 loss= 0.0012887752382084727\n",
      "epoch= 166 iteration= 15519 loss= 0.026432307437062263\n",
      "test_data MSELoss:(pred-real)/real= 0.007051543539596928\n",
      "epoch= 167 iteration= 15532 loss= 0.0003567172971088439\n",
      "epoch= 167 iteration= 15552 loss= 0.004943900741636753\n",
      "epoch= 167 iteration= 15572 loss= 0.0015585798537358642\n",
      "epoch= 167 iteration= 15592 loss= 0.0012880652211606503\n",
      "epoch= 167 iteration= 15612 loss= 0.026355985552072525\n",
      "test_data MSELoss:(pred-real)/real= 0.0070070585675744545\n",
      "epoch= 168 iteration= 15625 loss= 0.0003570565895643085\n",
      "epoch= 168 iteration= 15645 loss= 0.004937236662954092\n",
      "epoch= 168 iteration= 15665 loss= 0.00155515200458467\n",
      "epoch= 168 iteration= 15685 loss= 0.0012868508929386735\n",
      "epoch= 168 iteration= 15705 loss= 0.026271428912878036\n",
      "test_data MSELoss:(pred-real)/real= 0.006965899704179416\n",
      "epoch= 169 iteration= 15718 loss= 0.0003564840299077332\n",
      "epoch= 169 iteration= 15738 loss= 0.004929458722472191\n",
      "epoch= 169 iteration= 15758 loss= 0.0015498851425945759\n",
      "epoch= 169 iteration= 15778 loss= 0.0012857508845627308\n",
      "epoch= 169 iteration= 15798 loss= 0.026182513684034348\n",
      "test_data MSELoss:(pred-real)/real= 0.006924285087734461\n",
      "epoch= 170 iteration= 15811 loss= 0.00035555328940972686\n",
      "epoch= 170 iteration= 15831 loss= 0.00492128124460578\n",
      "epoch= 170 iteration= 15851 loss= 0.0015458513516932726\n",
      "epoch= 170 iteration= 15871 loss= 0.001284778118133545\n",
      "epoch= 170 iteration= 15891 loss= 0.026108788326382637\n",
      "test_data MSELoss:(pred-real)/real= 0.006884790724143386\n",
      "epoch= 171 iteration= 15904 loss= 0.0003549835237208754\n",
      "epoch= 171 iteration= 15924 loss= 0.00491663534194231\n",
      "epoch= 171 iteration= 15944 loss= 0.0015424112789332867\n",
      "epoch= 171 iteration= 15964 loss= 0.0012835654197260737\n",
      "epoch= 171 iteration= 15984 loss= 0.026030104607343674\n",
      "test_data MSELoss:(pred-real)/real= 0.006845125553405119\n",
      "epoch= 172 iteration= 15997 loss= 0.00035458404454402626\n",
      "epoch= 172 iteration= 16017 loss= 0.004908308386802673\n",
      "epoch= 172 iteration= 16037 loss= 0.00153660390060395\n",
      "epoch= 172 iteration= 16057 loss= 0.001282100798562169\n",
      "epoch= 172 iteration= 16077 loss= 0.025948915630578995\n",
      "test_data MSELoss:(pred-real)/real= 0.006802240186112208\n",
      "epoch= 173 iteration= 16090 loss= 0.0003544175415299833\n",
      "epoch= 173 iteration= 16110 loss= 0.004900409374386072\n",
      "epoch= 173 iteration= 16130 loss= 0.0015335348434746265\n",
      "epoch= 173 iteration= 16150 loss= 0.0012815366499125957\n",
      "epoch= 173 iteration= 16170 loss= 0.025859255343675613\n",
      "test_data MSELoss:(pred-real)/real= 0.006758768370168077\n",
      "epoch= 174 iteration= 16183 loss= 0.0003537488228175789\n",
      "epoch= 174 iteration= 16203 loss= 0.004893861711025238\n",
      "epoch= 174 iteration= 16223 loss= 0.0015306116547435522\n",
      "epoch= 174 iteration= 16243 loss= 0.0012803671415895224\n",
      "epoch= 174 iteration= 16263 loss= 0.02577435038983822\n",
      "test_data MSELoss:(pred-real)/real= 0.006717550768775659\n",
      "epoch= 175 iteration= 16276 loss= 0.00035296511487104\n",
      "epoch= 175 iteration= 16296 loss= 0.004888590890914202\n",
      "epoch= 175 iteration= 16316 loss= 0.0015260223299264908\n",
      "epoch= 175 iteration= 16336 loss= 0.0012791694607585669\n",
      "epoch= 175 iteration= 16356 loss= 0.02569028176367283\n",
      "test_data MSELoss:(pred-real)/real= 0.006677749857772142\n",
      "epoch= 176 iteration= 16369 loss= 0.00035283309989608824\n",
      "epoch= 176 iteration= 16389 loss= 0.004875890910625458\n",
      "epoch= 176 iteration= 16409 loss= 0.0015220679342746735\n",
      "epoch= 176 iteration= 16429 loss= 0.0012786035658791661\n",
      "epoch= 176 iteration= 16449 loss= 0.025602735579013824\n",
      "test_data MSELoss:(pred-real)/real= 0.006639633133697013\n",
      "epoch= 177 iteration= 16462 loss= 0.00035231385845690966\n",
      "epoch= 177 iteration= 16482 loss= 0.00487030204385519\n",
      "epoch= 177 iteration= 16502 loss= 0.0015171982813626528\n",
      "epoch= 177 iteration= 16522 loss= 0.001277473522350192\n",
      "epoch= 177 iteration= 16542 loss= 0.02552631124854088\n",
      "test_data MSELoss:(pred-real)/real= 0.006601297387128903\n",
      "epoch= 178 iteration= 16555 loss= 0.0003525935171637684\n",
      "epoch= 178 iteration= 16575 loss= 0.004862201400101185\n",
      "epoch= 178 iteration= 16595 loss= 0.0015143188647925854\n",
      "epoch= 178 iteration= 16615 loss= 0.0012765461578965187\n",
      "epoch= 178 iteration= 16635 loss= 0.025441579520702362\n",
      "test_data MSELoss:(pred-real)/real= 0.006561825239461743\n",
      "epoch= 179 iteration= 16648 loss= 0.00035197482793591917\n",
      "epoch= 179 iteration= 16668 loss= 0.0048555294051766396\n",
      "epoch= 179 iteration= 16688 loss= 0.0015100205782800913\n",
      "epoch= 179 iteration= 16708 loss= 0.0012753952760249376\n",
      "epoch= 179 iteration= 16728 loss= 0.02535504475235939\n",
      "test_data MSELoss:(pred-real)/real= 0.006521831020816333\n",
      "epoch= 180 iteration= 16741 loss= 0.00035155925434082747\n",
      "epoch= 180 iteration= 16761 loss= 0.004845569375902414\n",
      "epoch= 180 iteration= 16781 loss= 0.0015043902676552534\n",
      "epoch= 180 iteration= 16801 loss= 0.0012740070233121514\n",
      "epoch= 180 iteration= 16821 loss= 0.02527366206049919\n",
      "test_data MSELoss:(pred-real)/real= 0.006483704841230065\n",
      "epoch= 181 iteration= 16834 loss= 0.0003513472038321197\n",
      "epoch= 181 iteration= 16854 loss= 0.004841589368879795\n",
      "epoch= 181 iteration= 16874 loss= 0.001501758466474712\n",
      "epoch= 181 iteration= 16894 loss= 0.0012735414784401655\n",
      "epoch= 181 iteration= 16914 loss= 0.025199774652719498\n",
      "test_data MSELoss:(pred-real)/real= 0.006448498545473235\n",
      "epoch= 182 iteration= 16927 loss= 0.0003516883007250726\n",
      "epoch= 182 iteration= 16947 loss= 0.004829683806747198\n",
      "epoch= 182 iteration= 16967 loss= 0.0014971462078392506\n",
      "epoch= 182 iteration= 16987 loss= 0.0012723332038149238\n",
      "epoch= 182 iteration= 17007 loss= 0.025106539949774742\n",
      "test_data MSELoss:(pred-real)/real= 0.006408302921853546\n",
      "epoch= 183 iteration= 17020 loss= 0.00035057030618190765\n",
      "epoch= 183 iteration= 17040 loss= 0.004822373855859041\n",
      "epoch= 183 iteration= 17060 loss= 0.0014944624854251742\n",
      "epoch= 183 iteration= 17080 loss= 0.0012710947776213288\n",
      "epoch= 183 iteration= 17100 loss= 0.02502422407269478\n",
      "test_data MSELoss:(pred-real)/real= 0.006369343447861158\n",
      "epoch= 184 iteration= 17113 loss= 0.0003504614869598299\n",
      "epoch= 184 iteration= 17133 loss= 0.004819124471396208\n",
      "epoch= 184 iteration= 17153 loss= 0.0014910397585481405\n",
      "epoch= 184 iteration= 17173 loss= 0.0012702792882919312\n",
      "epoch= 184 iteration= 17193 loss= 0.02494060806930065\n",
      "test_data MSELoss:(pred-real)/real= 0.006327187668325173\n",
      "epoch= 185 iteration= 17206 loss= 0.0003497366269584745\n",
      "epoch= 185 iteration= 17226 loss= 0.0048132785595953465\n",
      "epoch= 185 iteration= 17246 loss= 0.001486100722104311\n",
      "epoch= 185 iteration= 17266 loss= 0.0012698150239884853\n",
      "epoch= 185 iteration= 17286 loss= 0.02485247701406479\n",
      "test_data MSELoss:(pred-real)/real= 0.00629021760283245\n",
      "epoch= 186 iteration= 17299 loss= 0.00034943956416100264\n",
      "epoch= 186 iteration= 17319 loss= 0.004801617935299873\n",
      "epoch= 186 iteration= 17339 loss= 0.0014824534300714731\n",
      "epoch= 186 iteration= 17359 loss= 0.0012684878893196583\n",
      "epoch= 186 iteration= 17379 loss= 0.024761363863945007\n",
      "test_data MSELoss:(pred-real)/real= 0.006253951998789691\n",
      "epoch= 187 iteration= 17392 loss= 0.00034912145929411054\n",
      "epoch= 187 iteration= 17412 loss= 0.004791440442204475\n",
      "epoch= 187 iteration= 17432 loss= 0.0014767949469387531\n",
      "epoch= 187 iteration= 17452 loss= 0.0012674012687057257\n",
      "epoch= 187 iteration= 17472 loss= 0.024681303650140762\n",
      "test_data MSELoss:(pred-real)/real= 0.006218283605347905\n",
      "epoch= 188 iteration= 17485 loss= 0.0003498154692351818\n",
      "epoch= 188 iteration= 17505 loss= 0.004779555834829807\n",
      "epoch= 188 iteration= 17525 loss= 0.0014728178502991796\n",
      "epoch= 188 iteration= 17545 loss= 0.001266741193830967\n",
      "epoch= 188 iteration= 17565 loss= 0.024589719250798225\n",
      "test_data MSELoss:(pred-real)/real= 0.006181364846674519\n",
      "epoch= 189 iteration= 17578 loss= 0.00035013409797102213\n",
      "epoch= 189 iteration= 17598 loss= 0.004775486420840025\n",
      "epoch= 189 iteration= 17618 loss= 0.0014696128200739622\n",
      "epoch= 189 iteration= 17638 loss= 0.0012659558560699224\n",
      "epoch= 189 iteration= 17658 loss= 0.02450481429696083\n",
      "test_data MSELoss:(pred-real)/real= 0.006145306604190005\n",
      "epoch= 190 iteration= 17671 loss= 0.0003488576039671898\n",
      "epoch= 190 iteration= 17691 loss= 0.004762769211083651\n",
      "epoch= 190 iteration= 17711 loss= 0.0014652885729447007\n",
      "epoch= 190 iteration= 17731 loss= 0.001265213591977954\n",
      "epoch= 190 iteration= 17751 loss= 0.02442694641649723\n",
      "test_data MSELoss:(pred-real)/real= 0.006112379946797673\n",
      "epoch= 191 iteration= 17764 loss= 0.00034858082653954625\n",
      "epoch= 191 iteration= 17784 loss= 0.004758594091981649\n",
      "epoch= 191 iteration= 17804 loss= 0.0014618959976360202\n",
      "epoch= 191 iteration= 17824 loss= 0.0012642897199839354\n",
      "epoch= 191 iteration= 17844 loss= 0.024336248636245728\n",
      "test_data MSELoss:(pred-real)/real= 0.006073327101249661\n",
      "epoch= 192 iteration= 17857 loss= 0.0003486162459012121\n",
      "epoch= 192 iteration= 17877 loss= 0.004747635684907436\n",
      "epoch= 192 iteration= 17897 loss= 0.0014576420653611422\n",
      "epoch= 192 iteration= 17917 loss= 0.001263628015294671\n",
      "epoch= 192 iteration= 17937 loss= 0.024251611903309822\n",
      "test_data MSELoss:(pred-real)/real= 0.0060379456392385894\n",
      "epoch= 193 iteration= 17950 loss= 0.0003485673223622143\n",
      "epoch= 193 iteration= 17970 loss= 0.004738548304885626\n",
      "epoch= 193 iteration= 17990 loss= 0.001453355303965509\n",
      "epoch= 193 iteration= 18010 loss= 0.0012623823713511229\n",
      "epoch= 193 iteration= 18030 loss= 0.024170195683836937\n",
      "test_data MSELoss:(pred-real)/real= 0.006004297562564413\n",
      "epoch= 194 iteration= 18043 loss= 0.0003481814346741885\n",
      "epoch= 194 iteration= 18063 loss= 0.004731625318527222\n",
      "epoch= 194 iteration= 18083 loss= 0.0014500556280836463\n",
      "epoch= 194 iteration= 18103 loss= 0.0012616077437996864\n",
      "epoch= 194 iteration= 18123 loss= 0.024090321734547615\n",
      "test_data MSELoss:(pred-real)/real= 0.005968605084086044\n",
      "epoch= 195 iteration= 18136 loss= 0.0003482853644527495\n",
      "epoch= 195 iteration= 18156 loss= 0.004722613375633955\n",
      "epoch= 195 iteration= 18176 loss= 0.0014460470993071795\n",
      "epoch= 195 iteration= 18196 loss= 0.0012611933052539825\n",
      "epoch= 195 iteration= 18216 loss= 0.02400173246860504\n",
      "test_data MSELoss:(pred-real)/real= 0.005935796180791739\n",
      "epoch= 196 iteration= 18229 loss= 0.00034827503259293735\n",
      "epoch= 196 iteration= 18249 loss= 0.0047139055095613\n",
      "epoch= 196 iteration= 18269 loss= 0.0014413766330108047\n",
      "epoch= 196 iteration= 18289 loss= 0.0012598640751093626\n",
      "epoch= 196 iteration= 18309 loss= 0.023915473371744156\n",
      "test_data MSELoss:(pred-real)/real= 0.005901620024815202\n",
      "epoch= 197 iteration= 18322 loss= 0.0003483672044239938\n",
      "epoch= 197 iteration= 18342 loss= 0.00470268540084362\n",
      "epoch= 197 iteration= 18362 loss= 0.0014385048998519778\n",
      "epoch= 197 iteration= 18382 loss= 0.0012589951511472464\n",
      "epoch= 197 iteration= 18402 loss= 0.023830730468034744\n",
      "test_data MSELoss:(pred-real)/real= 0.005867894350861509\n",
      "epoch= 198 iteration= 18415 loss= 0.0003483207547105849\n",
      "epoch= 198 iteration= 18435 loss= 0.004694090224802494\n",
      "epoch= 198 iteration= 18455 loss= 0.0014340878697112203\n",
      "epoch= 198 iteration= 18475 loss= 0.0012584410142153502\n",
      "epoch= 198 iteration= 18495 loss= 0.023746633902192116\n",
      "test_data MSELoss:(pred-real)/real= 0.0058332475148037905\n",
      "epoch= 199 iteration= 18508 loss= 0.0003479848674032837\n",
      "epoch= 199 iteration= 18528 loss= 0.004686648026108742\n",
      "epoch= 199 iteration= 18548 loss= 0.0014298413880169392\n",
      "epoch= 199 iteration= 18568 loss= 0.001257475116290152\n",
      "epoch= 199 iteration= 18588 loss= 0.023661307990550995\n",
      "test_data MSELoss:(pred-real)/real= 0.005802328058052808\n",
      "epoch= 200 iteration= 18601 loss= 0.0003486753557808697\n",
      "epoch= 200 iteration= 18621 loss= 0.00467671500518918\n",
      "epoch= 200 iteration= 18641 loss= 0.001426334260031581\n",
      "epoch= 200 iteration= 18661 loss= 0.0012567787198349833\n",
      "epoch= 200 iteration= 18681 loss= 0.023575399070978165\n",
      "test_data MSELoss:(pred-real)/real= 0.005768392817117274\n",
      "epoch= 201 iteration= 18694 loss= 0.0003490638337098062\n",
      "epoch= 201 iteration= 18714 loss= 0.004669290967285633\n",
      "epoch= 201 iteration= 18734 loss= 0.0014224700862541795\n",
      "epoch= 201 iteration= 18754 loss= 0.0012554789427667856\n",
      "epoch= 201 iteration= 18774 loss= 0.0234859436750412\n",
      "test_data MSELoss:(pred-real)/real= 0.005733850457343376\n",
      "epoch= 202 iteration= 18787 loss= 0.00034938627504743636\n",
      "epoch= 202 iteration= 18807 loss= 0.004662400111556053\n",
      "epoch= 202 iteration= 18827 loss= 0.0014184066094458103\n",
      "epoch= 202 iteration= 18847 loss= 0.0012554751010611653\n",
      "epoch= 202 iteration= 18867 loss= 0.023404348641633987\n",
      "test_data MSELoss:(pred-real)/real= 0.005702944394821922\n",
      "epoch= 203 iteration= 18880 loss= 0.00034892125404439867\n",
      "epoch= 203 iteration= 18900 loss= 0.004646860063076019\n",
      "epoch= 203 iteration= 18920 loss= 0.001413162099197507\n",
      "epoch= 203 iteration= 18940 loss= 0.0012545494828373194\n",
      "epoch= 203 iteration= 18960 loss= 0.02332065999507904\n",
      "test_data MSELoss:(pred-real)/real= 0.005668337540959733\n",
      "epoch= 204 iteration= 18973 loss= 0.0003497657598927617\n",
      "epoch= 204 iteration= 18993 loss= 0.004636033903807402\n",
      "epoch= 204 iteration= 19013 loss= 0.0014094256330281496\n",
      "epoch= 204 iteration= 19033 loss= 0.0012538605369627476\n",
      "epoch= 204 iteration= 19053 loss= 0.02324048802256584\n",
      "test_data MSELoss:(pred-real)/real= 0.00563755751742671\n",
      "epoch= 205 iteration= 19066 loss= 0.0003504199266899377\n",
      "epoch= 205 iteration= 19086 loss= 0.004629657603800297\n",
      "epoch= 205 iteration= 19106 loss= 0.0014068942982703447\n",
      "epoch= 205 iteration= 19126 loss= 0.0012530277017503977\n",
      "epoch= 205 iteration= 19146 loss= 0.023148898035287857\n",
      "test_data MSELoss:(pred-real)/real= 0.005607291668032606\n",
      "epoch= 206 iteration= 19159 loss= 0.0003486911882646382\n",
      "epoch= 206 iteration= 19179 loss= 0.004623418673872948\n",
      "epoch= 206 iteration= 19199 loss= 0.0014019691152498126\n",
      "epoch= 206 iteration= 19219 loss= 0.001252293004654348\n",
      "epoch= 206 iteration= 19239 loss= 0.023067761212587357\n",
      "test_data MSELoss:(pred-real)/real= 0.005578136964080234\n",
      "epoch= 207 iteration= 19252 loss= 0.00034986744867637753\n",
      "epoch= 207 iteration= 19272 loss= 0.004611183423548937\n",
      "epoch= 207 iteration= 19292 loss= 0.0013987499987706542\n",
      "epoch= 207 iteration= 19312 loss= 0.0012512567918747663\n",
      "epoch= 207 iteration= 19332 loss= 0.022990314289927483\n",
      "test_data MSELoss:(pred-real)/real= 0.005549117806367576\n",
      "epoch= 208 iteration= 19345 loss= 0.00035026759724132717\n",
      "epoch= 208 iteration= 19365 loss= 0.004603751935064793\n",
      "epoch= 208 iteration= 19385 loss= 0.0013946297112852335\n",
      "epoch= 208 iteration= 19405 loss= 0.0012502791360020638\n",
      "epoch= 208 iteration= 19425 loss= 0.022912390530109406\n",
      "test_data MSELoss:(pred-real)/real= 0.005518596224849009\n",
      "epoch= 209 iteration= 19438 loss= 0.0003503566258586943\n",
      "epoch= 209 iteration= 19458 loss= 0.004590073134750128\n",
      "epoch= 209 iteration= 19478 loss= 0.0013895367737859488\n",
      "epoch= 209 iteration= 19498 loss= 0.0012495324481278658\n",
      "epoch= 209 iteration= 19518 loss= 0.022826597094535828\n",
      "test_data MSELoss:(pred-real)/real= 0.005490036383788619\n",
      "epoch= 210 iteration= 19531 loss= 0.00035100849345326424\n",
      "epoch= 210 iteration= 19551 loss= 0.004582708235830069\n",
      "epoch= 210 iteration= 19571 loss= 0.0013858025195077062\n",
      "epoch= 210 iteration= 19591 loss= 0.0012495111441239715\n",
      "epoch= 210 iteration= 19611 loss= 0.022743480280041695\n",
      "test_data MSELoss:(pred-real)/real= 0.0054589121718890965\n",
      "epoch= 211 iteration= 19624 loss= 0.0003509701055008918\n",
      "epoch= 211 iteration= 19644 loss= 0.004569896496832371\n",
      "epoch= 211 iteration= 19664 loss= 0.0013819196028634906\n",
      "epoch= 211 iteration= 19684 loss= 0.0012486116029322147\n",
      "epoch= 211 iteration= 19704 loss= 0.022659379988908768\n",
      "test_data MSELoss:(pred-real)/real= 0.005428607232816931\n",
      "epoch= 212 iteration= 19717 loss= 0.00035109781310893595\n",
      "epoch= 212 iteration= 19737 loss= 0.004565763287246227\n",
      "epoch= 212 iteration= 19757 loss= 0.0013775252737104893\n",
      "epoch= 212 iteration= 19777 loss= 0.0012481367448344827\n",
      "epoch= 212 iteration= 19797 loss= 0.02258271723985672\n",
      "test_data MSELoss:(pred-real)/real= 0.00540092396032479\n",
      "epoch= 213 iteration= 19810 loss= 0.00035188146284781396\n",
      "epoch= 213 iteration= 19830 loss= 0.004554021172225475\n",
      "epoch= 213 iteration= 19850 loss= 0.001374584622681141\n",
      "epoch= 213 iteration= 19870 loss= 0.0012476823758333921\n",
      "epoch= 213 iteration= 19890 loss= 0.022493984550237656\n",
      "test_data MSELoss:(pred-real)/real= 0.0053719848252108526\n",
      "epoch= 214 iteration= 19903 loss= 0.00035248612402938306\n",
      "epoch= 214 iteration= 19923 loss= 0.0045424560084939\n",
      "epoch= 214 iteration= 19943 loss= 0.0013705515302717686\n",
      "epoch= 214 iteration= 19963 loss= 0.0012465913314372301\n",
      "epoch= 214 iteration= 19983 loss= 0.02241801843047142\n",
      "test_data MSELoss:(pred-real)/real= 0.0053437096778199905\n",
      "epoch= 215 iteration= 19996 loss= 0.0003539377066772431\n",
      "epoch= 215 iteration= 20016 loss= 0.004533959552645683\n",
      "epoch= 215 iteration= 20036 loss= 0.0013670192565768957\n",
      "epoch= 215 iteration= 20056 loss= 0.0012457275297492743\n",
      "epoch= 215 iteration= 20076 loss= 0.02233213186264038\n",
      "test_data MSELoss:(pred-real)/real= 0.005316268652677536\n",
      "epoch= 216 iteration= 20089 loss= 0.0003536580188665539\n",
      "epoch= 216 iteration= 20109 loss= 0.004526006057858467\n",
      "epoch= 216 iteration= 20129 loss= 0.001362886861898005\n",
      "epoch= 216 iteration= 20149 loss= 0.0012457952834665775\n",
      "epoch= 216 iteration= 20169 loss= 0.02225131168961525\n",
      "test_data MSELoss:(pred-real)/real= 0.00529051690440004\n",
      "epoch= 217 iteration= 20182 loss= 0.0003537584561854601\n",
      "epoch= 217 iteration= 20202 loss= 0.004514412488788366\n",
      "epoch= 217 iteration= 20222 loss= 0.001360731665045023\n",
      "epoch= 217 iteration= 20242 loss= 0.0012445133179426193\n",
      "epoch= 217 iteration= 20262 loss= 0.02217426896095276\n",
      "test_data MSELoss:(pred-real)/real= 0.00526349159837183\n",
      "epoch= 218 iteration= 20275 loss= 0.0003551786649040878\n",
      "epoch= 218 iteration= 20295 loss= 0.004504406824707985\n",
      "epoch= 218 iteration= 20315 loss= 0.0013561202213168144\n",
      "epoch= 218 iteration= 20335 loss= 0.0012443948071449995\n",
      "epoch= 218 iteration= 20355 loss= 0.022095423191785812\n",
      "test_data MSELoss:(pred-real)/real= 0.005237930732417024\n",
      "epoch= 219 iteration= 20368 loss= 0.00035642809234559536\n",
      "epoch= 219 iteration= 20388 loss= 0.0044931997545063496\n",
      "epoch= 219 iteration= 20408 loss= 0.0013532743323594332\n",
      "epoch= 219 iteration= 20428 loss= 0.0012437195982784033\n",
      "epoch= 219 iteration= 20448 loss= 0.022019371390342712\n",
      "test_data MSELoss:(pred-real)/real= 0.005210730693458269\n",
      "epoch= 220 iteration= 20461 loss= 0.00035705126356333494\n",
      "epoch= 220 iteration= 20481 loss= 0.004481591749936342\n",
      "epoch= 220 iteration= 20501 loss= 0.001349987811408937\n",
      "epoch= 220 iteration= 20521 loss= 0.0012430613860487938\n",
      "epoch= 220 iteration= 20541 loss= 0.02193550392985344\n",
      "test_data MSELoss:(pred-real)/real= 0.0051861088787619435\n",
      "epoch= 221 iteration= 20554 loss= 0.0003587524697650224\n",
      "epoch= 221 iteration= 20574 loss= 0.0044712405651807785\n",
      "epoch= 221 iteration= 20594 loss= 0.001345270313322544\n",
      "epoch= 221 iteration= 20614 loss= 0.0012423479929566383\n",
      "epoch= 221 iteration= 20634 loss= 0.02185915783047676\n",
      "test_data MSELoss:(pred-real)/real= 0.00516187113761488\n",
      "epoch= 222 iteration= 20647 loss= 0.0003592480788938701\n",
      "epoch= 222 iteration= 20667 loss= 0.0044568851590156555\n",
      "epoch= 222 iteration= 20687 loss= 0.00134152895770967\n",
      "epoch= 222 iteration= 20707 loss= 0.001241675578057766\n",
      "epoch= 222 iteration= 20727 loss= 0.021778283640742302\n",
      "test_data MSELoss:(pred-real)/real= 0.005138028859316061\n",
      "epoch= 223 iteration= 20740 loss= 0.00036056165117770433\n",
      "epoch= 223 iteration= 20760 loss= 0.004447295796126127\n",
      "epoch= 223 iteration= 20780 loss= 0.0013382177567109466\n",
      "epoch= 223 iteration= 20800 loss= 0.0012411735951900482\n",
      "epoch= 223 iteration= 20820 loss= 0.021700043231248856\n",
      "test_data MSELoss:(pred-real)/real= 0.005113663533443792\n",
      "epoch= 224 iteration= 20833 loss= 0.00035982602275907993\n",
      "epoch= 224 iteration= 20853 loss= 0.004439915996044874\n",
      "epoch= 224 iteration= 20873 loss= 0.0013340467121452093\n",
      "epoch= 224 iteration= 20893 loss= 0.0012404108420014381\n",
      "epoch= 224 iteration= 20913 loss= 0.021627498790621758\n",
      "test_data MSELoss:(pred-real)/real= 0.005087543660516126\n",
      "epoch= 225 iteration= 20926 loss= 0.00036133109824731946\n",
      "epoch= 225 iteration= 20946 loss= 0.004429666791111231\n",
      "epoch= 225 iteration= 20966 loss= 0.0013298997655510902\n",
      "epoch= 225 iteration= 20986 loss= 0.0012400442501530051\n",
      "epoch= 225 iteration= 21006 loss= 0.021547459065914154\n",
      "test_data MSELoss:(pred-real)/real= 0.005064140203305417\n",
      "epoch= 226 iteration= 21019 loss= 0.0003627431869972497\n",
      "epoch= 226 iteration= 21039 loss= 0.004420335404574871\n",
      "epoch= 226 iteration= 21059 loss= 0.001327628269791603\n",
      "epoch= 226 iteration= 21079 loss= 0.0012394553050398827\n",
      "epoch= 226 iteration= 21099 loss= 0.02147173136472702\n",
      "test_data MSELoss:(pred-real)/real= 0.00504056503996253\n",
      "epoch= 227 iteration= 21112 loss= 0.00036243186332285404\n",
      "epoch= 227 iteration= 21132 loss= 0.004411135800182819\n",
      "epoch= 227 iteration= 21152 loss= 0.001324847573414445\n",
      "epoch= 227 iteration= 21172 loss= 0.001239231787621975\n",
      "epoch= 227 iteration= 21192 loss= 0.02139999158680439\n",
      "test_data MSELoss:(pred-real)/real= 0.005017887994957467\n",
      "epoch= 228 iteration= 21205 loss= 0.0003646894474513829\n",
      "epoch= 228 iteration= 21225 loss= 0.00439833477139473\n",
      "epoch= 228 iteration= 21245 loss= 0.0013195734936743975\n",
      "epoch= 228 iteration= 21265 loss= 0.0012385663576424122\n",
      "epoch= 228 iteration= 21285 loss= 0.021325360983610153\n",
      "test_data MSELoss:(pred-real)/real= 0.004995122791216191\n",
      "epoch= 229 iteration= 21298 loss= 0.00036544306203722954\n",
      "epoch= 229 iteration= 21318 loss= 0.004386462736874819\n",
      "epoch= 229 iteration= 21338 loss= 0.0013167929137125611\n",
      "epoch= 229 iteration= 21358 loss= 0.0012379909167066216\n",
      "epoch= 229 iteration= 21378 loss= 0.021249817684292793\n",
      "test_data MSELoss:(pred-real)/real= 0.004972802015901025\n",
      "epoch= 230 iteration= 21391 loss= 0.0003664633259177208\n",
      "epoch= 230 iteration= 21411 loss= 0.004376335069537163\n",
      "epoch= 230 iteration= 21431 loss= 0.0013130197767168283\n",
      "epoch= 230 iteration= 21451 loss= 0.0012375274673104286\n",
      "epoch= 230 iteration= 21471 loss= 0.021179985255002975\n",
      "test_data MSELoss:(pred-real)/real= 0.0049521623037031125\n",
      "epoch= 231 iteration= 21484 loss= 0.0003675841144286096\n",
      "epoch= 231 iteration= 21504 loss= 0.004366155248135328\n",
      "epoch= 231 iteration= 21524 loss= 0.0013099330244585872\n",
      "epoch= 231 iteration= 21544 loss= 0.0012371453922241926\n",
      "epoch= 231 iteration= 21564 loss= 0.021104183048009872\n",
      "test_data MSELoss:(pred-real)/real= 0.004929589269320584\n",
      "epoch= 232 iteration= 21577 loss= 0.0003690655285026878\n",
      "epoch= 232 iteration= 21597 loss= 0.004355725832283497\n",
      "epoch= 232 iteration= 21617 loss= 0.0013067512772977352\n",
      "epoch= 232 iteration= 21637 loss= 0.0012367521412670612\n",
      "epoch= 232 iteration= 21657 loss= 0.021032264456152916\n",
      "test_data MSELoss:(pred-real)/real= 0.004908196388795558\n",
      "epoch= 233 iteration= 21670 loss= 0.0003696261555887759\n",
      "epoch= 233 iteration= 21690 loss= 0.00434526801109314\n",
      "epoch= 233 iteration= 21710 loss= 0.0013032322749495506\n",
      "epoch= 233 iteration= 21730 loss= 0.0012359528336673975\n",
      "epoch= 233 iteration= 21750 loss= 0.020959313958883286\n",
      "test_data MSELoss:(pred-real)/real= 0.004885614610328857\n",
      "epoch= 234 iteration= 21763 loss= 0.0003690167795866728\n",
      "epoch= 234 iteration= 21783 loss= 0.004335305653512478\n",
      "epoch= 234 iteration= 21803 loss= 0.0012989030219614506\n",
      "epoch= 234 iteration= 21823 loss= 0.0012360154651105404\n",
      "epoch= 234 iteration= 21843 loss= 0.020887214690446854\n",
      "test_data MSELoss:(pred-real)/real= 0.004865741454220067\n",
      "epoch= 235 iteration= 21856 loss= 0.00037088902899995446\n",
      "epoch= 235 iteration= 21876 loss= 0.00432476494461298\n",
      "epoch= 235 iteration= 21896 loss= 0.0012963777408003807\n",
      "epoch= 235 iteration= 21916 loss= 0.0012349225580692291\n",
      "epoch= 235 iteration= 21936 loss= 0.020814359188079834\n",
      "test_data MSELoss:(pred-real)/real= 0.004845315247722384\n",
      "epoch= 236 iteration= 21949 loss= 0.0003724469861481339\n",
      "epoch= 236 iteration= 21969 loss= 0.004312431439757347\n",
      "epoch= 236 iteration= 21989 loss= 0.0012933106627315283\n",
      "epoch= 236 iteration= 22009 loss= 0.0012343400157988071\n",
      "epoch= 236 iteration= 22029 loss= 0.020747408270835876\n",
      "test_data MSELoss:(pred-real)/real= 0.004825833363510255\n",
      "epoch= 237 iteration= 22042 loss= 0.0003750033210963011\n",
      "epoch= 237 iteration= 22062 loss= 0.004301300272345543\n",
      "epoch= 237 iteration= 22082 loss= 0.0012893682578578591\n",
      "epoch= 237 iteration= 22102 loss= 0.0012332196347415447\n",
      "epoch= 237 iteration= 22122 loss= 0.020670399069786072\n",
      "test_data MSELoss:(pred-real)/real= 0.004804687975491915\n",
      "epoch= 238 iteration= 22135 loss= 0.00037599774077534676\n",
      "epoch= 238 iteration= 22155 loss= 0.00429064966738224\n",
      "epoch= 238 iteration= 22175 loss= 0.001285485108383\n",
      "epoch= 238 iteration= 22195 loss= 0.0012335889041423798\n",
      "epoch= 238 iteration= 22215 loss= 0.02060067653656006\n",
      "test_data MSELoss:(pred-real)/real= 0.004784422771384318\n",
      "epoch= 239 iteration= 22228 loss= 0.0003776657395064831\n",
      "epoch= 239 iteration= 22248 loss= 0.004283098503947258\n",
      "epoch= 239 iteration= 22268 loss= 0.0012824579607695341\n",
      "epoch= 239 iteration= 22288 loss= 0.001232603914104402\n",
      "epoch= 239 iteration= 22308 loss= 0.020531609654426575\n",
      "test_data MSELoss:(pred-real)/real= 0.0047646340422539245\n",
      "epoch= 240 iteration= 22321 loss= 0.0003784596920013428\n",
      "epoch= 240 iteration= 22341 loss= 0.004268604330718517\n",
      "epoch= 240 iteration= 22361 loss= 0.0012790755135938525\n",
      "epoch= 240 iteration= 22381 loss= 0.0012319820234552026\n",
      "epoch= 240 iteration= 22401 loss= 0.02046051062643528\n",
      "test_data MSELoss:(pred-real)/real= 0.004745136125064973\n",
      "epoch= 241 iteration= 22414 loss= 0.00037995525053702295\n",
      "epoch= 241 iteration= 22434 loss= 0.004258389584720135\n",
      "epoch= 241 iteration= 22454 loss= 0.0012763753766193986\n",
      "epoch= 241 iteration= 22474 loss= 0.001231784000992775\n",
      "epoch= 241 iteration= 22494 loss= 0.02039450779557228\n",
      "test_data MSELoss:(pred-real)/real= 0.004725652598103302\n",
      "epoch= 242 iteration= 22507 loss= 0.0003814456285908818\n",
      "epoch= 242 iteration= 22527 loss= 0.004248479381203651\n",
      "epoch= 242 iteration= 22547 loss= 0.0012738846708089113\n",
      "epoch= 242 iteration= 22567 loss= 0.001231420785188675\n",
      "epoch= 242 iteration= 22587 loss= 0.02033150941133499\n",
      "test_data MSELoss:(pred-real)/real= 0.004707301546457327\n",
      "epoch= 243 iteration= 22600 loss= 0.0003826794563792646\n",
      "epoch= 243 iteration= 22620 loss= 0.004235999658703804\n",
      "epoch= 243 iteration= 22640 loss= 0.0012695607729256153\n",
      "epoch= 243 iteration= 22660 loss= 0.0012307761935517192\n",
      "epoch= 243 iteration= 22680 loss= 0.020259495824575424\n",
      "test_data MSELoss:(pred-real)/real= 0.004690579669032659\n",
      "epoch= 244 iteration= 22693 loss= 0.0003851560759358108\n",
      "epoch= 244 iteration= 22713 loss= 0.004225632641464472\n",
      "epoch= 244 iteration= 22733 loss= 0.0012666088296100497\n",
      "epoch= 244 iteration= 22753 loss= 0.0012300917878746986\n",
      "epoch= 244 iteration= 22773 loss= 0.020193446427583694\n",
      "test_data MSELoss:(pred-real)/real= 0.004672028822824359\n",
      "epoch= 245 iteration= 22786 loss= 0.00038658082485198975\n",
      "epoch= 245 iteration= 22806 loss= 0.004218422807753086\n",
      "epoch= 245 iteration= 22826 loss= 0.0012635733000934124\n",
      "epoch= 245 iteration= 22846 loss= 0.0012295788619667292\n",
      "epoch= 245 iteration= 22866 loss= 0.020130500197410583\n",
      "test_data MSELoss:(pred-real)/real= 0.004654820394029634\n",
      "epoch= 246 iteration= 22879 loss= 0.00038726418279111385\n",
      "epoch= 246 iteration= 22899 loss= 0.004205520264804363\n",
      "epoch= 246 iteration= 22919 loss= 0.0012596102897077799\n",
      "epoch= 246 iteration= 22939 loss= 0.0012294494081288576\n",
      "epoch= 246 iteration= 22959 loss= 0.020063310861587524\n",
      "test_data MSELoss:(pred-real)/real= 0.004637205953865002\n",
      "epoch= 247 iteration= 22972 loss= 0.00038964732084423304\n",
      "epoch= 247 iteration= 22992 loss= 0.004195015877485275\n",
      "epoch= 247 iteration= 23012 loss= 0.0012578184250742197\n",
      "epoch= 247 iteration= 23032 loss= 0.0012287404388189316\n",
      "epoch= 247 iteration= 23052 loss= 0.019999150186777115\n",
      "test_data MSELoss:(pred-real)/real= 0.004620014405291941\n",
      "epoch= 248 iteration= 23065 loss= 0.00039232682320289314\n",
      "epoch= 248 iteration= 23085 loss= 0.0041834213770926\n",
      "epoch= 248 iteration= 23105 loss= 0.001254757517017424\n",
      "epoch= 248 iteration= 23125 loss= 0.0012283404357731342\n",
      "epoch= 248 iteration= 23145 loss= 0.019931357353925705\n",
      "test_data MSELoss:(pred-real)/real= 0.00460175352378024\n",
      "epoch= 249 iteration= 23158 loss= 0.00039321815711446106\n",
      "epoch= 249 iteration= 23178 loss= 0.0041726380586624146\n",
      "epoch= 249 iteration= 23198 loss= 0.001250321278348565\n",
      "epoch= 249 iteration= 23218 loss= 0.0012279400834813714\n",
      "epoch= 249 iteration= 23238 loss= 0.019871026277542114\n",
      "test_data MSELoss:(pred-real)/real= 0.004585297293184946\n",
      "epoch= 250 iteration= 23251 loss= 0.0003960705653298646\n",
      "epoch= 250 iteration= 23271 loss= 0.0041600679978728294\n",
      "epoch= 250 iteration= 23291 loss= 0.001248746644705534\n",
      "epoch= 250 iteration= 23311 loss= 0.0012274717446416616\n",
      "epoch= 250 iteration= 23331 loss= 0.019801601767539978\n",
      "test_data MSELoss:(pred-real)/real= 0.004568766692601558\n",
      "epoch= 251 iteration= 23344 loss= 0.00039596387068741024\n",
      "epoch= 251 iteration= 23364 loss= 0.004148445557802916\n",
      "epoch= 251 iteration= 23384 loss= 0.0012453901581466198\n",
      "epoch= 251 iteration= 23404 loss= 0.00122661585919559\n",
      "epoch= 251 iteration= 23424 loss= 0.019739974290132523\n",
      "test_data MSELoss:(pred-real)/real= 0.004552181848945717\n",
      "epoch= 252 iteration= 23437 loss= 0.00039979073335416615\n",
      "epoch= 252 iteration= 23457 loss= 0.0041372161358594894\n",
      "epoch= 252 iteration= 23477 loss= 0.0012432941002771258\n",
      "epoch= 252 iteration= 23497 loss= 0.001226208871230483\n",
      "epoch= 252 iteration= 23517 loss= 0.019678693264722824\n",
      "test_data MSELoss:(pred-real)/real= 0.004535395839613759\n",
      "epoch= 253 iteration= 23530 loss= 0.0004008880932815373\n",
      "epoch= 253 iteration= 23550 loss= 0.004128695465624332\n",
      "epoch= 253 iteration= 23570 loss= 0.0012392133940011263\n",
      "epoch= 253 iteration= 23590 loss= 0.0012262093368917704\n",
      "epoch= 253 iteration= 23610 loss= 0.019614554941654205\n",
      "test_data MSELoss:(pred-real)/real= 0.00452014418422348\n",
      "epoch= 254 iteration= 23623 loss= 0.0004025479138363153\n",
      "epoch= 254 iteration= 23643 loss= 0.004118124954402447\n",
      "epoch= 254 iteration= 23663 loss= 0.001237479504197836\n",
      "epoch= 254 iteration= 23683 loss= 0.0012252444867044687\n",
      "epoch= 254 iteration= 23703 loss= 0.01955745741724968\n",
      "test_data MSELoss:(pred-real)/real= 0.004504137491393421\n",
      "epoch= 255 iteration= 23716 loss= 0.0004046433314215392\n",
      "epoch= 255 iteration= 23736 loss= 0.004103079903870821\n",
      "epoch= 255 iteration= 23756 loss= 0.0012335041537880898\n",
      "epoch= 255 iteration= 23776 loss= 0.001224087318405509\n",
      "epoch= 255 iteration= 23796 loss= 0.01949962228536606\n",
      "test_data MSELoss:(pred-real)/real= 0.004488886618572805\n",
      "epoch= 256 iteration= 23809 loss= 0.0004071742296218872\n",
      "epoch= 256 iteration= 23829 loss= 0.004091656766831875\n",
      "epoch= 256 iteration= 23849 loss= 0.0012300903908908367\n",
      "epoch= 256 iteration= 23869 loss= 0.001223730854690075\n",
      "epoch= 256 iteration= 23889 loss= 0.01943480595946312\n",
      "test_data MSELoss:(pred-real)/real= 0.004473350631693999\n",
      "epoch= 257 iteration= 23902 loss= 0.00040925084613263607\n",
      "epoch= 257 iteration= 23922 loss= 0.0040817116387188435\n",
      "epoch= 257 iteration= 23942 loss= 0.0012274259934201837\n",
      "epoch= 257 iteration= 23962 loss= 0.0012236973270773888\n",
      "epoch= 257 iteration= 23982 loss= 0.0193791501224041\n",
      "test_data MSELoss:(pred-real)/real= 0.004458107662180232\n",
      "epoch= 258 iteration= 23995 loss= 0.00041000268538482487\n",
      "epoch= 258 iteration= 24015 loss= 0.004072277341037989\n",
      "epoch= 258 iteration= 24035 loss= 0.0012247984996065497\n",
      "epoch= 258 iteration= 24055 loss= 0.0012230419088155031\n",
      "epoch= 258 iteration= 24075 loss= 0.019318323582410812\n",
      "test_data MSELoss:(pred-real)/real= 0.004442743374966085\n",
      "epoch= 259 iteration= 24088 loss= 0.0004121178644709289\n",
      "epoch= 259 iteration= 24108 loss= 0.004062654450535774\n",
      "epoch= 259 iteration= 24128 loss= 0.0012214374728500843\n",
      "epoch= 259 iteration= 24148 loss= 0.0012226304970681667\n",
      "epoch= 259 iteration= 24168 loss= 0.019256997853517532\n",
      "test_data MSELoss:(pred-real)/real= 0.004428660813977735\n",
      "epoch= 260 iteration= 24181 loss= 0.00041414686711505055\n",
      "epoch= 260 iteration= 24201 loss= 0.004051854833960533\n",
      "epoch= 260 iteration= 24221 loss= 0.001217965385876596\n",
      "epoch= 260 iteration= 24241 loss= 0.0012222947552800179\n",
      "epoch= 260 iteration= 24261 loss= 0.01920517161488533\n",
      "test_data MSELoss:(pred-real)/real= 0.004414803147988601\n",
      "epoch= 261 iteration= 24274 loss= 0.00041615270311012864\n",
      "epoch= 261 iteration= 24294 loss= 0.004039169289171696\n",
      "epoch= 261 iteration= 24314 loss= 0.0012156028533354402\n",
      "epoch= 261 iteration= 24334 loss= 0.0012209564447402954\n",
      "epoch= 261 iteration= 24354 loss= 0.019150305539369583\n",
      "test_data MSELoss:(pred-real)/real= 0.004400114179588854\n",
      "epoch= 262 iteration= 24367 loss= 0.0004189056344330311\n",
      "epoch= 262 iteration= 24387 loss= 0.004028173163533211\n",
      "epoch= 262 iteration= 24407 loss= 0.0012128796661272645\n",
      "epoch= 262 iteration= 24427 loss= 0.001221055630594492\n",
      "epoch= 262 iteration= 24447 loss= 0.01909482479095459\n",
      "test_data MSELoss:(pred-real)/real= 0.004386271285410557\n",
      "epoch= 263 iteration= 24460 loss= 0.00042081158608198166\n",
      "epoch= 263 iteration= 24480 loss= 0.0040167514234781265\n",
      "epoch= 263 iteration= 24500 loss= 0.0012105219066143036\n",
      "epoch= 263 iteration= 24520 loss= 0.0012201903155073524\n",
      "epoch= 263 iteration= 24540 loss= 0.019035227596759796\n",
      "test_data MSELoss:(pred-real)/real= 0.004372582958442056\n",
      "epoch= 264 iteration= 24553 loss= 0.0004213851352687925\n",
      "epoch= 264 iteration= 24573 loss= 0.0040077692829072475\n",
      "epoch= 264 iteration= 24593 loss= 0.0012061995221301913\n",
      "epoch= 264 iteration= 24613 loss= 0.0012196716852486134\n",
      "epoch= 264 iteration= 24633 loss= 0.018985029309988022\n",
      "test_data MSELoss:(pred-real)/real= 0.004358741849298692\n",
      "epoch= 265 iteration= 24646 loss= 0.0004248693585395813\n",
      "epoch= 265 iteration= 24666 loss= 0.003997403662651777\n",
      "epoch= 265 iteration= 24686 loss= 0.001203329535201192\n",
      "epoch= 265 iteration= 24706 loss= 0.0012191797140985727\n",
      "epoch= 265 iteration= 24726 loss= 0.018930602818727493\n",
      "test_data MSELoss:(pred-real)/real= 0.004345440706755552\n",
      "epoch= 266 iteration= 24739 loss= 0.0004268224583938718\n",
      "epoch= 266 iteration= 24759 loss= 0.003986906260251999\n",
      "epoch= 266 iteration= 24779 loss= 0.0012007249752059579\n",
      "epoch= 266 iteration= 24799 loss= 0.0012183081125840545\n",
      "epoch= 266 iteration= 24819 loss= 0.018877703696489334\n",
      "test_data MSELoss:(pred-real)/real= 0.00433207000605762\n",
      "epoch= 267 iteration= 24832 loss= 0.0004295172984711826\n",
      "epoch= 267 iteration= 24852 loss= 0.003973034210503101\n",
      "epoch= 267 iteration= 24872 loss= 0.0011990010971203446\n",
      "epoch= 267 iteration= 24892 loss= 0.0012179433833807707\n",
      "epoch= 267 iteration= 24912 loss= 0.018822094425559044\n",
      "test_data MSELoss:(pred-real)/real= 0.004319263027153081\n",
      "epoch= 268 iteration= 24925 loss= 0.0004318819846957922\n",
      "epoch= 268 iteration= 24945 loss= 0.003963084891438484\n",
      "epoch= 268 iteration= 24965 loss= 0.001196458237245679\n",
      "epoch= 268 iteration= 24985 loss= 0.0012170961126685143\n",
      "epoch= 268 iteration= 25005 loss= 0.018773412331938744\n",
      "test_data MSELoss:(pred-real)/real= 0.004305986629333347\n",
      "epoch= 269 iteration= 25018 loss= 0.00043454254046082497\n",
      "epoch= 269 iteration= 25038 loss= 0.003954497165977955\n",
      "epoch= 269 iteration= 25058 loss= 0.001193604664877057\n",
      "epoch= 269 iteration= 25078 loss= 0.001216559554450214\n",
      "epoch= 269 iteration= 25098 loss= 0.01872115582227707\n",
      "test_data MSELoss:(pred-real)/real= 0.004293708117782242\n",
      "epoch= 270 iteration= 25111 loss= 0.0004372855764813721\n",
      "epoch= 270 iteration= 25131 loss= 0.003940407652407885\n",
      "epoch= 270 iteration= 25151 loss= 0.0011895513162016869\n",
      "epoch= 270 iteration= 25171 loss= 0.0012160739861428738\n",
      "epoch= 270 iteration= 25191 loss= 0.01866953819990158\n",
      "test_data MSELoss:(pred-real)/real= 0.004280313228567441\n",
      "epoch= 271 iteration= 25204 loss= 0.00043799361446872354\n",
      "epoch= 271 iteration= 25224 loss= 0.0039305053651332855\n",
      "epoch= 271 iteration= 25244 loss= 0.0011868557194247842\n",
      "epoch= 271 iteration= 25264 loss= 0.001214985502883792\n",
      "epoch= 271 iteration= 25284 loss= 0.018623661249876022\n",
      "test_data MSELoss:(pred-real)/real= 0.004268279279737423\n",
      "epoch= 272 iteration= 25297 loss= 0.0004401468322612345\n",
      "epoch= 272 iteration= 25317 loss= 0.003920600283890963\n",
      "epoch= 272 iteration= 25337 loss= 0.001184554654173553\n",
      "epoch= 272 iteration= 25357 loss= 0.0012148786336183548\n",
      "epoch= 272 iteration= 25377 loss= 0.018574893474578857\n",
      "test_data MSELoss:(pred-real)/real= 0.004256727665455805\n",
      "epoch= 273 iteration= 25390 loss= 0.0004428999964147806\n",
      "epoch= 273 iteration= 25410 loss= 0.003913433291018009\n",
      "epoch= 273 iteration= 25430 loss= 0.0011816384503617883\n",
      "epoch= 273 iteration= 25450 loss= 0.0012139175087213516\n",
      "epoch= 273 iteration= 25470 loss= 0.018526293337345123\n",
      "test_data MSELoss:(pred-real)/real= 0.004245204721680946\n",
      "epoch= 274 iteration= 25483 loss= 0.0004445435479283333\n",
      "epoch= 274 iteration= 25503 loss= 0.0038982483092695475\n",
      "epoch= 274 iteration= 25523 loss= 0.0011800797656178474\n",
      "epoch= 274 iteration= 25543 loss= 0.0012131344992667437\n",
      "epoch= 274 iteration= 25563 loss= 0.018482614308595657\n",
      "test_data MSELoss:(pred-real)/real= 0.004233909693236153\n",
      "epoch= 275 iteration= 25576 loss= 0.00044692461960949004\n",
      "epoch= 275 iteration= 25596 loss= 0.003890823572874069\n",
      "epoch= 275 iteration= 25616 loss= 0.0011772727593779564\n",
      "epoch= 275 iteration= 25636 loss= 0.0012130558025091887\n",
      "epoch= 275 iteration= 25656 loss= 0.01843547262251377\n",
      "test_data MSELoss:(pred-real)/real= 0.0042228747883604634\n",
      "epoch= 276 iteration= 25669 loss= 0.0004487122059799731\n",
      "epoch= 276 iteration= 25689 loss= 0.0038792011328041553\n",
      "epoch= 276 iteration= 25709 loss= 0.001174943638034165\n",
      "epoch= 276 iteration= 25729 loss= 0.0012121994514018297\n",
      "epoch= 276 iteration= 25749 loss= 0.018385104835033417\n",
      "test_data MSELoss:(pred-real)/real= 0.0042108325675750775\n",
      "epoch= 277 iteration= 25762 loss= 0.00045171225792728364\n",
      "epoch= 277 iteration= 25782 loss= 0.0038715016562491655\n",
      "epoch= 277 iteration= 25802 loss= 0.0011728289537131786\n",
      "epoch= 277 iteration= 25822 loss= 0.0012114067794755101\n",
      "epoch= 277 iteration= 25842 loss= 0.01834297366440296\n",
      "test_data MSELoss:(pred-real)/real= 0.004199437014499886\n",
      "epoch= 278 iteration= 25855 loss= 0.0004528510617092252\n",
      "epoch= 278 iteration= 25875 loss= 0.00386241776868701\n",
      "epoch= 278 iteration= 25895 loss= 0.0011689660605043173\n",
      "epoch= 278 iteration= 25915 loss= 0.0012106001377105713\n",
      "epoch= 278 iteration= 25935 loss= 0.01829471066594124\n",
      "test_data MSELoss:(pred-real)/real= 0.0041875733295455575\n",
      "epoch= 279 iteration= 25948 loss= 0.00045485180453397334\n",
      "epoch= 279 iteration= 25968 loss= 0.003850272623822093\n",
      "epoch= 279 iteration= 25988 loss= 0.0011660989839583635\n",
      "epoch= 279 iteration= 26008 loss= 0.0012101029278710485\n",
      "epoch= 279 iteration= 26028 loss= 0.01825118623673916\n",
      "test_data MSELoss:(pred-real)/real= 0.004176686905945341\n",
      "epoch= 280 iteration= 26041 loss= 0.00045693281572312117\n",
      "epoch= 280 iteration= 26061 loss= 0.0038362618070095778\n",
      "epoch= 280 iteration= 26081 loss= 0.0011634666007012129\n",
      "epoch= 280 iteration= 26101 loss= 0.0012091815005987883\n",
      "epoch= 280 iteration= 26121 loss= 0.018208971247076988\n",
      "test_data MSELoss:(pred-real)/real= 0.0041666240043317275\n",
      "epoch= 281 iteration= 26134 loss= 0.00046058488078415394\n",
      "epoch= 281 iteration= 26154 loss= 0.0038262547459453344\n",
      "epoch= 281 iteration= 26174 loss= 0.0011615571565926075\n",
      "epoch= 281 iteration= 26194 loss= 0.0012083076871931553\n",
      "epoch= 281 iteration= 26214 loss= 0.018163200467824936\n",
      "test_data MSELoss:(pred-real)/real= 0.004155290086701926\n",
      "epoch= 282 iteration= 26227 loss= 0.00046185270184651017\n",
      "epoch= 282 iteration= 26247 loss= 0.0038230624049901962\n",
      "epoch= 282 iteration= 26267 loss= 0.0011596722761169076\n",
      "epoch= 282 iteration= 26287 loss= 0.0012080855667591095\n",
      "epoch= 282 iteration= 26307 loss= 0.018124999478459358\n",
      "test_data MSELoss:(pred-real)/real= 0.004145760341392209\n",
      "epoch= 283 iteration= 26320 loss= 0.00046245643170550466\n",
      "epoch= 283 iteration= 26340 loss= 0.003807671368122101\n",
      "epoch= 283 iteration= 26360 loss= 0.0011569592170417309\n",
      "epoch= 283 iteration= 26380 loss= 0.00120740314014256\n",
      "epoch= 283 iteration= 26400 loss= 0.01808280497789383\n",
      "test_data MSELoss:(pred-real)/real= 0.004135464531524728\n",
      "epoch= 284 iteration= 26413 loss= 0.0004656905948650092\n",
      "epoch= 284 iteration= 26433 loss= 0.003798890160396695\n",
      "epoch= 284 iteration= 26453 loss= 0.0011551107745617628\n",
      "epoch= 284 iteration= 26473 loss= 0.0012062637833878398\n",
      "epoch= 284 iteration= 26493 loss= 0.018041808158159256\n",
      "test_data MSELoss:(pred-real)/real= 0.004124748675773541\n",
      "epoch= 285 iteration= 26506 loss= 0.00046829512575641274\n",
      "epoch= 285 iteration= 26526 loss= 0.0037890980020165443\n",
      "epoch= 285 iteration= 26546 loss= 0.0011524023720994592\n",
      "epoch= 285 iteration= 26566 loss= 0.0012065725168213248\n",
      "epoch= 285 iteration= 26586 loss= 0.01799904555082321\n",
      "test_data MSELoss:(pred-real)/real= 0.004115096332194905\n",
      "epoch= 286 iteration= 26599 loss= 0.0004708381893578917\n",
      "epoch= 286 iteration= 26619 loss= 0.003776930272579193\n",
      "epoch= 286 iteration= 26639 loss= 0.0011503982823342085\n",
      "epoch= 286 iteration= 26659 loss= 0.0012054448015987873\n",
      "epoch= 286 iteration= 26679 loss= 0.017958540469408035\n",
      "test_data MSELoss:(pred-real)/real= 0.004104832691761355\n",
      "epoch= 287 iteration= 26692 loss= 0.00047334632836282253\n",
      "epoch= 287 iteration= 26712 loss= 0.0037687295116484165\n",
      "epoch= 287 iteration= 26732 loss= 0.0011466759024187922\n",
      "epoch= 287 iteration= 26752 loss= 0.0012043965980410576\n",
      "epoch= 287 iteration= 26772 loss= 0.017921462655067444\n",
      "test_data MSELoss:(pred-real)/real= 0.004094916732153959\n",
      "epoch= 288 iteration= 26785 loss= 0.00047484852257184684\n",
      "epoch= 288 iteration= 26805 loss= 0.0037625031545758247\n",
      "epoch= 288 iteration= 26825 loss= 0.0011441888054832816\n",
      "epoch= 288 iteration= 26845 loss= 0.0012035854160785675\n",
      "epoch= 288 iteration= 26865 loss= 0.017881812527775764\n",
      "test_data MSELoss:(pred-real)/real= 0.004086077594870908\n",
      "epoch= 289 iteration= 26878 loss= 0.0004772594547830522\n",
      "epoch= 289 iteration= 26898 loss= 0.003748480696231127\n",
      "epoch= 289 iteration= 26918 loss= 0.0011420767987146974\n",
      "epoch= 289 iteration= 26938 loss= 0.0012030138168483973\n",
      "epoch= 289 iteration= 26958 loss= 0.017844241112470627\n",
      "test_data MSELoss:(pred-real)/real= 0.004075754737843656\n",
      "epoch= 290 iteration= 26971 loss= 0.0004790926759596914\n",
      "epoch= 290 iteration= 26991 loss= 0.0037414126563817263\n",
      "epoch= 290 iteration= 27011 loss= 0.0011394042521715164\n",
      "epoch= 290 iteration= 27031 loss= 0.0012018190464004874\n",
      "epoch= 290 iteration= 27051 loss= 0.017798949033021927\n",
      "test_data MSELoss:(pred-real)/real= 0.004066705503242297\n",
      "epoch= 291 iteration= 27064 loss= 0.00048178539145737886\n",
      "epoch= 291 iteration= 27084 loss= 0.003728959709405899\n",
      "epoch= 291 iteration= 27104 loss= 0.0011379343923181295\n",
      "epoch= 291 iteration= 27124 loss= 0.0012011653743684292\n",
      "epoch= 291 iteration= 27144 loss= 0.017766529694199562\n",
      "test_data MSELoss:(pred-real)/real= 0.004056744639658266\n",
      "epoch= 292 iteration= 27157 loss= 0.0004841267073061317\n",
      "epoch= 292 iteration= 27177 loss= 0.0037201736122369766\n",
      "epoch= 292 iteration= 27197 loss= 0.0011348093394190073\n",
      "epoch= 292 iteration= 27217 loss= 0.0012002803850919008\n",
      "epoch= 292 iteration= 27237 loss= 0.01773461140692234\n",
      "test_data MSELoss:(pred-real)/real= 0.004047824891232368\n",
      "epoch= 293 iteration= 27250 loss= 0.0004854024446103722\n",
      "epoch= 293 iteration= 27270 loss= 0.0037076114676892757\n",
      "epoch= 293 iteration= 27290 loss= 0.0011326438980177045\n",
      "epoch= 293 iteration= 27310 loss= 0.0011999025009572506\n",
      "epoch= 293 iteration= 27330 loss= 0.01769549399614334\n",
      "test_data MSELoss:(pred-real)/real= 0.004039171352309899\n",
      "epoch= 294 iteration= 27343 loss= 0.00048723252257332206\n",
      "epoch= 294 iteration= 27363 loss= 0.0037020347081124783\n",
      "epoch= 294 iteration= 27383 loss= 0.0011298353783786297\n",
      "epoch= 294 iteration= 27403 loss= 0.0011987180914729834\n",
      "epoch= 294 iteration= 27423 loss= 0.01766085997223854\n",
      "test_data MSELoss:(pred-real)/real= 0.004030153207066987\n",
      "epoch= 295 iteration= 27436 loss= 0.0004882215289399028\n",
      "epoch= 295 iteration= 27456 loss= 0.003690970130264759\n",
      "epoch= 295 iteration= 27476 loss= 0.0011273319832980633\n",
      "epoch= 295 iteration= 27496 loss= 0.0011985520832240582\n",
      "epoch= 295 iteration= 27516 loss= 0.01762848161160946\n",
      "test_data MSELoss:(pred-real)/real= 0.004020890240402271\n",
      "epoch= 296 iteration= 27529 loss= 0.0004895430756732821\n",
      "epoch= 296 iteration= 27549 loss= 0.0036806720308959484\n",
      "epoch= 296 iteration= 27569 loss= 0.0011261857580393553\n",
      "epoch= 296 iteration= 27589 loss= 0.0011980669805780053\n",
      "epoch= 296 iteration= 27609 loss= 0.01759307272732258\n",
      "test_data MSELoss:(pred-real)/real= 0.004012022671910624\n",
      "epoch= 297 iteration= 27622 loss= 0.0004910934367217124\n",
      "epoch= 297 iteration= 27642 loss= 0.0036730640567839146\n",
      "epoch= 297 iteration= 27662 loss= 0.001123775145970285\n",
      "epoch= 297 iteration= 27682 loss= 0.0011968296021223068\n",
      "epoch= 297 iteration= 27702 loss= 0.017558131366968155\n",
      "test_data MSELoss:(pred-real)/real= 0.004003690180575682\n",
      "epoch= 298 iteration= 27715 loss= 0.0004923423402942717\n",
      "epoch= 298 iteration= 27735 loss= 0.003667183918878436\n",
      "epoch= 298 iteration= 27755 loss= 0.0011209227377548814\n",
      "epoch= 298 iteration= 27775 loss= 0.0011961398413404822\n",
      "epoch= 298 iteration= 27795 loss= 0.01752469874918461\n",
      "test_data MSELoss:(pred-real)/real= 0.003994908623604311\n",
      "epoch= 299 iteration= 27808 loss= 0.0004951348528265953\n",
      "epoch= 299 iteration= 27828 loss= 0.0036576553247869015\n",
      "epoch= 299 iteration= 27848 loss= 0.0011181703303009272\n",
      "epoch= 299 iteration= 27868 loss= 0.0011951043270528316\n",
      "epoch= 299 iteration= 27888 loss= 0.017492037266492844\n",
      "test_data MSELoss:(pred-real)/real= 0.003986585600715544\n",
      "epoch= 300 iteration= 27901 loss= 0.0004969621659256518\n",
      "epoch= 300 iteration= 27921 loss= 0.003645705059170723\n",
      "epoch= 300 iteration= 27941 loss= 0.0011157714761793613\n",
      "epoch= 300 iteration= 27961 loss= 0.0011944693978875875\n",
      "epoch= 300 iteration= 27981 loss= 0.017460741102695465\n",
      "test_data MSELoss:(pred-real)/real= 0.00397800758946687\n",
      "epoch= 301 iteration= 27994 loss= 0.0004986230051144958\n",
      "epoch= 301 iteration= 28014 loss= 0.0036351880989968777\n",
      "epoch= 301 iteration= 28034 loss= 0.0011144900927320123\n",
      "epoch= 301 iteration= 28054 loss= 0.0011933923233300447\n",
      "epoch= 301 iteration= 28074 loss= 0.017431087791919708\n",
      "test_data MSELoss:(pred-real)/real= 0.003969711122206516\n",
      "epoch= 302 iteration= 28087 loss= 0.0005004750564694405\n",
      "epoch= 302 iteration= 28107 loss= 0.003626507706940174\n",
      "epoch= 302 iteration= 28127 loss= 0.0011117231333628297\n",
      "epoch= 302 iteration= 28147 loss= 0.001192691270262003\n",
      "epoch= 302 iteration= 28167 loss= 0.017400052398443222\n",
      "test_data MSELoss:(pred-real)/real= 0.003961160261597898\n",
      "epoch= 303 iteration= 28180 loss= 0.0005042943521402776\n",
      "epoch= 303 iteration= 28200 loss= 0.0036149693187326193\n",
      "epoch= 303 iteration= 28220 loss= 0.0011088792234659195\n",
      "epoch= 303 iteration= 28240 loss= 0.0011924236314371228\n",
      "epoch= 303 iteration= 28260 loss= 0.017370358109474182\n",
      "test_data MSELoss:(pred-real)/real= 0.003953504395515968\n",
      "epoch= 304 iteration= 28273 loss= 0.0005042414413765073\n",
      "epoch= 304 iteration= 28293 loss= 0.0036087154876440763\n",
      "epoch= 304 iteration= 28313 loss= 0.001106760697439313\n",
      "epoch= 304 iteration= 28333 loss= 0.0011911110486835241\n",
      "epoch= 304 iteration= 28353 loss= 0.017335379496216774\n",
      "test_data MSELoss:(pred-real)/real= 0.003945240414597922\n",
      "epoch= 305 iteration= 28366 loss= 0.0005060028051957488\n",
      "epoch= 305 iteration= 28386 loss= 0.003598754061385989\n",
      "epoch= 305 iteration= 28406 loss= 0.00110585184302181\n",
      "epoch= 305 iteration= 28426 loss= 0.001189950737170875\n",
      "epoch= 305 iteration= 28446 loss= 0.01730709709227085\n",
      "test_data MSELoss:(pred-real)/real= 0.003937004386292149\n",
      "epoch= 306 iteration= 28459 loss= 0.0005096333916299045\n",
      "epoch= 306 iteration= 28479 loss= 0.0035882231313735247\n",
      "epoch= 306 iteration= 28499 loss= 0.0011036730138584971\n",
      "epoch= 306 iteration= 28519 loss= 0.0011892286129295826\n",
      "epoch= 306 iteration= 28539 loss= 0.0172770693898201\n",
      "test_data MSELoss:(pred-real)/real= 0.0039288708810797995\n",
      "epoch= 307 iteration= 28552 loss= 0.0005105552263557911\n",
      "epoch= 307 iteration= 28572 loss= 0.0035794712603092194\n",
      "epoch= 307 iteration= 28592 loss= 0.0011013628682121634\n",
      "epoch= 307 iteration= 28612 loss= 0.0011888687731698155\n",
      "epoch= 307 iteration= 28632 loss= 0.017246834933757782\n",
      "test_data MSELoss:(pred-real)/real= 0.00392093822463519\n",
      "epoch= 308 iteration= 28645 loss= 0.0005116983084008098\n",
      "epoch= 308 iteration= 28665 loss= 0.0035684877075254917\n",
      "epoch= 308 iteration= 28685 loss= 0.0010990978917106986\n",
      "epoch= 308 iteration= 28705 loss= 0.0011878833174705505\n",
      "epoch= 308 iteration= 28725 loss= 0.017217814922332764\n",
      "test_data MSELoss:(pred-real)/real= 0.003913708658526755\n",
      "epoch= 309 iteration= 28738 loss= 0.0005144579336047173\n",
      "epoch= 309 iteration= 28758 loss= 0.003562278114259243\n",
      "epoch= 309 iteration= 28778 loss= 0.0010959443170577288\n",
      "epoch= 309 iteration= 28798 loss= 0.001187239307910204\n",
      "epoch= 309 iteration= 28818 loss= 0.017185617238283157\n",
      "test_data MSELoss:(pred-real)/real= 0.0039061655536190504\n",
      "epoch= 310 iteration= 28831 loss= 0.0005169374635443091\n",
      "epoch= 310 iteration= 28851 loss= 0.003551980946213007\n",
      "epoch= 310 iteration= 28871 loss= 0.0010941927321255207\n",
      "epoch= 310 iteration= 28891 loss= 0.001186259789392352\n",
      "epoch= 310 iteration= 28911 loss= 0.01715872436761856\n",
      "test_data MSELoss:(pred-real)/real= 0.0038982544146064254\n",
      "epoch= 311 iteration= 28924 loss= 0.0005183669272810221\n",
      "epoch= 311 iteration= 28944 loss= 0.003543912200257182\n",
      "epoch= 311 iteration= 28964 loss= 0.0010927182156592607\n",
      "epoch= 311 iteration= 28984 loss= 0.0011852043680846691\n",
      "epoch= 311 iteration= 29004 loss= 0.017132647335529327\n",
      "test_data MSELoss:(pred-real)/real= 0.0038905006885114643\n",
      "epoch= 312 iteration= 29017 loss= 0.0005187217029742897\n",
      "epoch= 312 iteration= 29037 loss= 0.003532734466716647\n",
      "epoch= 312 iteration= 29057 loss= 0.0010895917657762766\n",
      "epoch= 312 iteration= 29077 loss= 0.001184498076327145\n",
      "epoch= 312 iteration= 29097 loss= 0.017104825004935265\n",
      "test_data MSELoss:(pred-real)/real= 0.0038826776758974623\n",
      "epoch= 313 iteration= 29110 loss= 0.0005204805638641119\n",
      "epoch= 313 iteration= 29130 loss= 0.00352671486325562\n",
      "epoch= 313 iteration= 29150 loss= 0.0010886355303227901\n",
      "epoch= 313 iteration= 29170 loss= 0.001183593412861228\n",
      "epoch= 313 iteration= 29190 loss= 0.017073623836040497\n",
      "test_data MSELoss:(pred-real)/real= 0.003875207770357115\n",
      "epoch= 314 iteration= 29203 loss= 0.0005217609577812254\n",
      "epoch= 314 iteration= 29223 loss= 0.0035163683351129293\n",
      "epoch= 314 iteration= 29243 loss= 0.0010858627501875162\n",
      "epoch= 314 iteration= 29263 loss= 0.0011822154046967626\n",
      "epoch= 314 iteration= 29283 loss= 0.01705010235309601\n",
      "test_data MSELoss:(pred-real)/real= 0.003867657589984851\n",
      "epoch= 315 iteration= 29296 loss= 0.0005234283744357526\n",
      "epoch= 315 iteration= 29316 loss= 0.0035080937668681145\n",
      "epoch= 315 iteration= 29336 loss= 0.0010846800869330764\n",
      "epoch= 315 iteration= 29356 loss= 0.0011818348430097103\n",
      "epoch= 315 iteration= 29376 loss= 0.017025774344801903\n",
      "test_data MSELoss:(pred-real)/real= 0.003859701411177715\n",
      "epoch= 316 iteration= 29389 loss= 0.0005252117989584804\n",
      "epoch= 316 iteration= 29409 loss= 0.0034969281405210495\n",
      "epoch= 316 iteration= 29429 loss= 0.0010819248855113983\n",
      "epoch= 316 iteration= 29449 loss= 0.0011814627796411514\n",
      "epoch= 316 iteration= 29469 loss= 0.017001653090119362\n",
      "test_data MSELoss:(pred-real)/real= 0.003852874072941227\n",
      "epoch= 317 iteration= 29482 loss= 0.0005271323607303202\n",
      "epoch= 317 iteration= 29502 loss= 0.0034893248230218887\n",
      "epoch= 317 iteration= 29522 loss= 0.001080206362530589\n",
      "epoch= 317 iteration= 29542 loss= 0.0011801766231656075\n",
      "epoch= 317 iteration= 29562 loss= 0.016968153417110443\n",
      "test_data MSELoss:(pred-real)/real= 0.003845095640927967\n",
      "epoch= 318 iteration= 29575 loss= 0.0005293695721775293\n",
      "epoch= 318 iteration= 29595 loss= 0.0034774697851389647\n",
      "epoch= 318 iteration= 29615 loss= 0.0010779567528516054\n",
      "epoch= 318 iteration= 29635 loss= 0.0011793513549491763\n",
      "epoch= 318 iteration= 29655 loss= 0.01694481447339058\n",
      "test_data MSELoss:(pred-real)/real= 0.003837641968857497\n",
      "epoch= 319 iteration= 29668 loss= 0.0005314532318152487\n",
      "epoch= 319 iteration= 29688 loss= 0.003470542374998331\n",
      "epoch= 319 iteration= 29708 loss= 0.0010764525504782796\n",
      "epoch= 319 iteration= 29728 loss= 0.0011785009410232306\n",
      "epoch= 319 iteration= 29748 loss= 0.016918200999498367\n",
      "test_data MSELoss:(pred-real)/real= 0.003830722875944856\n",
      "epoch= 320 iteration= 29761 loss= 0.0005331170978024602\n",
      "epoch= 320 iteration= 29781 loss= 0.0034610226284712553\n",
      "epoch= 320 iteration= 29801 loss= 0.0010749854845926166\n",
      "epoch= 320 iteration= 29821 loss= 0.0011775755556300282\n",
      "epoch= 320 iteration= 29841 loss= 0.016893528401851654\n",
      "test_data MSELoss:(pred-real)/real= 0.0038236091172115672\n",
      "epoch= 321 iteration= 29854 loss= 0.0005346701946109533\n",
      "epoch= 321 iteration= 29874 loss= 0.0034519098699092865\n",
      "epoch= 321 iteration= 29894 loss= 0.0010730210924521089\n",
      "epoch= 321 iteration= 29914 loss= 0.0011761334026232362\n",
      "epoch= 321 iteration= 29934 loss= 0.01686636172235012\n",
      "test_data MSELoss:(pred-real)/real= 0.003816346152840803\n",
      "epoch= 322 iteration= 29947 loss= 0.0005358014605008066\n",
      "epoch= 322 iteration= 29967 loss= 0.003444182686507702\n",
      "epoch= 322 iteration= 29987 loss= 0.0010698068654164672\n",
      "epoch= 322 iteration= 30007 loss= 0.001175598008558154\n",
      "epoch= 322 iteration= 30027 loss= 0.016841573640704155\n",
      "test_data MSELoss:(pred-real)/real= 0.003809007764276531\n",
      "epoch= 323 iteration= 30040 loss= 0.0005366067634895444\n",
      "epoch= 323 iteration= 30060 loss= 0.0034363234881311655\n",
      "epoch= 323 iteration= 30080 loss= 0.0010667925234884024\n",
      "epoch= 323 iteration= 30100 loss= 0.0011745861265808344\n",
      "epoch= 323 iteration= 30120 loss= 0.016818851232528687\n",
      "test_data MSELoss:(pred-real)/real= 0.0038019821060717935\n",
      "epoch= 324 iteration= 30133 loss= 0.0005383058451116085\n",
      "epoch= 324 iteration= 30153 loss= 0.0034275297075510025\n",
      "epoch= 324 iteration= 30173 loss= 0.0010656124213710427\n",
      "epoch= 324 iteration= 30193 loss= 0.0011738265166059136\n",
      "epoch= 324 iteration= 30213 loss= 0.016788702458143234\n",
      "test_data MSELoss:(pred-real)/real= 0.0037949573080469337\n",
      "epoch= 325 iteration= 30226 loss= 0.0005417696083895862\n",
      "epoch= 325 iteration= 30246 loss= 0.003414942417293787\n",
      "epoch= 325 iteration= 30266 loss= 0.0010642976267263293\n",
      "epoch= 325 iteration= 30286 loss= 0.0011726042721420527\n",
      "epoch= 325 iteration= 30306 loss= 0.016767028719186783\n",
      "test_data MSELoss:(pred-real)/real= 0.0037867805610100427\n",
      "epoch= 326 iteration= 30319 loss= 0.0005416607018560171\n",
      "epoch= 326 iteration= 30339 loss= 0.0034060899633914232\n",
      "epoch= 326 iteration= 30359 loss= 0.0010618952801451087\n",
      "epoch= 326 iteration= 30379 loss= 0.0011719852918758988\n",
      "epoch= 326 iteration= 30399 loss= 0.016740160062909126\n",
      "test_data MSELoss:(pred-real)/real= 0.0037800347393689058\n",
      "epoch= 327 iteration= 30412 loss= 0.0005420380621217191\n",
      "epoch= 327 iteration= 30432 loss= 0.0034012338146567345\n",
      "epoch= 327 iteration= 30452 loss= 0.0010602958500385284\n",
      "epoch= 327 iteration= 30472 loss= 0.0011711870320141315\n",
      "epoch= 327 iteration= 30492 loss= 0.016715453937649727\n",
      "test_data MSELoss:(pred-real)/real= 0.003773030669738849\n",
      "epoch= 328 iteration= 30505 loss= 0.0005428881268016994\n",
      "epoch= 328 iteration= 30525 loss= 0.0033925590105354786\n",
      "epoch= 328 iteration= 30545 loss= 0.0010583613766357303\n",
      "epoch= 328 iteration= 30565 loss= 0.0011700480245053768\n",
      "epoch= 328 iteration= 30585 loss= 0.016689859330654144\n",
      "test_data MSELoss:(pred-real)/real= 0.003766057579519434\n",
      "epoch= 329 iteration= 30598 loss= 0.0005440694512799382\n",
      "epoch= 329 iteration= 30618 loss= 0.003385853487998247\n",
      "epoch= 329 iteration= 30638 loss= 0.0010565350530669093\n",
      "epoch= 329 iteration= 30658 loss= 0.0011692775879055262\n",
      "epoch= 329 iteration= 30678 loss= 0.016662515699863434\n",
      "test_data MSELoss:(pred-real)/real= 0.0037585682584904134\n",
      "epoch= 330 iteration= 30691 loss= 0.0005435594939626753\n",
      "epoch= 330 iteration= 30711 loss= 0.0033752545714378357\n",
      "epoch= 330 iteration= 30731 loss= 0.0010547650745138526\n",
      "epoch= 330 iteration= 30751 loss= 0.0011684598866850138\n",
      "epoch= 330 iteration= 30771 loss= 0.016638806089758873\n",
      "test_data MSELoss:(pred-real)/real= 0.003751961475548645\n",
      "epoch= 331 iteration= 30784 loss= 0.0005436247447505593\n",
      "epoch= 331 iteration= 30804 loss= 0.003369760001078248\n",
      "epoch= 331 iteration= 30824 loss= 0.0010517305927351117\n",
      "epoch= 331 iteration= 30844 loss= 0.0011676216963678598\n",
      "epoch= 331 iteration= 30864 loss= 0.016614116728305817\n",
      "test_data MSELoss:(pred-real)/real= 0.0037446657467323043\n",
      "epoch= 332 iteration= 30877 loss= 0.000543713744264096\n",
      "epoch= 332 iteration= 30897 loss= 0.003360095899552107\n",
      "epoch= 332 iteration= 30917 loss= 0.0010491720167919993\n",
      "epoch= 332 iteration= 30937 loss= 0.0011668457882478833\n",
      "epoch= 332 iteration= 30957 loss= 0.016589563339948654\n",
      "test_data MSELoss:(pred-real)/real= 0.0037376142895987462\n",
      "epoch= 333 iteration= 30970 loss= 0.0005461048567667603\n",
      "epoch= 333 iteration= 30990 loss= 0.003352855332195759\n",
      "epoch= 333 iteration= 31010 loss= 0.0010470983106642962\n",
      "epoch= 333 iteration= 31030 loss= 0.0011656865244731307\n",
      "epoch= 333 iteration= 31050 loss= 0.016565484926104546\n",
      "test_data MSELoss:(pred-real)/real= 0.003731286069119556\n",
      "epoch= 334 iteration= 31063 loss= 0.0005487673915922642\n",
      "epoch= 334 iteration= 31083 loss= 0.0033452343195676804\n",
      "epoch= 334 iteration= 31103 loss= 0.0010450206464156508\n",
      "epoch= 334 iteration= 31123 loss= 0.0011647173669189215\n",
      "epoch= 334 iteration= 31143 loss= 0.016541248187422752\n",
      "test_data MSELoss:(pred-real)/real= 0.0037237862383739818\n",
      "epoch= 335 iteration= 31156 loss= 0.0005487935850396752\n",
      "epoch= 335 iteration= 31176 loss= 0.003334441687911749\n",
      "epoch= 335 iteration= 31196 loss= 0.0010439816396683455\n",
      "epoch= 335 iteration= 31216 loss= 0.001163458451628685\n",
      "epoch= 335 iteration= 31236 loss= 0.0165165513753891\n",
      "test_data MSELoss:(pred-real)/real= 0.003716505087342941\n",
      "epoch= 336 iteration= 31249 loss= 0.000547036062926054\n",
      "epoch= 336 iteration= 31269 loss= 0.003325481666252017\n",
      "epoch= 336 iteration= 31289 loss= 0.0010422035120427608\n",
      "epoch= 336 iteration= 31309 loss= 0.0011629215441644192\n",
      "epoch= 336 iteration= 31329 loss= 0.0164871197193861\n",
      "test_data MSELoss:(pred-real)/real= 0.0037096117545540133\n",
      "epoch= 337 iteration= 31342 loss= 0.0005476140649989247\n",
      "epoch= 337 iteration= 31362 loss= 0.0033191549591720104\n",
      "epoch= 337 iteration= 31382 loss= 0.0010396275902166963\n",
      "epoch= 337 iteration= 31402 loss= 0.0011620144359767437\n",
      "epoch= 337 iteration= 31422 loss= 0.016464296728372574\n",
      "test_data MSELoss:(pred-real)/real= 0.003702805047699561\n",
      "epoch= 338 iteration= 31435 loss= 0.0005461498512886465\n",
      "epoch= 338 iteration= 31455 loss= 0.003309061983600259\n",
      "epoch= 338 iteration= 31475 loss= 0.0010373778641223907\n",
      "epoch= 338 iteration= 31495 loss= 0.001161055639386177\n",
      "epoch= 338 iteration= 31515 loss= 0.01644541136920452\n",
      "test_data MSELoss:(pred-real)/real= 0.0036957260066022477\n",
      "epoch= 339 iteration= 31528 loss= 0.0005464456626214087\n",
      "epoch= 339 iteration= 31548 loss= 0.0033030197955667973\n",
      "epoch= 339 iteration= 31568 loss= 0.0010348347714170814\n",
      "epoch= 339 iteration= 31588 loss= 0.0011605466715991497\n",
      "epoch= 339 iteration= 31608 loss= 0.016418352723121643\n",
      "test_data MSELoss:(pred-real)/real= 0.0036890501829071175\n",
      "epoch= 340 iteration= 31621 loss= 0.0005478215753100812\n",
      "epoch= 340 iteration= 31641 loss= 0.0032969291787594557\n",
      "epoch= 340 iteration= 31661 loss= 0.001033869106322527\n",
      "epoch= 340 iteration= 31681 loss= 0.0011595864780247211\n",
      "epoch= 340 iteration= 31701 loss= 0.0163944773375988\n",
      "test_data MSELoss:(pred-real)/real= 0.003681754370013045\n",
      "epoch= 341 iteration= 31714 loss= 0.0005472714547067881\n",
      "epoch= 341 iteration= 31734 loss= 0.0032914583571255207\n",
      "epoch= 341 iteration= 31754 loss= 0.0010311227524653077\n",
      "epoch= 341 iteration= 31774 loss= 0.0011587050976231694\n",
      "epoch= 341 iteration= 31794 loss= 0.01636841520667076\n",
      "test_data MSELoss:(pred-real)/real= 0.003675128591971265\n",
      "epoch= 342 iteration= 31807 loss= 0.000548747309949249\n",
      "epoch= 342 iteration= 31827 loss= 0.0032830429263412952\n",
      "epoch= 342 iteration= 31847 loss= 0.0010301436996087432\n",
      "epoch= 342 iteration= 31867 loss= 0.0011575184762477875\n",
      "epoch= 342 iteration= 31887 loss= 0.016347212716937065\n",
      "test_data MSELoss:(pred-real)/real= 0.003668336527577291\n",
      "epoch= 343 iteration= 31900 loss= 0.0005476808291859925\n",
      "epoch= 343 iteration= 31920 loss= 0.003274829126894474\n",
      "epoch= 343 iteration= 31940 loss= 0.0010281380964443088\n",
      "epoch= 343 iteration= 31960 loss= 0.001157041871920228\n",
      "epoch= 343 iteration= 31980 loss= 0.01632094755768776\n",
      "test_data MSELoss:(pred-real)/real= 0.003661161101061023\n",
      "epoch= 344 iteration= 31993 loss= 0.0005479543469846249\n",
      "epoch= 344 iteration= 32013 loss= 0.003266027430072427\n",
      "epoch= 344 iteration= 32033 loss= 0.0010267185280099511\n",
      "epoch= 344 iteration= 32053 loss= 0.001155748963356018\n",
      "epoch= 344 iteration= 32073 loss= 0.016295703127980232\n",
      "test_data MSELoss:(pred-real)/real= 0.0036543038570218617\n",
      "epoch= 345 iteration= 32086 loss= 0.000546461611520499\n",
      "epoch= 345 iteration= 32106 loss= 0.0032623661682009697\n",
      "epoch= 345 iteration= 32126 loss= 0.0010237651877105236\n",
      "epoch= 345 iteration= 32146 loss= 0.0011550815543159842\n",
      "epoch= 345 iteration= 32166 loss= 0.016269197687506676\n",
      "test_data MSELoss:(pred-real)/real= 0.0036475200839858088\n",
      "epoch= 346 iteration= 32179 loss= 0.0005467825103551149\n",
      "epoch= 346 iteration= 32199 loss= 0.0032531272154301405\n",
      "epoch= 346 iteration= 32219 loss= 0.0010219470132142305\n",
      "epoch= 346 iteration= 32239 loss= 0.0011543467408046126\n",
      "epoch= 346 iteration= 32259 loss= 0.016249466687440872\n",
      "test_data MSELoss:(pred-real)/real= 0.0036402622095516157\n",
      "epoch= 347 iteration= 32272 loss= 0.0005467284936457872\n",
      "epoch= 347 iteration= 32292 loss= 0.0032446046825498343\n",
      "epoch= 347 iteration= 32312 loss= 0.001020857598632574\n",
      "epoch= 347 iteration= 32332 loss= 0.0011528582544997334\n",
      "epoch= 347 iteration= 32352 loss= 0.01622440293431282\n",
      "test_data MSELoss:(pred-real)/real= 0.003633858849449704\n",
      "epoch= 348 iteration= 32365 loss= 0.0005450817989185452\n",
      "epoch= 348 iteration= 32385 loss= 0.003237126860767603\n",
      "epoch= 348 iteration= 32405 loss= 0.0010192544432356954\n",
      "epoch= 348 iteration= 32425 loss= 0.0011524149449542165\n",
      "epoch= 348 iteration= 32445 loss= 0.01620146632194519\n",
      "test_data MSELoss:(pred-real)/real= 0.0036269289169770977\n",
      "epoch= 349 iteration= 32458 loss= 0.0005474704666994512\n",
      "epoch= 349 iteration= 32478 loss= 0.0032310967799276114\n",
      "epoch= 349 iteration= 32498 loss= 0.0010173326591029763\n",
      "epoch= 349 iteration= 32518 loss= 0.001151063828729093\n",
      "epoch= 349 iteration= 32538 loss= 0.016173098236322403\n",
      "test_data MSELoss:(pred-real)/real= 0.0036202603498370284\n",
      "epoch= 350 iteration= 32551 loss= 0.0005468258750624955\n",
      "epoch= 350 iteration= 32571 loss= 0.0032222599256783724\n",
      "epoch= 350 iteration= 32591 loss= 0.0010156944626942277\n",
      "epoch= 350 iteration= 32611 loss= 0.0011507563758641481\n",
      "epoch= 350 iteration= 32631 loss= 0.016149334609508514\n",
      "test_data MSELoss:(pred-real)/real= 0.003613447518243144\n",
      "epoch= 351 iteration= 32644 loss= 0.0005472619086503983\n",
      "epoch= 351 iteration= 32664 loss= 0.003215776989236474\n",
      "epoch= 351 iteration= 32684 loss= 0.0010129695292562246\n",
      "epoch= 351 iteration= 32704 loss= 0.0011495682410895824\n",
      "epoch= 351 iteration= 32724 loss= 0.016126101836562157\n",
      "test_data MSELoss:(pred-real)/real= 0.0036068346558345687\n",
      "epoch= 352 iteration= 32737 loss= 0.0005479249521158636\n",
      "epoch= 352 iteration= 32757 loss= 0.003210431430488825\n",
      "epoch= 352 iteration= 32777 loss= 0.0010109464637935162\n",
      "epoch= 352 iteration= 32797 loss= 0.0011487676529213786\n",
      "epoch= 352 iteration= 32817 loss= 0.016099169850349426\n",
      "test_data MSELoss:(pred-real)/real= 0.0036002428645992447\n",
      "epoch= 353 iteration= 32830 loss= 0.0005453749909065664\n",
      "epoch= 353 iteration= 32850 loss= 0.003203363623470068\n",
      "epoch= 353 iteration= 32870 loss= 0.0010091050062328577\n",
      "epoch= 353 iteration= 32890 loss= 0.00114833889529109\n",
      "epoch= 353 iteration= 32910 loss= 0.016078464686870575\n",
      "test_data MSELoss:(pred-real)/real= 0.003592947245730708\n",
      "epoch= 354 iteration= 32923 loss= 0.0005448190495371819\n",
      "epoch= 354 iteration= 32943 loss= 0.0031968741677701473\n",
      "epoch= 354 iteration= 32963 loss= 0.0010082537773996592\n",
      "epoch= 354 iteration= 32983 loss= 0.0011469426099210978\n",
      "epoch= 354 iteration= 33003 loss= 0.01605262979865074\n",
      "test_data MSELoss:(pred-real)/real= 0.0035864261768033933\n",
      "epoch= 355 iteration= 33016 loss= 0.0005441927351057529\n",
      "epoch= 355 iteration= 33036 loss= 0.003188943490386009\n",
      "epoch= 355 iteration= 33056 loss= 0.0010074488818645477\n",
      "epoch= 355 iteration= 33076 loss= 0.001146480324678123\n",
      "epoch= 355 iteration= 33096 loss= 0.016025565564632416\n",
      "test_data MSELoss:(pred-real)/real= 0.003579759937969761\n",
      "epoch= 356 iteration= 33109 loss= 0.0005440955283120275\n",
      "epoch= 356 iteration= 33129 loss= 0.003178877755999565\n",
      "epoch= 356 iteration= 33149 loss= 0.00100485619623214\n",
      "epoch= 356 iteration= 33169 loss= 0.0011456890497356653\n",
      "epoch= 356 iteration= 33189 loss= 0.016003796830773354\n",
      "test_data MSELoss:(pred-real)/real= 0.0035725494445715514\n",
      "epoch= 357 iteration= 33202 loss= 0.0005433622281998396\n",
      "epoch= 357 iteration= 33222 loss= 0.0031739952974021435\n",
      "epoch= 357 iteration= 33242 loss= 0.0010024108923971653\n",
      "epoch= 357 iteration= 33262 loss= 0.0011448285076767206\n",
      "epoch= 357 iteration= 33282 loss= 0.015977943316102028\n",
      "test_data MSELoss:(pred-real)/real= 0.003565749159962353\n",
      "epoch= 358 iteration= 33295 loss= 0.0005432427860796452\n",
      "epoch= 358 iteration= 33315 loss= 0.0031650455202907324\n",
      "epoch= 358 iteration= 33335 loss= 0.0010007219389081001\n",
      "epoch= 358 iteration= 33355 loss= 0.0011438895016908646\n",
      "epoch= 358 iteration= 33375 loss= 0.015955708920955658\n",
      "test_data MSELoss:(pred-real)/real= 0.0035591196178251672\n",
      "epoch= 359 iteration= 33388 loss= 0.0005411558086052537\n",
      "epoch= 359 iteration= 33408 loss= 0.0031589313875883818\n",
      "epoch= 359 iteration= 33428 loss= 0.0009995164582505822\n",
      "epoch= 359 iteration= 33448 loss= 0.0011427607387304306\n",
      "epoch= 359 iteration= 33468 loss= 0.01593059115111828\n",
      "test_data MSELoss:(pred-real)/real= 0.0035521682131932015\n",
      "epoch= 360 iteration= 33481 loss= 0.0005412698956206441\n",
      "epoch= 360 iteration= 33501 loss= 0.0031515357550233603\n",
      "epoch= 360 iteration= 33521 loss= 0.0009970766259357333\n",
      "epoch= 360 iteration= 33541 loss= 0.001142019173130393\n",
      "epoch= 360 iteration= 33561 loss= 0.015902167186141014\n",
      "test_data MSELoss:(pred-real)/real= 0.003545815901209911\n",
      "epoch= 361 iteration= 33574 loss= 0.0005420919042080641\n",
      "epoch= 361 iteration= 33594 loss= 0.0031443857587873936\n",
      "epoch= 361 iteration= 33614 loss= 0.000995296984910965\n",
      "epoch= 361 iteration= 33634 loss= 0.0011413313914090395\n",
      "epoch= 361 iteration= 33654 loss= 0.015879705548286438\n",
      "test_data MSELoss:(pred-real)/real= 0.003538323734473023\n",
      "epoch= 362 iteration= 33667 loss= 0.0005423089023679495\n",
      "epoch= 362 iteration= 33687 loss= 0.0031381859444081783\n",
      "epoch= 362 iteration= 33707 loss= 0.0009938464500010014\n",
      "epoch= 362 iteration= 33727 loss= 0.001140176085755229\n",
      "epoch= 362 iteration= 33747 loss= 0.015853386372327805\n",
      "test_data MSELoss:(pred-real)/real= 0.00353223406839081\n",
      "epoch= 363 iteration= 33760 loss= 0.0005413434118963778\n",
      "epoch= 363 iteration= 33780 loss= 0.0031303423456847668\n",
      "epoch= 363 iteration= 33800 loss= 0.000992274610325694\n",
      "epoch= 363 iteration= 33820 loss= 0.0011394510511308908\n",
      "epoch= 363 iteration= 33840 loss= 0.01582786627113819\n",
      "test_data MSELoss:(pred-real)/real= 0.003525344014633447\n",
      "epoch= 364 iteration= 33853 loss= 0.0005397319910116494\n",
      "epoch= 364 iteration= 33873 loss= 0.0031226922292262316\n",
      "epoch= 364 iteration= 33893 loss= 0.0009909142972901464\n",
      "epoch= 364 iteration= 33913 loss= 0.0011387962149456143\n",
      "epoch= 364 iteration= 33933 loss= 0.015805432572960854\n",
      "test_data MSELoss:(pred-real)/real= 0.003518843376595113\n",
      "epoch= 365 iteration= 33946 loss= 0.0005375295295380056\n",
      "epoch= 365 iteration= 33966 loss= 0.003116210922598839\n",
      "epoch= 365 iteration= 33986 loss= 0.000988632906228304\n",
      "epoch= 365 iteration= 34006 loss= 0.0011376681504771113\n",
      "epoch= 365 iteration= 34026 loss= 0.015782197937369347\n",
      "test_data MSELoss:(pred-real)/real= 0.003512771328031603\n",
      "epoch= 366 iteration= 34039 loss= 0.0005369489081203938\n",
      "epoch= 366 iteration= 34059 loss= 0.0031110141426324844\n",
      "epoch= 366 iteration= 34079 loss= 0.0009863257873803377\n",
      "epoch= 366 iteration= 34099 loss= 0.0011369397398084402\n",
      "epoch= 366 iteration= 34119 loss= 0.015755794942378998\n",
      "test_data MSELoss:(pred-real)/real= 0.0035050249555044705\n",
      "epoch= 367 iteration= 34132 loss= 0.0005371932638809085\n",
      "epoch= 367 iteration= 34152 loss= 0.0031030455138534307\n",
      "epoch= 367 iteration= 34172 loss= 0.00098564219661057\n",
      "epoch= 367 iteration= 34192 loss= 0.001136008882895112\n",
      "epoch= 367 iteration= 34212 loss= 0.015727996826171875\n",
      "test_data MSELoss:(pred-real)/real= 0.003498601972953313\n",
      "epoch= 368 iteration= 34225 loss= 0.0005356591427698731\n",
      "epoch= 368 iteration= 34245 loss= 0.003095818217843771\n",
      "epoch= 368 iteration= 34265 loss= 0.0009838424157351255\n",
      "epoch= 368 iteration= 34285 loss= 0.0011347096879035234\n",
      "epoch= 368 iteration= 34305 loss= 0.015708213672041893\n",
      "test_data MSELoss:(pred-real)/real= 0.0034918237619826365\n",
      "epoch= 369 iteration= 34318 loss= 0.0005354431923478842\n",
      "epoch= 369 iteration= 34338 loss= 0.003088729688897729\n",
      "epoch= 369 iteration= 34358 loss= 0.000982307712547481\n",
      "epoch= 369 iteration= 34378 loss= 0.0011346482206135988\n",
      "epoch= 369 iteration= 34398 loss= 0.015678314492106438\n",
      "test_data MSELoss:(pred-real)/real= 0.003485094820563164\n",
      "epoch= 370 iteration= 34411 loss= 0.0005355693283490837\n",
      "epoch= 370 iteration= 34431 loss= 0.003084232797846198\n",
      "epoch= 370 iteration= 34451 loss= 0.0009797685779631138\n",
      "epoch= 370 iteration= 34471 loss= 0.001133904093876481\n",
      "epoch= 370 iteration= 34491 loss= 0.015657540410757065\n",
      "test_data MSELoss:(pred-real)/real= 0.0034783388255164027\n",
      "epoch= 371 iteration= 34504 loss= 0.000534184102434665\n",
      "epoch= 371 iteration= 34524 loss= 0.003076909575611353\n",
      "epoch= 371 iteration= 34544 loss= 0.000978120369836688\n",
      "epoch= 371 iteration= 34564 loss= 0.0011331846471875906\n",
      "epoch= 371 iteration= 34584 loss= 0.015629388391971588\n",
      "test_data MSELoss:(pred-real)/real= 0.003471636006401645\n",
      "epoch= 372 iteration= 34597 loss= 0.0005337502807378769\n",
      "epoch= 372 iteration= 34617 loss= 0.003069531172513962\n",
      "epoch= 372 iteration= 34637 loss= 0.0009770620381459594\n",
      "epoch= 372 iteration= 34657 loss= 0.001132274279370904\n",
      "epoch= 372 iteration= 34677 loss= 0.015607541427016258\n",
      "test_data MSELoss:(pred-real)/real= 0.003465054944778482\n",
      "epoch= 373 iteration= 34690 loss= 0.0005325780948624015\n",
      "epoch= 373 iteration= 34710 loss= 0.003063028212636709\n",
      "epoch= 373 iteration= 34730 loss= 0.0009750311728566885\n",
      "epoch= 373 iteration= 34750 loss= 0.0011311237467452884\n",
      "epoch= 373 iteration= 34770 loss= 0.015576486475765705\n",
      "test_data MSELoss:(pred-real)/real= 0.003457952226098213\n",
      "epoch= 374 iteration= 34783 loss= 0.0005310144042596221\n",
      "epoch= 374 iteration= 34803 loss= 0.003056552493944764\n",
      "epoch= 374 iteration= 34823 loss= 0.0009734201012179255\n",
      "epoch= 374 iteration= 34843 loss= 0.0011303125647827983\n",
      "epoch= 374 iteration= 34863 loss= 0.015557544305920601\n",
      "test_data MSELoss:(pred-real)/real= 0.0034514981477210918\n",
      "epoch= 375 iteration= 34876 loss= 0.000529264158103615\n",
      "epoch= 375 iteration= 34896 loss= 0.0030515838880091906\n",
      "epoch= 375 iteration= 34916 loss= 0.000971333822235465\n",
      "epoch= 375 iteration= 34936 loss= 0.0011296928860247135\n",
      "epoch= 375 iteration= 34956 loss= 0.015527155250310898\n",
      "test_data MSELoss:(pred-real)/real= 0.0034443278305439484\n",
      "epoch= 376 iteration= 34969 loss= 0.0005309352418407798\n",
      "epoch= 376 iteration= 34989 loss= 0.0030422471463680267\n",
      "epoch= 376 iteration= 35009 loss= 0.0009708747966215014\n",
      "epoch= 376 iteration= 35029 loss= 0.0011288524838164449\n",
      "epoch= 376 iteration= 35049 loss= 0.015502560883760452\n",
      "test_data MSELoss:(pred-real)/real= 0.0034380005155172613\n",
      "epoch= 377 iteration= 35062 loss= 0.0005305100348778069\n",
      "epoch= 377 iteration= 35082 loss= 0.003034852212294936\n",
      "epoch= 377 iteration= 35102 loss= 0.000969165877904743\n",
      "epoch= 377 iteration= 35122 loss= 0.001128380186855793\n",
      "epoch= 377 iteration= 35142 loss= 0.015477658249437809\n",
      "test_data MSELoss:(pred-real)/real= 0.003431676363106817\n",
      "epoch= 378 iteration= 35155 loss= 0.0005287356907501817\n",
      "epoch= 378 iteration= 35175 loss= 0.0030278495978564024\n",
      "epoch= 378 iteration= 35195 loss= 0.000967636180575937\n",
      "epoch= 378 iteration= 35215 loss= 0.001127235358580947\n",
      "epoch= 378 iteration= 35235 loss= 0.015456318855285645\n",
      "test_data MSELoss:(pred-real)/real= 0.0034250066964887083\n",
      "epoch= 379 iteration= 35248 loss= 0.0005265664658509195\n",
      "epoch= 379 iteration= 35268 loss= 0.003022480756044388\n",
      "epoch= 379 iteration= 35288 loss= 0.0009655593894422054\n",
      "epoch= 379 iteration= 35308 loss= 0.0011260475730523467\n",
      "epoch= 379 iteration= 35328 loss= 0.01543219480663538\n",
      "test_data MSELoss:(pred-real)/real= 0.003418431707864834\n",
      "epoch= 380 iteration= 35341 loss= 0.0005271213012747467\n",
      "epoch= 380 iteration= 35361 loss= 0.0030155382119119167\n",
      "epoch= 380 iteration= 35381 loss= 0.0009645226527936757\n",
      "epoch= 380 iteration= 35401 loss= 0.0011259117163717747\n",
      "epoch= 380 iteration= 35421 loss= 0.015401314944028854\n",
      "test_data MSELoss:(pred-real)/real= 0.003410762033632232\n",
      "epoch= 381 iteration= 35434 loss= 0.0005268515087664127\n",
      "epoch= 381 iteration= 35454 loss= 0.0030099102295935154\n",
      "epoch= 381 iteration= 35474 loss= 0.0009616957977414131\n",
      "epoch= 381 iteration= 35494 loss= 0.0011250934330746531\n",
      "epoch= 381 iteration= 35514 loss= 0.015376577153801918\n",
      "test_data MSELoss:(pred-real)/real= 0.0034042669576592743\n",
      "epoch= 382 iteration= 35527 loss= 0.0005248733796179295\n",
      "epoch= 382 iteration= 35547 loss= 0.0030008796602487564\n",
      "epoch= 382 iteration= 35567 loss= 0.0009607787942513824\n",
      "epoch= 382 iteration= 35587 loss= 0.0011246176436543465\n",
      "epoch= 382 iteration= 35607 loss= 0.01535432506352663\n",
      "test_data MSELoss:(pred-real)/real= 0.003397969807135976\n",
      "epoch= 383 iteration= 35620 loss= 0.0005229206290096045\n",
      "epoch= 383 iteration= 35640 loss= 0.0029944886919111013\n",
      "epoch= 383 iteration= 35660 loss= 0.0009604815859347582\n",
      "epoch= 383 iteration= 35680 loss= 0.0011231302050873637\n",
      "epoch= 383 iteration= 35700 loss= 0.015326879918575287\n",
      "test_data MSELoss:(pred-real)/real= 0.003390948508038289\n",
      "epoch= 384 iteration= 35713 loss= 0.0005223514162935317\n",
      "epoch= 384 iteration= 35733 loss= 0.0029866169206798077\n",
      "epoch= 384 iteration= 35753 loss= 0.0009585466468706727\n",
      "epoch= 384 iteration= 35773 loss= 0.001123042544350028\n",
      "epoch= 384 iteration= 35793 loss= 0.015304441563785076\n",
      "test_data MSELoss:(pred-real)/real= 0.0033845946826558146\n",
      "epoch= 385 iteration= 35806 loss= 0.0005224748165346682\n",
      "epoch= 385 iteration= 35826 loss= 0.0029796669259667397\n",
      "epoch= 385 iteration= 35846 loss= 0.0009560715407133102\n",
      "epoch= 385 iteration= 35866 loss= 0.0011215764097869396\n",
      "epoch= 385 iteration= 35886 loss= 0.015275356359779835\n",
      "test_data MSELoss:(pred-real)/real= 0.0033776789981250963\n",
      "epoch= 386 iteration= 35899 loss= 0.0005228501977398992\n",
      "epoch= 386 iteration= 35919 loss= 0.002975172596052289\n",
      "epoch= 386 iteration= 35939 loss= 0.00095502840122208\n",
      "epoch= 386 iteration= 35959 loss= 0.0011213200632482767\n",
      "epoch= 386 iteration= 35979 loss= 0.015252983197569847\n",
      "test_data MSELoss:(pred-real)/real= 0.003370775821773956\n",
      "epoch= 387 iteration= 35992 loss= 0.0005218486185185611\n",
      "epoch= 387 iteration= 36012 loss= 0.00296927522867918\n",
      "epoch= 387 iteration= 36032 loss= 0.0009535825811326504\n",
      "epoch= 387 iteration= 36052 loss= 0.001120633096434176\n",
      "epoch= 387 iteration= 36072 loss= 0.015227774158120155\n",
      "test_data MSELoss:(pred-real)/real= 0.0033647174261406893\n",
      "epoch= 388 iteration= 36085 loss= 0.0005201285821385682\n",
      "epoch= 388 iteration= 36105 loss= 0.0029589044861495495\n",
      "epoch= 388 iteration= 36125 loss= 0.0009522743057459593\n",
      "epoch= 388 iteration= 36145 loss= 0.0011194045655429363\n",
      "epoch= 388 iteration= 36165 loss= 0.015200408175587654\n",
      "test_data MSELoss:(pred-real)/real= 0.003357941970332629\n",
      "epoch= 389 iteration= 36178 loss= 0.0005188299110159278\n",
      "epoch= 389 iteration= 36198 loss= 0.00295556359924376\n",
      "epoch= 389 iteration= 36218 loss= 0.0009505219059064984\n",
      "epoch= 389 iteration= 36238 loss= 0.0011187978088855743\n",
      "epoch= 389 iteration= 36258 loss= 0.015180351212620735\n",
      "test_data MSELoss:(pred-real)/real= 0.0033507442890873384\n",
      "epoch= 390 iteration= 36271 loss= 0.0005180219886824489\n",
      "epoch= 390 iteration= 36291 loss= 0.0029455802869051695\n",
      "epoch= 390 iteration= 36311 loss= 0.0009487525094300508\n",
      "epoch= 390 iteration= 36331 loss= 0.0011180341243743896\n",
      "epoch= 390 iteration= 36351 loss= 0.015150399878621101\n",
      "test_data MSELoss:(pred-real)/real= 0.003344298392120335\n",
      "epoch= 391 iteration= 36364 loss= 0.0005169764626771212\n",
      "epoch= 391 iteration= 36384 loss= 0.0029384917579591274\n",
      "epoch= 391 iteration= 36404 loss= 0.0009480029111728072\n",
      "epoch= 391 iteration= 36424 loss= 0.0011177308624610305\n",
      "epoch= 391 iteration= 36444 loss= 0.015123022720217705\n",
      "test_data MSELoss:(pred-real)/real= 0.0033376647172392243\n",
      "epoch= 392 iteration= 36457 loss= 0.0005175633705221117\n",
      "epoch= 392 iteration= 36477 loss= 0.0029318532906472683\n",
      "epoch= 392 iteration= 36497 loss= 0.0009464822942391038\n",
      "epoch= 392 iteration= 36517 loss= 0.0011166527401655912\n",
      "epoch= 392 iteration= 36537 loss= 0.01510266400873661\n",
      "test_data MSELoss:(pred-real)/real= 0.0033311142405081126\n",
      "epoch= 393 iteration= 36550 loss= 0.000515007646754384\n",
      "epoch= 393 iteration= 36570 loss= 0.002925609238445759\n",
      "epoch= 393 iteration= 36590 loss= 0.0009444725583307445\n",
      "epoch= 393 iteration= 36610 loss= 0.001116346218623221\n",
      "epoch= 393 iteration= 36630 loss= 0.015075056813657284\n",
      "test_data MSELoss:(pred-real)/real= 0.0033248270564298662\n",
      "epoch= 394 iteration= 36643 loss= 0.0005130496574565768\n",
      "epoch= 394 iteration= 36663 loss= 0.0029188552871346474\n",
      "epoch= 394 iteration= 36683 loss= 0.0009438388515263796\n",
      "epoch= 394 iteration= 36703 loss= 0.0011147349141538143\n",
      "epoch= 394 iteration= 36723 loss= 0.01505266036838293\n",
      "test_data MSELoss:(pred-real)/real= 0.003317456860612664\n",
      "epoch= 395 iteration= 36736 loss= 0.0005152954836376011\n",
      "epoch= 395 iteration= 36756 loss= 0.00291138282045722\n",
      "epoch= 395 iteration= 36776 loss= 0.0009421999566257\n",
      "epoch= 395 iteration= 36796 loss= 0.0011143761221319437\n",
      "epoch= 395 iteration= 36816 loss= 0.015022976323962212\n",
      "test_data MSELoss:(pred-real)/real= 0.0033110155749859083\n",
      "epoch= 396 iteration= 36829 loss= 0.0005167676135897636\n",
      "epoch= 396 iteration= 36849 loss= 0.0029056579805910587\n",
      "epoch= 396 iteration= 36869 loss= 0.0009415897075086832\n",
      "epoch= 396 iteration= 36889 loss= 0.0011134595843032002\n",
      "epoch= 396 iteration= 36909 loss= 0.01499913539737463\n",
      "test_data MSELoss:(pred-real)/real= 0.0033045004109024173\n",
      "epoch= 397 iteration= 36922 loss= 0.0005152012454345822\n",
      "epoch= 397 iteration= 36942 loss= 0.0028976150788366795\n",
      "epoch= 397 iteration= 36962 loss= 0.0009398177498951554\n",
      "epoch= 397 iteration= 36982 loss= 0.001112842932343483\n",
      "epoch= 397 iteration= 37002 loss= 0.014971883967518806\n",
      "test_data MSELoss:(pred-real)/real= 0.003297762098049538\n",
      "epoch= 398 iteration= 37015 loss= 0.0005143897142261267\n",
      "epoch= 398 iteration= 37035 loss= 0.0028909274842590094\n",
      "epoch= 398 iteration= 37055 loss= 0.0009378665126860142\n",
      "epoch= 398 iteration= 37075 loss= 0.0011120750568807125\n",
      "epoch= 398 iteration= 37095 loss= 0.014945306815207005\n",
      "test_data MSELoss:(pred-real)/real= 0.00329160892094175\n",
      "epoch= 399 iteration= 37108 loss= 0.0005135475075803697\n",
      "epoch= 399 iteration= 37128 loss= 0.0028822626918554306\n",
      "epoch= 399 iteration= 37148 loss= 0.0009366546291857958\n",
      "epoch= 399 iteration= 37168 loss= 0.0011118128895759583\n",
      "epoch= 399 iteration= 37188 loss= 0.014923442155122757\n",
      "test_data MSELoss:(pred-real)/real= 0.003284492659279042\n",
      "epoch= 400 iteration= 37201 loss= 0.0005130236968398094\n",
      "epoch= 400 iteration= 37221 loss= 0.0028785299509763718\n",
      "epoch= 400 iteration= 37241 loss= 0.0009361591655761003\n",
      "epoch= 400 iteration= 37261 loss= 0.0011105441953986883\n",
      "epoch= 400 iteration= 37281 loss= 0.014897249639034271\n",
      "test_data MSELoss:(pred-real)/real= 0.003277654520287696\n",
      "epoch= 401 iteration= 37294 loss= 0.000511008664034307\n",
      "epoch= 401 iteration= 37314 loss= 0.0028677682857960463\n",
      "epoch= 401 iteration= 37334 loss= 0.0009341070544905961\n",
      "epoch= 401 iteration= 37354 loss= 0.0011097436072304845\n",
      "epoch= 401 iteration= 37374 loss= 0.01486978679895401\n",
      "test_data MSELoss:(pred-real)/real= 0.0032715557996804514\n",
      "epoch= 402 iteration= 37387 loss= 0.0005112275248393416\n",
      "epoch= 402 iteration= 37407 loss= 0.0028625703416764736\n",
      "epoch= 402 iteration= 37427 loss= 0.0009329425520263612\n",
      "epoch= 402 iteration= 37447 loss= 0.001108960947021842\n",
      "epoch= 402 iteration= 37467 loss= 0.014846455305814743\n",
      "test_data MSELoss:(pred-real)/real= 0.0032650703409065804\n",
      "epoch= 403 iteration= 37480 loss= 0.0005086922901682556\n",
      "epoch= 403 iteration= 37500 loss= 0.002858677413314581\n",
      "epoch= 403 iteration= 37520 loss= 0.0009303786791861057\n",
      "epoch= 403 iteration= 37540 loss= 0.0011081058764830232\n",
      "epoch= 403 iteration= 37560 loss= 0.014821002259850502\n",
      "test_data MSELoss:(pred-real)/real= 0.0032582328874721294\n",
      "epoch= 404 iteration= 37573 loss= 0.0005077736568637192\n",
      "epoch= 404 iteration= 37593 loss= 0.0028499909676611423\n",
      "epoch= 404 iteration= 37613 loss= 0.0009300692472606897\n",
      "epoch= 404 iteration= 37633 loss= 0.0011072217021137476\n",
      "epoch= 404 iteration= 37653 loss= 0.014793920330703259\n",
      "test_data MSELoss:(pred-real)/real= 0.003251103522618198\n",
      "epoch= 405 iteration= 37666 loss= 0.0005072415806353092\n",
      "epoch= 405 iteration= 37686 loss= 0.0028422398027032614\n",
      "epoch= 405 iteration= 37706 loss= 0.0009281518869102001\n",
      "epoch= 405 iteration= 37726 loss= 0.0011065652361139655\n",
      "epoch= 405 iteration= 37746 loss= 0.014771286398172379\n",
      "test_data MSELoss:(pred-real)/real= 0.003244641670284586\n",
      "epoch= 406 iteration= 37759 loss= 0.0005071410560049117\n",
      "epoch= 406 iteration= 37779 loss= 0.0028346097096800804\n",
      "epoch= 406 iteration= 37799 loss= 0.0009286427521146834\n",
      "epoch= 406 iteration= 37819 loss= 0.0011059818789362907\n",
      "epoch= 406 iteration= 37839 loss= 0.014742004685103893\n",
      "test_data MSELoss:(pred-real)/real= 0.00323841221527093\n",
      "epoch= 407 iteration= 37852 loss= 0.0005044036079198122\n",
      "epoch= 407 iteration= 37872 loss= 0.0028288967441767454\n",
      "epoch= 407 iteration= 37892 loss= 0.0009257625206373632\n",
      "epoch= 407 iteration= 37912 loss= 0.00110525730997324\n",
      "epoch= 407 iteration= 37932 loss= 0.014719151891767979\n",
      "test_data MSELoss:(pred-real)/real= 0.003231668075184441\n",
      "epoch= 408 iteration= 37945 loss= 0.0005044144345447421\n",
      "epoch= 408 iteration= 37965 loss= 0.0028221297543495893\n",
      "epoch= 408 iteration= 37985 loss= 0.0009245953406207263\n",
      "epoch= 408 iteration= 38005 loss= 0.0011042524129152298\n",
      "epoch= 408 iteration= 38025 loss= 0.014688887633383274\n",
      "test_data MSELoss:(pred-real)/real= 0.003224617180724939\n",
      "epoch= 409 iteration= 38038 loss= 0.0005053673521615565\n",
      "epoch= 409 iteration= 38058 loss= 0.002815689193084836\n",
      "epoch= 409 iteration= 38078 loss= 0.0009233058663085103\n",
      "epoch= 409 iteration= 38098 loss= 0.0011035108473151922\n",
      "epoch= 409 iteration= 38118 loss= 0.014667347073554993\n",
      "test_data MSELoss:(pred-real)/real= 0.003218119035914747\n",
      "epoch= 410 iteration= 38131 loss= 0.0005045053549110889\n",
      "epoch= 410 iteration= 38151 loss= 0.002808327553793788\n",
      "epoch= 410 iteration= 38171 loss= 0.0009219479979947209\n",
      "epoch= 410 iteration= 38191 loss= 0.0011028943117707968\n",
      "epoch= 410 iteration= 38211 loss= 0.01464487612247467\n",
      "test_data MSELoss:(pred-real)/real= 0.00321216570187567\n",
      "epoch= 411 iteration= 38224 loss= 0.000503583112731576\n",
      "epoch= 411 iteration= 38244 loss= 0.0028009330853819847\n",
      "epoch= 411 iteration= 38264 loss= 0.0009215029422193766\n",
      "epoch= 411 iteration= 38284 loss= 0.0011020926758646965\n",
      "epoch= 411 iteration= 38304 loss= 0.014616861008107662\n",
      "test_data MSELoss:(pred-real)/real= 0.003205399562527115\n",
      "epoch= 412 iteration= 38317 loss= 0.0005027339211665094\n",
      "epoch= 412 iteration= 38337 loss= 0.0027950769290328026\n",
      "epoch= 412 iteration= 38357 loss= 0.0009195911698043346\n",
      "epoch= 412 iteration= 38377 loss= 0.0011014100164175034\n",
      "epoch= 412 iteration= 38397 loss= 0.014590869657695293\n",
      "test_data MSELoss:(pred-real)/real= 0.003198374622216862\n",
      "epoch= 413 iteration= 38410 loss= 0.0005004324484616518\n",
      "epoch= 413 iteration= 38430 loss= 0.00278615509159863\n",
      "epoch= 413 iteration= 38450 loss= 0.0009176807361654937\n",
      "epoch= 413 iteration= 38470 loss= 0.0011002810206264257\n",
      "epoch= 413 iteration= 38490 loss= 0.01456394512206316\n",
      "test_data MSELoss:(pred-real)/real= 0.0031916715314663532\n",
      "epoch= 414 iteration= 38503 loss= 0.0004995395429432392\n",
      "epoch= 414 iteration= 38523 loss= 0.002780808601528406\n",
      "epoch= 414 iteration= 38543 loss= 0.000917239929549396\n",
      "epoch= 414 iteration= 38563 loss= 0.001099849701859057\n",
      "epoch= 414 iteration= 38583 loss= 0.014539200812578201\n",
      "test_data MSELoss:(pred-real)/real= 0.0031850518619952104\n",
      "epoch= 415 iteration= 38596 loss= 0.0005007744184695184\n",
      "epoch= 415 iteration= 38616 loss= 0.00277515035122633\n",
      "epoch= 415 iteration= 38636 loss= 0.0009157440508715808\n",
      "epoch= 415 iteration= 38656 loss= 0.0010993152391165495\n",
      "epoch= 415 iteration= 38676 loss= 0.014515621587634087\n",
      "test_data MSELoss:(pred-real)/real= 0.003178635337260655\n",
      "epoch= 416 iteration= 38689 loss= 0.0005004987469874322\n",
      "epoch= 416 iteration= 38709 loss= 0.0027651668060570955\n",
      "epoch= 416 iteration= 38729 loss= 0.0009144387440755963\n",
      "epoch= 416 iteration= 38749 loss= 0.001098470762372017\n",
      "epoch= 416 iteration= 38769 loss= 0.014488883316516876\n",
      "test_data MSELoss:(pred-real)/real= 0.003172148368321359\n",
      "epoch= 417 iteration= 38782 loss= 0.0004989573499187827\n",
      "epoch= 417 iteration= 38802 loss= 0.0027587320655584335\n",
      "epoch= 417 iteration= 38822 loss= 0.0009139536996372044\n",
      "epoch= 417 iteration= 38842 loss= 0.001097335247322917\n",
      "epoch= 417 iteration= 38862 loss= 0.014462491497397423\n",
      "test_data MSELoss:(pred-real)/real= 0.003165775612514052\n",
      "epoch= 418 iteration= 38875 loss= 0.000500981230288744\n",
      "epoch= 418 iteration= 38895 loss= 0.0027521124575287104\n",
      "epoch= 418 iteration= 38915 loss= 0.0009122823248617351\n",
      "epoch= 418 iteration= 38935 loss= 0.0010970510775223374\n",
      "epoch= 418 iteration= 38955 loss= 0.01443500630557537\n",
      "test_data MSELoss:(pred-real)/real= 0.0031587923488890133\n",
      "epoch= 419 iteration= 38968 loss= 0.00050046929391101\n",
      "epoch= 419 iteration= 38988 loss= 0.0027433941140770912\n",
      "epoch= 419 iteration= 39008 loss= 0.0009125273209065199\n",
      "epoch= 419 iteration= 39028 loss= 0.0010961227817460895\n",
      "epoch= 419 iteration= 39048 loss= 0.01441064290702343\n",
      "test_data MSELoss:(pred-real)/real= 0.0031525187918709386\n",
      "epoch= 420 iteration= 39061 loss= 0.0004999915254302323\n",
      "epoch= 420 iteration= 39081 loss= 0.0027384040877223015\n",
      "epoch= 420 iteration= 39101 loss= 0.0009106025099754333\n",
      "epoch= 420 iteration= 39121 loss= 0.0010956720216199756\n",
      "epoch= 420 iteration= 39141 loss= 0.014386201277375221\n",
      "test_data MSELoss:(pred-real)/real= 0.003145471528922725\n",
      "epoch= 421 iteration= 39154 loss= 0.0004993777256458998\n",
      "epoch= 421 iteration= 39174 loss= 0.0027317656204104424\n",
      "epoch= 421 iteration= 39194 loss= 0.0009100046590901911\n",
      "epoch= 421 iteration= 39214 loss= 0.0010944029781967402\n",
      "epoch= 421 iteration= 39234 loss= 0.014361623674631119\n",
      "test_data MSELoss:(pred-real)/real= 0.003139350505080074\n",
      "epoch= 422 iteration= 39247 loss= 0.0004977959906682372\n",
      "epoch= 422 iteration= 39267 loss= 0.002724891994148493\n",
      "epoch= 422 iteration= 39287 loss= 0.0009086276404559612\n",
      "epoch= 422 iteration= 39307 loss= 0.0010937703773379326\n",
      "epoch= 422 iteration= 39327 loss= 0.014336112886667252\n",
      "test_data MSELoss:(pred-real)/real= 0.003132818114762712\n",
      "epoch= 423 iteration= 39340 loss= 0.0004962761304341257\n",
      "epoch= 423 iteration= 39360 loss= 0.002718272153288126\n",
      "epoch= 423 iteration= 39380 loss= 0.0009060046286322176\n",
      "epoch= 423 iteration= 39400 loss= 0.0010932048317044973\n",
      "epoch= 423 iteration= 39420 loss= 0.014307782053947449\n",
      "test_data MSELoss:(pred-real)/real= 0.003126228503080913\n",
      "epoch= 424 iteration= 39433 loss= 0.0004959775251336396\n",
      "epoch= 424 iteration= 39453 loss= 0.002710309810936451\n",
      "epoch= 424 iteration= 39473 loss= 0.0009052114328369498\n",
      "epoch= 424 iteration= 39493 loss= 0.0010920376516878605\n",
      "epoch= 424 iteration= 39513 loss= 0.014281881973147392\n",
      "test_data MSELoss:(pred-real)/real= 0.003119808382406417\n",
      "epoch= 425 iteration= 39526 loss= 0.0004952583112753928\n",
      "epoch= 425 iteration= 39546 loss= 0.0027039628475904465\n",
      "epoch= 425 iteration= 39566 loss= 0.0009040660806931555\n",
      "epoch= 425 iteration= 39586 loss= 0.0010914136655628681\n",
      "epoch= 425 iteration= 39606 loss= 0.014256846159696579\n",
      "test_data MSELoss:(pred-real)/real= 0.0031132996276331446\n",
      "epoch= 426 iteration= 39619 loss= 0.0004942652303725481\n",
      "epoch= 426 iteration= 39639 loss= 0.0026966899167746305\n",
      "epoch= 426 iteration= 39659 loss= 0.0009030414512380958\n",
      "epoch= 426 iteration= 39679 loss= 0.0010911618592217565\n",
      "epoch= 426 iteration= 39699 loss= 0.014232315123081207\n",
      "test_data MSELoss:(pred-real)/real= 0.0031062123048791867\n",
      "epoch= 427 iteration= 39712 loss= 0.0004965462721884251\n",
      "epoch= 427 iteration= 39732 loss= 0.002690720371901989\n",
      "epoch= 427 iteration= 39752 loss= 0.000902188359759748\n",
      "epoch= 427 iteration= 39772 loss= 0.0010900989873334765\n",
      "epoch= 427 iteration= 39792 loss= 0.014206301420927048\n",
      "test_data MSELoss:(pred-real)/real= 0.0030995148376354743\n",
      "epoch= 428 iteration= 39805 loss= 0.0004937604535371065\n",
      "epoch= 428 iteration= 39825 loss= 0.002681441605091095\n",
      "epoch= 428 iteration= 39845 loss= 0.0009020372526720166\n",
      "epoch= 428 iteration= 39865 loss= 0.00108984240796417\n",
      "epoch= 428 iteration= 39885 loss= 0.01418156735599041\n",
      "test_data MSELoss:(pred-real)/real= 0.0030929964365592846\n",
      "epoch= 429 iteration= 39898 loss= 0.0004947544657625258\n",
      "epoch= 429 iteration= 39918 loss= 0.0026744480710476637\n",
      "epoch= 429 iteration= 39938 loss= 0.0009002332808449864\n",
      "epoch= 429 iteration= 39958 loss= 0.0010888376273214817\n",
      "epoch= 429 iteration= 39978 loss= 0.014158024452626705\n",
      "test_data MSELoss:(pred-real)/real= 0.003086832866150265\n",
      "epoch= 430 iteration= 39991 loss= 0.0004946054541505873\n",
      "epoch= 430 iteration= 40011 loss= 0.0026678938884288073\n",
      "epoch= 430 iteration= 40031 loss= 0.0008993762312456965\n",
      "epoch= 430 iteration= 40051 loss= 0.0010875971056520939\n",
      "epoch= 430 iteration= 40071 loss= 0.014131363481283188\n",
      "test_data MSELoss:(pred-real)/real= 0.003079804223186026\n",
      "epoch= 431 iteration= 40084 loss= 0.0004934056778438389\n",
      "epoch= 431 iteration= 40104 loss= 0.0026603550650179386\n",
      "epoch= 431 iteration= 40124 loss= 0.0008968281326815486\n",
      "epoch= 431 iteration= 40144 loss= 0.0010870385449379683\n",
      "epoch= 431 iteration= 40164 loss= 0.014102354645729065\n",
      "test_data MSELoss:(pred-real)/real= 0.0030733162129763514\n",
      "epoch= 432 iteration= 40177 loss= 0.0004912973963655531\n",
      "epoch= 432 iteration= 40197 loss= 0.0026549960020929575\n",
      "epoch= 432 iteration= 40217 loss= 0.000896722951438278\n",
      "epoch= 432 iteration= 40237 loss= 0.0010861181654036045\n",
      "epoch= 432 iteration= 40257 loss= 0.01407911442220211\n",
      "test_data MSELoss:(pred-real)/real= 0.003067041624389175\n",
      "epoch= 433 iteration= 40270 loss= 0.0004899519844911993\n",
      "epoch= 433 iteration= 40290 loss= 0.002649227622896433\n",
      "epoch= 433 iteration= 40310 loss= 0.0008945509325712919\n",
      "epoch= 433 iteration= 40330 loss= 0.0010858809109777212\n",
      "epoch= 433 iteration= 40350 loss= 0.014051416888833046\n",
      "test_data MSELoss:(pred-real)/real= 0.0030607389538393668\n",
      "epoch= 434 iteration= 40363 loss= 0.0004897999460808933\n",
      "epoch= 434 iteration= 40383 loss= 0.002642460633069277\n",
      "epoch= 434 iteration= 40403 loss= 0.0008936898084357381\n",
      "epoch= 434 iteration= 40423 loss= 0.0010850983671844006\n",
      "epoch= 434 iteration= 40443 loss= 0.014030519872903824\n",
      "test_data MSELoss:(pred-real)/real= 0.003054020641785529\n",
      "epoch= 435 iteration= 40456 loss= 0.0004897236940450966\n",
      "epoch= 435 iteration= 40476 loss= 0.002635000506415963\n",
      "epoch= 435 iteration= 40496 loss= 0.0008925060974434018\n",
      "epoch= 435 iteration= 40516 loss= 0.0010841288603842258\n",
      "epoch= 435 iteration= 40536 loss= 0.014004372991621494\n",
      "test_data MSELoss:(pred-real)/real= 0.00304734371861236\n",
      "epoch= 436 iteration= 40549 loss= 0.0004893876030109823\n",
      "epoch= 436 iteration= 40569 loss= 0.0026292549446225166\n",
      "epoch= 436 iteration= 40589 loss= 0.0008922587148845196\n",
      "epoch= 436 iteration= 40609 loss= 0.0010834165150299668\n",
      "epoch= 436 iteration= 40629 loss= 0.01398276537656784\n",
      "test_data MSELoss:(pred-real)/real= 0.0030409384505926734\n",
      "epoch= 437 iteration= 40642 loss= 0.00048820360098034143\n",
      "epoch= 437 iteration= 40662 loss= 0.002621775260195136\n",
      "epoch= 437 iteration= 40682 loss= 0.0008916300721466541\n",
      "epoch= 437 iteration= 40702 loss= 0.0010825033532455564\n",
      "epoch= 437 iteration= 40722 loss= 0.013953601941466331\n",
      "test_data MSELoss:(pred-real)/real= 0.0030348200493285227\n",
      "epoch= 438 iteration= 40735 loss= 0.0004893395234830678\n",
      "epoch= 438 iteration= 40755 loss= 0.002613178687170148\n",
      "epoch= 438 iteration= 40775 loss= 0.0008886730065569282\n",
      "epoch= 438 iteration= 40795 loss= 0.0010821669129654765\n",
      "epoch= 438 iteration= 40815 loss= 0.01392827183008194\n",
      "test_data MSELoss:(pred-real)/real= 0.0030276755310801994\n",
      "epoch= 439 iteration= 40828 loss= 0.0004900014610029757\n",
      "epoch= 439 iteration= 40848 loss= 0.0026058347430080175\n",
      "epoch= 439 iteration= 40868 loss= 0.0008880167151801288\n",
      "epoch= 439 iteration= 40888 loss= 0.0010816424619406462\n",
      "epoch= 439 iteration= 40908 loss= 0.01390264742076397\n",
      "test_data MSELoss:(pred-real)/real= 0.0030213360571198994\n",
      "epoch= 440 iteration= 40921 loss= 0.0004915124736726284\n",
      "epoch= 440 iteration= 40941 loss= 0.0025978218764066696\n",
      "epoch= 440 iteration= 40961 loss= 0.0008887539152055979\n",
      "epoch= 440 iteration= 40981 loss= 0.0010807327926158905\n",
      "epoch= 440 iteration= 41001 loss= 0.013875088654458523\n",
      "test_data MSELoss:(pred-real)/real= 0.003014599063640667\n",
      "epoch= 441 iteration= 41014 loss= 0.0004906139220111072\n",
      "epoch= 441 iteration= 41034 loss= 0.002590971766039729\n",
      "epoch= 441 iteration= 41054 loss= 0.0008877056534402072\n",
      "epoch= 441 iteration= 41074 loss= 0.0010798911098390818\n",
      "epoch= 441 iteration= 41094 loss= 0.013848531991243362\n",
      "test_data MSELoss:(pred-real)/real= 0.003008340647081948\n",
      "epoch= 442 iteration= 41107 loss= 0.0004882137873210013\n",
      "epoch= 442 iteration= 41127 loss= 0.002586216665804386\n",
      "epoch= 442 iteration= 41147 loss= 0.0008850966114550829\n",
      "epoch= 442 iteration= 41167 loss= 0.001079454319551587\n",
      "epoch= 442 iteration= 41187 loss= 0.013822902925312519\n",
      "test_data MSELoss:(pred-real)/real= 0.0030022068078526193\n",
      "epoch= 443 iteration= 41200 loss= 0.00048771058209240437\n",
      "epoch= 443 iteration= 41220 loss= 0.002578804735094309\n",
      "epoch= 443 iteration= 41240 loss= 0.0008843737887218595\n",
      "epoch= 443 iteration= 41260 loss= 0.0010784367332234979\n",
      "epoch= 443 iteration= 41280 loss= 0.013797879219055176\n",
      "test_data MSELoss:(pred-real)/real= 0.0029955603434549025\n",
      "epoch= 444 iteration= 41293 loss= 0.000487281649839133\n",
      "epoch= 444 iteration= 41313 loss= 0.0025710684712976217\n",
      "epoch= 444 iteration= 41333 loss= 0.0008835088228806853\n",
      "epoch= 444 iteration= 41353 loss= 0.0010772614041343331\n",
      "epoch= 444 iteration= 41373 loss= 0.013775710016489029\n",
      "test_data MSELoss:(pred-real)/real= 0.0029891401710402635\n",
      "epoch= 445 iteration= 41386 loss= 0.00048822362441569567\n",
      "epoch= 445 iteration= 41406 loss= 0.0025648074224591255\n",
      "epoch= 445 iteration= 41426 loss= 0.0008832162711769342\n",
      "epoch= 445 iteration= 41446 loss= 0.001077092718333006\n",
      "epoch= 445 iteration= 41466 loss= 0.01374716218560934\n",
      "test_data MSELoss:(pred-real)/real= 0.0029824495270279134\n",
      "epoch= 446 iteration= 41479 loss= 0.0004883888177573681\n",
      "epoch= 446 iteration= 41499 loss= 0.0025565950199961662\n",
      "epoch= 446 iteration= 41519 loss= 0.0008816139888949692\n",
      "epoch= 446 iteration= 41539 loss= 0.0010760773438960314\n",
      "epoch= 446 iteration= 41559 loss= 0.013717932626605034\n",
      "test_data MSELoss:(pred-real)/real= 0.0029766130222318075\n",
      "epoch= 447 iteration= 41572 loss= 0.00048803345998749137\n",
      "epoch= 447 iteration= 41592 loss= 0.0025498061440885067\n",
      "epoch= 447 iteration= 41612 loss= 0.0008804900571703911\n",
      "epoch= 447 iteration= 41632 loss= 0.0010752396192401648\n",
      "epoch= 447 iteration= 41652 loss= 0.013697718270123005\n",
      "test_data MSELoss:(pred-real)/real= 0.0029697748638379076\n",
      "epoch= 448 iteration= 41665 loss= 0.0004872094141319394\n",
      "epoch= 448 iteration= 41685 loss= 0.00254564406350255\n",
      "epoch= 448 iteration= 41705 loss= 0.0008785569807514548\n",
      "epoch= 448 iteration= 41725 loss= 0.0010745390318334103\n",
      "epoch= 448 iteration= 41745 loss= 0.013671651482582092\n",
      "test_data MSELoss:(pred-real)/real= 0.0029636300815683273\n",
      "epoch= 449 iteration= 41758 loss= 0.00048647401854395866\n",
      "epoch= 449 iteration= 41778 loss= 0.002539378125220537\n",
      "epoch= 449 iteration= 41798 loss= 0.0008785466197878122\n",
      "epoch= 449 iteration= 41818 loss= 0.0010737974662333727\n",
      "epoch= 449 iteration= 41838 loss= 0.013645336963236332\n",
      "test_data MSELoss:(pred-real)/real= 0.0029576326422910723\n",
      "epoch= 450 iteration= 41851 loss= 0.000486134085804224\n",
      "epoch= 450 iteration= 41871 loss= 0.0025287396274507046\n",
      "epoch= 450 iteration= 41891 loss= 0.0008780562202446163\n",
      "epoch= 450 iteration= 41911 loss= 0.001073434017598629\n",
      "epoch= 450 iteration= 41931 loss= 0.013621581718325615\n",
      "test_data MSELoss:(pred-real)/real= 0.0029508299129601154\n",
      "epoch= 451 iteration= 41944 loss= 0.0004849832330364734\n",
      "epoch= 451 iteration= 41964 loss= 0.0025225472636520863\n",
      "epoch= 451 iteration= 41984 loss= 0.0008769847918301821\n",
      "epoch= 451 iteration= 42004 loss= 0.001072098035365343\n",
      "epoch= 451 iteration= 42024 loss= 0.013596665114164352\n",
      "test_data MSELoss:(pred-real)/real= 0.0029440156585122976\n",
      "epoch= 452 iteration= 42037 loss= 0.0004845552612096071\n",
      "epoch= 452 iteration= 42057 loss= 0.002517138374969363\n",
      "epoch= 452 iteration= 42077 loss= 0.0008760616765357554\n",
      "epoch= 452 iteration= 42097 loss= 0.001071823644451797\n",
      "epoch= 452 iteration= 42117 loss= 0.013569895178079605\n",
      "test_data MSELoss:(pred-real)/real= 0.0029375124722719193\n",
      "epoch= 453 iteration= 42130 loss= 0.00048509551561437547\n",
      "epoch= 453 iteration= 42150 loss= 0.002508833073079586\n",
      "epoch= 453 iteration= 42170 loss= 0.0008755357121117413\n",
      "epoch= 453 iteration= 42190 loss= 0.0010708158370107412\n",
      "epoch= 453 iteration= 42210 loss= 0.013545215129852295\n",
      "test_data MSELoss:(pred-real)/real= 0.0029312913603563276\n",
      "epoch= 454 iteration= 42223 loss= 0.0004839521134272218\n",
      "epoch= 454 iteration= 42243 loss= 0.002503396477550268\n",
      "epoch= 454 iteration= 42263 loss= 0.0008743362850509584\n",
      "epoch= 454 iteration= 42283 loss= 0.0010697715915739536\n",
      "epoch= 454 iteration= 42303 loss= 0.013523198664188385\n",
      "test_data MSELoss:(pred-real)/real= 0.0029239694639626476\n",
      "epoch= 455 iteration= 42316 loss= 0.0004845400690101087\n",
      "epoch= 455 iteration= 42336 loss= 0.0024952238891273737\n",
      "epoch= 455 iteration= 42356 loss= 0.0008733695140108466\n",
      "epoch= 455 iteration= 42376 loss= 0.001069220365025103\n",
      "epoch= 455 iteration= 42396 loss= 0.013493474572896957\n",
      "test_data MSELoss:(pred-real)/real= 0.0029186192421346074\n",
      "epoch= 456 iteration= 42409 loss= 0.00048453654744662344\n",
      "epoch= 456 iteration= 42429 loss= 0.0024909633211791515\n",
      "epoch= 456 iteration= 42449 loss= 0.0008719990728422999\n",
      "epoch= 456 iteration= 42469 loss= 0.001068234909325838\n",
      "epoch= 456 iteration= 42489 loss= 0.013466577976942062\n",
      "test_data MSELoss:(pred-real)/real= 0.0029117173495857664\n",
      "epoch= 457 iteration= 42502 loss= 0.0004837188171222806\n",
      "epoch= 457 iteration= 42522 loss= 0.002481546252965927\n",
      "epoch= 457 iteration= 42542 loss= 0.0008704886422492564\n",
      "epoch= 457 iteration= 42562 loss= 0.0010674125514924526\n",
      "epoch= 457 iteration= 42582 loss= 0.013446381315588951\n",
      "test_data MSELoss:(pred-real)/real= 0.0029060687423528484\n",
      "epoch= 458 iteration= 42595 loss= 0.0004841140180360526\n",
      "epoch= 458 iteration= 42615 loss= 0.002474444918334484\n",
      "epoch= 458 iteration= 42635 loss= 0.0008690594113431871\n",
      "epoch= 458 iteration= 42655 loss= 0.0010668591130524874\n",
      "epoch= 458 iteration= 42675 loss= 0.013418549671769142\n",
      "test_data MSELoss:(pred-real)/real= 0.0028996096160780224\n",
      "epoch= 459 iteration= 42688 loss= 0.0004837845335714519\n",
      "epoch= 459 iteration= 42708 loss= 0.002465863712131977\n",
      "epoch= 459 iteration= 42728 loss= 0.0008690346730872989\n",
      "epoch= 459 iteration= 42748 loss= 0.0010660411790013313\n",
      "epoch= 459 iteration= 42768 loss= 0.013394271023571491\n",
      "test_data MSELoss:(pred-real)/real= 0.00289294313147871\n",
      "epoch= 460 iteration= 42781 loss= 0.0004844513605348766\n",
      "epoch= 460 iteration= 42801 loss= 0.0024616667069494724\n",
      "epoch= 460 iteration= 42821 loss= 0.0008675557328388095\n",
      "epoch= 460 iteration= 42841 loss= 0.0010650933254510164\n",
      "epoch= 460 iteration= 42861 loss= 0.013365603983402252\n",
      "test_data MSELoss:(pred-real)/real= 0.0028867064910526904\n",
      "epoch= 461 iteration= 42874 loss= 0.0004841191112063825\n",
      "epoch= 461 iteration= 42894 loss= 0.002454413566738367\n",
      "epoch= 461 iteration= 42914 loss= 0.0008672180701978505\n",
      "epoch= 461 iteration= 42934 loss= 0.001064133713953197\n",
      "epoch= 461 iteration= 42954 loss= 0.0133458087220788\n",
      "test_data MSELoss:(pred-real)/real= 0.0028797701783736963\n",
      "epoch= 462 iteration= 42967 loss= 0.0004823586787097156\n",
      "epoch= 462 iteration= 42987 loss= 0.0024477727711200714\n",
      "epoch= 462 iteration= 43007 loss= 0.000865751295350492\n",
      "epoch= 462 iteration= 43027 loss= 0.0010642602574080229\n",
      "epoch= 462 iteration= 43047 loss= 0.013316587544977665\n",
      "test_data MSELoss:(pred-real)/real= 0.0028735163278825995\n",
      "epoch= 463 iteration= 43060 loss= 0.0004826427611988038\n",
      "epoch= 463 iteration= 43080 loss= 0.0024432013742625713\n",
      "epoch= 463 iteration= 43100 loss= 0.0008644630433991551\n",
      "epoch= 463 iteration= 43120 loss= 0.0010629678145051003\n",
      "epoch= 463 iteration= 43140 loss= 0.01329316571354866\n",
      "test_data MSELoss:(pred-real)/real= 0.0028673924680333585\n",
      "epoch= 464 iteration= 43153 loss= 0.000481376308016479\n",
      "epoch= 464 iteration= 43173 loss= 0.0024350371677428484\n",
      "epoch= 464 iteration= 43193 loss= 0.0008633009856566787\n",
      "epoch= 464 iteration= 43213 loss= 0.001061958377249539\n",
      "epoch= 464 iteration= 43233 loss= 0.013266793452203274\n",
      "test_data MSELoss:(pred-real)/real= 0.002861316192946914\n",
      "epoch= 465 iteration= 43246 loss= 0.00048275638255290687\n",
      "epoch= 465 iteration= 43266 loss= 0.002428715815767646\n",
      "epoch= 465 iteration= 43286 loss= 0.0008629905059933662\n",
      "epoch= 465 iteration= 43306 loss= 0.0010613054037094116\n",
      "epoch= 465 iteration= 43326 loss= 0.013241604901850224\n",
      "test_data MSELoss:(pred-real)/real= 0.002854683297401708\n",
      "epoch= 466 iteration= 43339 loss= 0.00048335979226976633\n",
      "epoch= 466 iteration= 43359 loss= 0.002420733217149973\n",
      "epoch= 466 iteration= 43379 loss= 0.0008616209961473942\n",
      "epoch= 466 iteration= 43399 loss= 0.0010606410214677453\n",
      "epoch= 466 iteration= 43419 loss= 0.01321687176823616\n",
      "test_data MSELoss:(pred-real)/real= 0.002848739855431227\n",
      "epoch= 467 iteration= 43432 loss= 0.00048295449232682586\n",
      "epoch= 467 iteration= 43452 loss= 0.0024118300061672926\n",
      "epoch= 467 iteration= 43472 loss= 0.0008623765897937119\n",
      "epoch= 467 iteration= 43492 loss= 0.001059983391314745\n",
      "epoch= 467 iteration= 43512 loss= 0.013191426172852516\n",
      "test_data MSELoss:(pred-real)/real= 0.002842132102361777\n",
      "epoch= 468 iteration= 43525 loss= 0.00048234680434688926\n",
      "epoch= 468 iteration= 43545 loss= 0.0024060914292931557\n",
      "epoch= 468 iteration= 43565 loss= 0.0008612741366960108\n",
      "epoch= 468 iteration= 43585 loss= 0.001059065223671496\n",
      "epoch= 468 iteration= 43605 loss= 0.013168900273740292\n",
      "test_data MSELoss:(pred-real)/real= 0.0028362451282899\n",
      "epoch= 469 iteration= 43618 loss= 0.0004818169982172549\n",
      "epoch= 469 iteration= 43638 loss= 0.0024001803249120712\n",
      "epoch= 469 iteration= 43658 loss= 0.0008594306418672204\n",
      "epoch= 469 iteration= 43678 loss= 0.0010584949050098658\n",
      "epoch= 469 iteration= 43698 loss= 0.013139447197318077\n",
      "test_data MSELoss:(pred-real)/real= 0.0028296689787465665\n",
      "epoch= 470 iteration= 43711 loss= 0.0004831621190533042\n",
      "epoch= 470 iteration= 43731 loss= 0.0023931919131428003\n",
      "epoch= 470 iteration= 43751 loss= 0.0008594081155024469\n",
      "epoch= 470 iteration= 43771 loss= 0.0010576206259429455\n",
      "epoch= 470 iteration= 43791 loss= 0.013116024434566498\n",
      "test_data MSELoss:(pred-real)/real= 0.002823843478432132\n",
      "epoch= 471 iteration= 43804 loss= 0.00048148559289984405\n",
      "epoch= 471 iteration= 43824 loss= 0.002386617474257946\n",
      "epoch= 471 iteration= 43844 loss= 0.0008576223626732826\n",
      "epoch= 471 iteration= 43864 loss= 0.0010570477461442351\n",
      "epoch= 471 iteration= 43884 loss= 0.013089238665997982\n",
      "test_data MSELoss:(pred-real)/real= 0.0028168774636772773\n",
      "epoch= 472 iteration= 43897 loss= 0.00048166929627768695\n",
      "epoch= 472 iteration= 43917 loss= 0.002378372475504875\n",
      "epoch= 472 iteration= 43937 loss= 0.0008574233506806195\n",
      "epoch= 472 iteration= 43957 loss= 0.0010566634591668844\n",
      "epoch= 472 iteration= 43977 loss= 0.013064342550933361\n",
      "test_data MSELoss:(pred-real)/real= 0.002810148240920777\n",
      "epoch= 473 iteration= 43990 loss= 0.0004810867249034345\n",
      "epoch= 473 iteration= 44010 loss= 0.002372703980654478\n",
      "epoch= 473 iteration= 44030 loss= 0.0008566276519559324\n",
      "epoch= 473 iteration= 44050 loss= 0.0010559454094618559\n",
      "epoch= 473 iteration= 44070 loss= 0.01304235402494669\n",
      "test_data MSELoss:(pred-real)/real= 0.00280470136527179\n",
      "epoch= 474 iteration= 44083 loss= 0.0004810891696251929\n",
      "epoch= 474 iteration= 44103 loss= 0.0023653802927583456\n",
      "epoch= 474 iteration= 44123 loss= 0.0008557891706004739\n",
      "epoch= 474 iteration= 44143 loss= 0.0010546469129621983\n",
      "epoch= 474 iteration= 44163 loss= 0.013015003874897957\n",
      "test_data MSELoss:(pred-real)/real= 0.002798232777018307\n",
      "epoch= 475 iteration= 44176 loss= 0.00047992539475671947\n",
      "epoch= 475 iteration= 44196 loss= 0.0023583637084811926\n",
      "epoch= 475 iteration= 44216 loss= 0.0008548995247110724\n",
      "epoch= 475 iteration= 44236 loss= 0.0010536430636420846\n",
      "epoch= 475 iteration= 44256 loss= 0.012990212067961693\n",
      "test_data MSELoss:(pred-real)/real= 0.002791295047952897\n",
      "epoch= 476 iteration= 44269 loss= 0.00048164543113671243\n",
      "epoch= 476 iteration= 44289 loss= 0.0023551513440907\n",
      "epoch= 476 iteration= 44309 loss= 0.0008534612716175616\n",
      "epoch= 476 iteration= 44329 loss= 0.001053330022841692\n",
      "epoch= 476 iteration= 44349 loss= 0.012966090813279152\n",
      "test_data MSELoss:(pred-real)/real= 0.0027856531895748857\n",
      "epoch= 477 iteration= 44362 loss= 0.00047923039528541267\n",
      "epoch= 477 iteration= 44382 loss= 0.0023449063301086426\n",
      "epoch= 477 iteration= 44402 loss= 0.000852958473842591\n",
      "epoch= 477 iteration= 44422 loss= 0.0010517722694203258\n",
      "epoch= 477 iteration= 44442 loss= 0.01293710246682167\n",
      "test_data MSELoss:(pred-real)/real= 0.0027790784596517268\n",
      "epoch= 478 iteration= 44455 loss= 0.0004818130692001432\n",
      "epoch= 478 iteration= 44475 loss= 0.0023404406383633614\n",
      "epoch= 478 iteration= 44495 loss= 0.0008517687674611807\n",
      "epoch= 478 iteration= 44515 loss= 0.001051506376825273\n",
      "epoch= 478 iteration= 44535 loss= 0.01291394792497158\n",
      "test_data MSELoss:(pred-real)/real= 0.0027729908243701276\n",
      "epoch= 479 iteration= 44548 loss= 0.0004799686139449477\n",
      "epoch= 479 iteration= 44568 loss= 0.0023321101907640696\n",
      "epoch= 479 iteration= 44588 loss= 0.0008508509490638971\n",
      "epoch= 479 iteration= 44608 loss= 0.00105064339004457\n",
      "epoch= 479 iteration= 44628 loss= 0.012890756130218506\n",
      "test_data MSELoss:(pred-real)/real= 0.0027671590505633503\n",
      "epoch= 480 iteration= 44641 loss= 0.00047847680980339646\n",
      "epoch= 480 iteration= 44661 loss= 0.0023260952439159155\n",
      "epoch= 480 iteration= 44681 loss= 0.0008504685247316957\n",
      "epoch= 480 iteration= 44701 loss= 0.0010501989163458347\n",
      "epoch= 480 iteration= 44721 loss= 0.01286386325955391\n",
      "test_data MSELoss:(pred-real)/real= 0.0027606074711204404\n",
      "epoch= 481 iteration= 44734 loss= 0.00047728378558531404\n",
      "epoch= 481 iteration= 44754 loss= 0.002320639556273818\n",
      "epoch= 481 iteration= 44774 loss= 0.0008489014580845833\n",
      "epoch= 481 iteration= 44794 loss= 0.0010491266148164868\n",
      "epoch= 481 iteration= 44814 loss= 0.012841860763728619\n",
      "test_data MSELoss:(pred-real)/real= 0.0027539257943216297\n",
      "epoch= 482 iteration= 44827 loss= 0.00048023086856119335\n",
      "epoch= 482 iteration= 44847 loss= 0.0023115871008485556\n",
      "epoch= 482 iteration= 44867 loss= 0.0008487777085974813\n",
      "epoch= 482 iteration= 44887 loss= 0.0010483411606401205\n",
      "epoch= 482 iteration= 44907 loss= 0.012819426134228706\n",
      "test_data MSELoss:(pred-real)/real= 0.002748043004733821\n",
      "epoch= 483 iteration= 44920 loss= 0.00047925050603225827\n",
      "epoch= 483 iteration= 44940 loss= 0.0023028170689940453\n",
      "epoch= 483 iteration= 44960 loss= 0.0008477457449771464\n",
      "epoch= 483 iteration= 44980 loss= 0.0010473483707755804\n",
      "epoch= 483 iteration= 45000 loss= 0.012790484353899956\n",
      "test_data MSELoss:(pred-real)/real= 0.00274156614452497\n",
      "epoch= 484 iteration= 45013 loss= 0.0004799227463081479\n",
      "epoch= 484 iteration= 45033 loss= 0.00229917885735631\n",
      "epoch= 484 iteration= 45053 loss= 0.0008472100598737597\n",
      "epoch= 484 iteration= 45073 loss= 0.0010467374231666327\n",
      "epoch= 484 iteration= 45093 loss= 0.01276804693043232\n",
      "test_data MSELoss:(pred-real)/real= 0.002736050816666749\n",
      "epoch= 485 iteration= 45106 loss= 0.0004762804019264877\n",
      "epoch= 485 iteration= 45126 loss= 0.0022944530937820673\n",
      "epoch= 485 iteration= 45146 loss= 0.0008459322853013873\n",
      "epoch= 485 iteration= 45166 loss= 0.0010462348582223058\n",
      "epoch= 485 iteration= 45186 loss= 0.012741059996187687\n",
      "test_data MSELoss:(pred-real)/real= 0.002729715430177748\n",
      "epoch= 486 iteration= 45199 loss= 0.0004773479886353016\n",
      "epoch= 486 iteration= 45219 loss= 0.002287713810801506\n",
      "epoch= 486 iteration= 45239 loss= 0.0008447512518614531\n",
      "epoch= 486 iteration= 45259 loss= 0.0010450438130646944\n",
      "epoch= 486 iteration= 45279 loss= 0.012716446071863174\n",
      "test_data MSELoss:(pred-real)/real= 0.0027236015820461842\n",
      "epoch= 487 iteration= 45292 loss= 0.0004768322396557778\n",
      "epoch= 487 iteration= 45312 loss= 0.0022798310965299606\n",
      "epoch= 487 iteration= 45332 loss= 0.0008442330872640014\n",
      "epoch= 487 iteration= 45352 loss= 0.001044063945300877\n",
      "epoch= 487 iteration= 45372 loss= 0.012691864743828773\n",
      "test_data MSELoss:(pred-real)/real= 0.002717351204612189\n",
      "epoch= 488 iteration= 45385 loss= 0.0004794705891981721\n",
      "epoch= 488 iteration= 45405 loss= 0.002274180529639125\n",
      "epoch= 488 iteration= 45425 loss= 0.0008432398899458349\n",
      "epoch= 488 iteration= 45445 loss= 0.0010436531156301498\n",
      "epoch= 488 iteration= 45465 loss= 0.01266691368073225\n",
      "test_data MSELoss:(pred-real)/real= 0.002710502077307966\n",
      "epoch= 489 iteration= 45478 loss= 0.00047874514712020755\n",
      "epoch= 489 iteration= 45498 loss= 0.0022682712879031897\n",
      "epoch= 489 iteration= 45518 loss= 0.0008427205611951649\n",
      "epoch= 489 iteration= 45538 loss= 0.0010427222587168217\n",
      "epoch= 489 iteration= 45558 loss= 0.012642892077565193\n",
      "test_data MSELoss:(pred-real)/real= 0.002704264799831435\n",
      "epoch= 490 iteration= 45571 loss= 0.00047872672439552844\n",
      "epoch= 490 iteration= 45591 loss= 0.002259179949760437\n",
      "epoch= 490 iteration= 45611 loss= 0.0008414629846811295\n",
      "epoch= 490 iteration= 45631 loss= 0.0010414603166282177\n",
      "epoch= 490 iteration= 45651 loss= 0.012618312612175941\n",
      "test_data MSELoss:(pred-real)/real= 0.0026988213228630936\n",
      "epoch= 491 iteration= 45664 loss= 0.0004766653582919389\n",
      "epoch= 491 iteration= 45684 loss= 0.0022565696854144335\n",
      "epoch= 491 iteration= 45704 loss= 0.0008405756670981646\n",
      "epoch= 491 iteration= 45724 loss= 0.0010412620613351464\n",
      "epoch= 491 iteration= 45744 loss= 0.01259034126996994\n",
      "test_data MSELoss:(pred-real)/real= 0.0026923557420054246\n",
      "epoch= 492 iteration= 45757 loss= 0.00047688980703242123\n",
      "epoch= 492 iteration= 45777 loss= 0.0022495007142424583\n",
      "epoch= 492 iteration= 45797 loss= 0.0008404090767726302\n",
      "epoch= 492 iteration= 45817 loss= 0.0010400156024843454\n",
      "epoch= 492 iteration= 45837 loss= 0.012567898258566856\n",
      "test_data MSELoss:(pred-real)/real= 0.0026863560487981886\n",
      "epoch= 493 iteration= 45850 loss= 0.0004766028141602874\n",
      "epoch= 493 iteration= 45870 loss= 0.002243421506136656\n",
      "epoch= 493 iteration= 45890 loss= 0.0008392528397962451\n",
      "epoch= 493 iteration= 45910 loss= 0.0010392855620011687\n",
      "epoch= 493 iteration= 45930 loss= 0.012547801248729229\n",
      "test_data MSELoss:(pred-real)/real= 0.0026809692919616485\n",
      "epoch= 494 iteration= 45943 loss= 0.0004757618298754096\n",
      "epoch= 494 iteration= 45963 loss= 0.0022373604588210583\n",
      "epoch= 494 iteration= 45983 loss= 0.000838457141071558\n",
      "epoch= 494 iteration= 46003 loss= 0.0010384239722043276\n",
      "epoch= 494 iteration= 46023 loss= 0.012520154938101768\n",
      "test_data MSELoss:(pred-real)/real= 0.0026743840523219355\n",
      "epoch= 495 iteration= 46036 loss= 0.0004763328761328012\n",
      "epoch= 495 iteration= 46056 loss= 0.0022277487441897392\n",
      "epoch= 495 iteration= 46076 loss= 0.000837721221614629\n",
      "epoch= 495 iteration= 46096 loss= 0.0010375881101936102\n",
      "epoch= 495 iteration= 46116 loss= 0.012499237433075905\n",
      "test_data MSELoss:(pred-real)/real= 0.0026679369491628474\n",
      "epoch= 496 iteration= 46129 loss= 0.00047760922461748123\n",
      "epoch= 496 iteration= 46149 loss= 0.0022205619607120752\n",
      "epoch= 496 iteration= 46169 loss= 0.0008370200521312654\n",
      "epoch= 496 iteration= 46189 loss= 0.0010374318808317184\n",
      "epoch= 496 iteration= 46209 loss= 0.012471899390220642\n",
      "test_data MSELoss:(pred-real)/real= 0.0026616124022338125\n",
      "epoch= 497 iteration= 46222 loss= 0.0004789179074577987\n",
      "epoch= 497 iteration= 46242 loss= 0.0022159835789352655\n",
      "epoch= 497 iteration= 46262 loss= 0.0008367935661226511\n",
      "epoch= 497 iteration= 46282 loss= 0.0010359261650592089\n",
      "epoch= 497 iteration= 46302 loss= 0.012446814216673374\n",
      "test_data MSELoss:(pred-real)/real= 0.0026558467329272795\n",
      "epoch= 498 iteration= 46315 loss= 0.0004774461267516017\n",
      "epoch= 498 iteration= 46335 loss= 0.0022084778174757957\n",
      "epoch= 498 iteration= 46355 loss= 0.0008354136371053755\n",
      "epoch= 498 iteration= 46375 loss= 0.0010353245306760073\n",
      "epoch= 498 iteration= 46395 loss= 0.012423518113791943\n",
      "test_data MSELoss:(pred-real)/real= 0.0026498354208241734\n",
      "epoch= 499 iteration= 46408 loss= 0.0004796995781362057\n",
      "epoch= 499 iteration= 46428 loss= 0.0022018011659383774\n",
      "epoch= 499 iteration= 46448 loss= 0.0008348003611899912\n",
      "epoch= 499 iteration= 46468 loss= 0.001034369459375739\n",
      "epoch= 499 iteration= 46488 loss= 0.01239698100835085\n",
      "test_data MSELoss:(pred-real)/real= 0.0026441306431984734\n",
      "epoch= 500 iteration= 46501 loss= 0.0004779591108672321\n",
      "epoch= 500 iteration= 46521 loss= 0.002194560831412673\n",
      "epoch= 500 iteration= 46541 loss= 0.0008346994873136282\n",
      "epoch= 500 iteration= 46561 loss= 0.0010335479164496064\n",
      "epoch= 500 iteration= 46581 loss= 0.012372491881251335\n",
      "test_data MSELoss:(pred-real)/real= 0.002637622751838838\n",
      "epoch= 501 iteration= 46594 loss= 0.00047701405128464103\n",
      "epoch= 501 iteration= 46614 loss= 0.0021871505305171013\n",
      "epoch= 501 iteration= 46634 loss= 0.0008334901067428291\n",
      "epoch= 501 iteration= 46654 loss= 0.001032512984238565\n",
      "epoch= 501 iteration= 46674 loss= 0.012348152697086334\n",
      "test_data MSELoss:(pred-real)/real= 0.002632155865689533\n",
      "epoch= 502 iteration= 46687 loss= 0.0004776474670507014\n",
      "epoch= 502 iteration= 46707 loss= 0.002182516735047102\n",
      "epoch= 502 iteration= 46727 loss= 0.0008328761323355138\n",
      "epoch= 502 iteration= 46747 loss= 0.0010323006426915526\n",
      "epoch= 502 iteration= 46767 loss= 0.012326080352067947\n",
      "test_data MSELoss:(pred-real)/real= 0.0026264530897606164\n",
      "epoch= 503 iteration= 46780 loss= 0.00047657699906267226\n",
      "epoch= 503 iteration= 46800 loss= 0.0021763737313449383\n",
      "epoch= 503 iteration= 46820 loss= 0.0008312869467772543\n",
      "epoch= 503 iteration= 46840 loss= 0.0010313931852579117\n",
      "epoch= 503 iteration= 46860 loss= 0.012303683906793594\n",
      "test_data MSELoss:(pred-real)/real= 0.0026199696003459394\n",
      "epoch= 504 iteration= 46873 loss= 0.00047680133138783276\n",
      "epoch= 504 iteration= 46893 loss= 0.0021713883616030216\n",
      "epoch= 504 iteration= 46913 loss= 0.0008306629024446011\n",
      "epoch= 504 iteration= 46933 loss= 0.001030170707963407\n",
      "epoch= 504 iteration= 46953 loss= 0.012278004549443722\n",
      "test_data MSELoss:(pred-real)/real= 0.0026134011988890255\n",
      "epoch= 505 iteration= 46966 loss= 0.0004780649906024337\n",
      "epoch= 505 iteration= 46986 loss= 0.0021647042594850063\n",
      "epoch= 505 iteration= 47006 loss= 0.0008299316396005452\n",
      "epoch= 505 iteration= 47026 loss= 0.001029490609653294\n",
      "epoch= 505 iteration= 47046 loss= 0.012255250476300716\n",
      "test_data MSELoss:(pred-real)/real= 0.00260747756369205\n",
      "epoch= 506 iteration= 47059 loss= 0.0004781675525009632\n",
      "epoch= 506 iteration= 47079 loss= 0.0021574522834271193\n",
      "epoch= 506 iteration= 47099 loss= 0.0008287856471724808\n",
      "epoch= 506 iteration= 47119 loss= 0.0010286644101142883\n",
      "epoch= 506 iteration= 47139 loss= 0.012225456535816193\n",
      "test_data MSELoss:(pred-real)/real= 0.002601471432070765\n",
      "epoch= 507 iteration= 47152 loss= 0.0004778597503900528\n",
      "epoch= 507 iteration= 47172 loss= 0.0021524056792259216\n",
      "epoch= 507 iteration= 47192 loss= 0.0008276886073872447\n",
      "epoch= 507 iteration= 47212 loss= 0.0010275904787704349\n",
      "epoch= 507 iteration= 47232 loss= 0.012206746265292168\n",
      "test_data MSELoss:(pred-real)/real= 0.0025957384107944867\n",
      "epoch= 508 iteration= 47245 loss= 0.00047986910794861615\n",
      "epoch= 508 iteration= 47265 loss= 0.002145150676369667\n",
      "epoch= 508 iteration= 47285 loss= 0.0008281262707896531\n",
      "epoch= 508 iteration= 47305 loss= 0.0010270609054714441\n",
      "epoch= 508 iteration= 47325 loss= 0.012179908342659473\n",
      "test_data MSELoss:(pred-real)/real= 0.0025899639391961196\n",
      "epoch= 509 iteration= 47338 loss= 0.0004791542887687683\n",
      "epoch= 509 iteration= 47358 loss= 0.002139332005754113\n",
      "epoch= 509 iteration= 47378 loss= 0.0008276347070932388\n",
      "epoch= 509 iteration= 47398 loss= 0.0010261452989652753\n",
      "epoch= 509 iteration= 47418 loss= 0.012157316319644451\n",
      "test_data MSELoss:(pred-real)/real= 0.0025838736069595646\n",
      "epoch= 510 iteration= 47431 loss= 0.00047717144479975104\n",
      "epoch= 510 iteration= 47451 loss= 0.002132267225533724\n",
      "epoch= 510 iteration= 47471 loss= 0.0008266310323961079\n",
      "epoch= 510 iteration= 47491 loss= 0.0010252250358462334\n",
      "epoch= 510 iteration= 47511 loss= 0.012132874689996243\n",
      "test_data MSELoss:(pred-real)/real= 0.002578067269900607\n",
      "epoch= 511 iteration= 47524 loss= 0.00047450163401663303\n",
      "epoch= 511 iteration= 47544 loss= 0.0021255898755043745\n",
      "epoch= 511 iteration= 47564 loss= 0.0008256187429651618\n",
      "epoch= 511 iteration= 47584 loss= 0.00102438242174685\n",
      "epoch= 511 iteration= 47604 loss= 0.012106831185519695\n",
      "test_data MSELoss:(pred-real)/real= 0.0025720336966009606\n",
      "epoch= 512 iteration= 47617 loss= 0.000476445653475821\n",
      "epoch= 512 iteration= 47637 loss= 0.0021205455996096134\n",
      "epoch= 512 iteration= 47657 loss= 0.0008246072102338076\n",
      "epoch= 512 iteration= 47677 loss= 0.0010237757815048099\n",
      "epoch= 512 iteration= 47697 loss= 0.012084214016795158\n",
      "test_data MSELoss:(pred-real)/real= 0.002566042611660022\n",
      "epoch= 513 iteration= 47710 loss= 0.0004758517607115209\n",
      "epoch= 513 iteration= 47730 loss= 0.0021143073681741953\n",
      "epoch= 513 iteration= 47750 loss= 0.0008236055145971477\n",
      "epoch= 513 iteration= 47770 loss= 0.001022768672555685\n",
      "epoch= 513 iteration= 47790 loss= 0.012057945132255554\n",
      "test_data MSELoss:(pred-real)/real= 0.0025606804410926998\n",
      "epoch= 514 iteration= 47803 loss= 0.0004762262396980077\n",
      "epoch= 514 iteration= 47823 loss= 0.0021085520274937153\n",
      "epoch= 514 iteration= 47843 loss= 0.0008236807188950479\n",
      "epoch= 514 iteration= 47863 loss= 0.0010211863555014133\n",
      "epoch= 514 iteration= 47883 loss= 0.01203770749270916\n",
      "test_data MSELoss:(pred-real)/real= 0.0025539957374955216\n",
      "epoch= 515 iteration= 47896 loss= 0.000476679764688015\n",
      "epoch= 515 iteration= 47916 loss= 0.00210134987719357\n",
      "epoch= 515 iteration= 47936 loss= 0.0008223524782806635\n",
      "epoch= 515 iteration= 47956 loss= 0.0010211409535259008\n",
      "epoch= 515 iteration= 47976 loss= 0.01201624795794487\n",
      "test_data MSELoss:(pred-real)/real= 0.002547979542416417\n",
      "epoch= 516 iteration= 47989 loss= 0.0004770960658788681\n",
      "epoch= 516 iteration= 48009 loss= 0.002094041556119919\n",
      "epoch= 516 iteration= 48029 loss= 0.0008224903140217066\n",
      "epoch= 516 iteration= 48049 loss= 0.0010194035712629557\n",
      "epoch= 516 iteration= 48069 loss= 0.011991612613201141\n",
      "test_data MSELoss:(pred-real)/real= 0.0025426908299171678\n",
      "epoch= 517 iteration= 48082 loss= 0.0004772317479364574\n",
      "epoch= 517 iteration= 48102 loss= 0.0020879777148365974\n",
      "epoch= 517 iteration= 48122 loss= 0.000821405672468245\n",
      "epoch= 517 iteration= 48142 loss= 0.0010195558425039053\n",
      "epoch= 517 iteration= 48162 loss= 0.011965897865593433\n",
      "test_data MSELoss:(pred-real)/real= 0.002537065369930739\n",
      "epoch= 518 iteration= 48175 loss= 0.0004770405648741871\n",
      "epoch= 518 iteration= 48195 loss= 0.0020803012885153294\n",
      "epoch= 518 iteration= 48215 loss= 0.0008209277875721455\n",
      "epoch= 518 iteration= 48235 loss= 0.0010182097321376204\n",
      "epoch= 518 iteration= 48255 loss= 0.011947743594646454\n",
      "test_data MSELoss:(pred-real)/real= 0.002531211147369403\n",
      "epoch= 519 iteration= 48268 loss= 0.0004762742610182613\n",
      "epoch= 519 iteration= 48288 loss= 0.0020754816941916943\n",
      "epoch= 519 iteration= 48308 loss= 0.0008201563032343984\n",
      "epoch= 519 iteration= 48328 loss= 0.001017518574371934\n",
      "epoch= 519 iteration= 48348 loss= 0.011920512653887272\n",
      "test_data MSELoss:(pred-real)/real= 0.00252502017085337\n",
      "epoch= 520 iteration= 48361 loss= 0.00047555670607835054\n",
      "epoch= 520 iteration= 48381 loss= 0.002070880029350519\n",
      "epoch= 520 iteration= 48401 loss= 0.0008188604842871428\n",
      "epoch= 520 iteration= 48421 loss= 0.001016857335343957\n",
      "epoch= 520 iteration= 48441 loss= 0.011904376558959484\n",
      "test_data MSELoss:(pred-real)/real= 0.0025192066580833248\n",
      "epoch= 521 iteration= 48454 loss= 0.0004738910647574812\n",
      "epoch= 521 iteration= 48474 loss= 0.0020652753300964832\n",
      "epoch= 521 iteration= 48494 loss= 0.0008178618736565113\n",
      "epoch= 521 iteration= 48514 loss= 0.0010158477816730738\n",
      "epoch= 521 iteration= 48534 loss= 0.011877140030264854\n",
      "test_data MSELoss:(pred-real)/real= 0.0025133970646291142\n",
      "epoch= 522 iteration= 48547 loss= 0.00047519386862404644\n",
      "epoch= 522 iteration= 48567 loss= 0.0020596631802618504\n",
      "epoch= 522 iteration= 48587 loss= 0.0008171761292032897\n",
      "epoch= 522 iteration= 48607 loss= 0.0010148126166313887\n",
      "epoch= 522 iteration= 48627 loss= 0.01185794360935688\n",
      "test_data MSELoss:(pred-real)/real= 0.002508444155359434\n",
      "epoch= 523 iteration= 48640 loss= 0.000475681183161214\n",
      "epoch= 523 iteration= 48660 loss= 0.002052176045253873\n",
      "epoch= 523 iteration= 48680 loss= 0.0008168731583282351\n",
      "epoch= 523 iteration= 48700 loss= 0.0010147151770070195\n",
      "epoch= 523 iteration= 48720 loss= 0.011829672381281853\n",
      "test_data MSELoss:(pred-real)/real= 0.0025025257491506636\n",
      "epoch= 524 iteration= 48733 loss= 0.00047657714458182454\n",
      "epoch= 524 iteration= 48753 loss= 0.0020455115009099245\n",
      "epoch= 524 iteration= 48773 loss= 0.0008166144252754748\n",
      "epoch= 524 iteration= 48793 loss= 0.0010131429880857468\n",
      "epoch= 524 iteration= 48813 loss= 0.011804561130702496\n",
      "test_data MSELoss:(pred-real)/real= 0.002496091645702513\n",
      "epoch= 525 iteration= 48826 loss= 0.0004761018790304661\n",
      "epoch= 525 iteration= 48846 loss= 0.002038131933659315\n",
      "epoch= 525 iteration= 48866 loss= 0.0008152096997946501\n",
      "epoch= 525 iteration= 48886 loss= 0.0010126919951289892\n",
      "epoch= 525 iteration= 48906 loss= 0.011785104870796204\n",
      "test_data MSELoss:(pred-real)/real= 0.0024911482857229808\n",
      "epoch= 526 iteration= 48919 loss= 0.0004770660598296672\n",
      "epoch= 526 iteration= 48939 loss= 0.0020320627372711897\n",
      "epoch= 526 iteration= 48959 loss= 0.0008148561464622617\n",
      "epoch= 526 iteration= 48979 loss= 0.001011560787446797\n",
      "epoch= 526 iteration= 48999 loss= 0.011763989925384521\n",
      "test_data MSELoss:(pred-real)/real= 0.002485371835064143\n",
      "epoch= 527 iteration= 49012 loss= 0.000474774104077369\n",
      "epoch= 527 iteration= 49032 loss= 0.0020273877307772636\n",
      "epoch= 527 iteration= 49052 loss= 0.000814539089333266\n",
      "epoch= 527 iteration= 49072 loss= 0.0010106241097673774\n",
      "epoch= 527 iteration= 49092 loss= 0.011740420013666153\n",
      "test_data MSELoss:(pred-real)/real= 0.002479527498103885\n",
      "epoch= 528 iteration= 49105 loss= 0.00047492922749370337\n",
      "epoch= 528 iteration= 49125 loss= 0.0020224733743816614\n",
      "epoch= 528 iteration= 49145 loss= 0.0008132796501740813\n",
      "epoch= 528 iteration= 49165 loss= 0.0010096961632370949\n",
      "epoch= 528 iteration= 49185 loss= 0.011718682944774628\n",
      "test_data MSELoss:(pred-real)/real= 0.002472666932994293\n",
      "epoch= 529 iteration= 49198 loss= 0.00047415384324267507\n",
      "epoch= 529 iteration= 49218 loss= 0.0020165853202342987\n",
      "epoch= 529 iteration= 49238 loss= 0.0008126660832203925\n",
      "epoch= 529 iteration= 49258 loss= 0.0010093108285218477\n",
      "epoch= 529 iteration= 49278 loss= 0.011694076471030712\n",
      "test_data MSELoss:(pred-real)/real= 0.0024675912831702996\n",
      "epoch= 530 iteration= 49291 loss= 0.0004758245195262134\n",
      "epoch= 530 iteration= 49311 loss= 0.002009134739637375\n",
      "epoch= 530 iteration= 49331 loss= 0.0008122457657009363\n",
      "epoch= 530 iteration= 49351 loss= 0.0010085698449984193\n",
      "epoch= 530 iteration= 49371 loss= 0.011675074696540833\n",
      "test_data MSELoss:(pred-real)/real= 0.002462237145260183\n",
      "epoch= 531 iteration= 49384 loss= 0.0004764317418448627\n",
      "epoch= 531 iteration= 49404 loss= 0.0020037805661559105\n",
      "epoch= 531 iteration= 49424 loss= 0.0008118607802316546\n",
      "epoch= 531 iteration= 49444 loss= 0.0010076387552544475\n",
      "epoch= 531 iteration= 49464 loss= 0.011649800464510918\n",
      "test_data MSELoss:(pred-real)/real= 0.0024564186783714425\n",
      "epoch= 532 iteration= 49477 loss= 0.0004771887033712119\n",
      "epoch= 532 iteration= 49497 loss= 0.001997382612898946\n",
      "epoch= 532 iteration= 49517 loss= 0.0008108894107863307\n",
      "epoch= 532 iteration= 49537 loss= 0.0010063400259241462\n",
      "epoch= 532 iteration= 49557 loss= 0.011628257110714912\n",
      "test_data MSELoss:(pred-real)/real= 0.0024513501266483217\n",
      "epoch= 533 iteration= 49570 loss= 0.00047538685612380505\n",
      "epoch= 533 iteration= 49590 loss= 0.0019913192372769117\n",
      "epoch= 533 iteration= 49610 loss= 0.0008099242695607245\n",
      "epoch= 533 iteration= 49630 loss= 0.0010060714557766914\n",
      "epoch= 533 iteration= 49650 loss= 0.011607028543949127\n",
      "test_data MSELoss:(pred-real)/real= 0.0024455811039337683\n",
      "epoch= 534 iteration= 49663 loss= 0.000476273475214839\n",
      "epoch= 534 iteration= 49683 loss= 0.0019848933443427086\n",
      "epoch= 534 iteration= 49703 loss= 0.000809825025498867\n",
      "epoch= 534 iteration= 49723 loss= 0.0010044679511338472\n",
      "epoch= 534 iteration= 49743 loss= 0.01158193126320839\n",
      "test_data MSELoss:(pred-real)/real= 0.002439986112424069\n",
      "epoch= 535 iteration= 49756 loss= 0.00047699137940071523\n",
      "epoch= 535 iteration= 49776 loss= 0.0019792751409113407\n",
      "epoch= 535 iteration= 49796 loss= 0.000808630371466279\n",
      "epoch= 535 iteration= 49816 loss= 0.0010036509484052658\n",
      "epoch= 535 iteration= 49836 loss= 0.011561490595340729\n",
      "test_data MSELoss:(pred-real)/real= 0.002433843060215521\n",
      "epoch= 536 iteration= 49849 loss= 0.0004772006650455296\n",
      "epoch= 536 iteration= 49869 loss= 0.0019736061803996563\n",
      "epoch= 536 iteration= 49889 loss= 0.000808066688477993\n",
      "epoch= 536 iteration= 49909 loss= 0.0010030160192400217\n",
      "epoch= 536 iteration= 49929 loss= 0.011543488129973412\n",
      "test_data MSELoss:(pred-real)/real= 0.0024288175142525383\n",
      "epoch= 537 iteration= 49942 loss= 0.0004754977999255061\n",
      "epoch= 537 iteration= 49962 loss= 0.0019664301071316004\n",
      "epoch= 537 iteration= 49982 loss= 0.0008072775672189891\n",
      "epoch= 537 iteration= 50002 loss= 0.0010021468624472618\n",
      "epoch= 537 iteration= 50022 loss= 0.011518219485878944\n",
      "test_data MSELoss:(pred-real)/real= 0.002423701856362944\n",
      "epoch= 538 iteration= 50035 loss= 0.0004740968579426408\n",
      "epoch= 538 iteration= 50055 loss= 0.0019622761756181717\n",
      "epoch= 538 iteration= 50075 loss= 0.0008067585295066237\n",
      "epoch= 538 iteration= 50095 loss= 0.0010005615185946226\n",
      "epoch= 538 iteration= 50115 loss= 0.011497553437948227\n",
      "test_data MSELoss:(pred-real)/real= 0.002418249189051696\n",
      "epoch= 539 iteration= 50128 loss= 0.00047333858674392104\n",
      "epoch= 539 iteration= 50148 loss= 0.001954800682142377\n",
      "epoch= 539 iteration= 50168 loss= 0.0008060332620516419\n",
      "epoch= 539 iteration= 50188 loss= 0.0010001063346862793\n",
      "epoch= 539 iteration= 50208 loss= 0.011476676911115646\n",
      "test_data MSELoss:(pred-real)/real= 0.002413073716322995\n",
      "epoch= 540 iteration= 50221 loss= 0.0004715333925560117\n",
      "epoch= 540 iteration= 50241 loss= 0.0019504608353599906\n",
      "epoch= 540 iteration= 50261 loss= 0.0008042287663556635\n",
      "epoch= 540 iteration= 50281 loss= 0.0009995679138228297\n",
      "epoch= 540 iteration= 50301 loss= 0.011452862992882729\n",
      "test_data MSELoss:(pred-real)/real= 0.0024070627502320954\n",
      "epoch= 541 iteration= 50314 loss= 0.0004749341169372201\n",
      "epoch= 541 iteration= 50334 loss= 0.001944615738466382\n",
      "epoch= 541 iteration= 50354 loss= 0.000804272887762636\n",
      "epoch= 541 iteration= 50374 loss= 0.0009980578906834126\n",
      "epoch= 541 iteration= 50394 loss= 0.011430769227445126\n",
      "test_data MSELoss:(pred-real)/real= 0.0024012741438734033\n",
      "epoch= 542 iteration= 50407 loss= 0.00047324912156909704\n",
      "epoch= 542 iteration= 50427 loss= 0.0019377929857000709\n",
      "epoch= 542 iteration= 50447 loss= 0.000804138311650604\n",
      "epoch= 542 iteration= 50467 loss= 0.000998124829493463\n",
      "epoch= 542 iteration= 50487 loss= 0.011407822370529175\n",
      "test_data MSELoss:(pred-real)/real= 0.0023959353145781076\n",
      "epoch= 543 iteration= 50500 loss= 0.00047447573160752654\n",
      "epoch= 543 iteration= 50520 loss= 0.0019325873581692576\n",
      "epoch= 543 iteration= 50540 loss= 0.0008033029735088348\n",
      "epoch= 543 iteration= 50560 loss= 0.0009969505481421947\n",
      "epoch= 543 iteration= 50580 loss= 0.011387793347239494\n",
      "test_data MSELoss:(pred-real)/real= 0.002390219060341931\n",
      "epoch= 544 iteration= 50593 loss= 0.0004741161537822336\n",
      "epoch= 544 iteration= 50613 loss= 0.0019266735762357712\n",
      "epoch= 544 iteration= 50633 loss= 0.0008017504005692899\n",
      "epoch= 544 iteration= 50653 loss= 0.0009954918641597033\n",
      "epoch= 544 iteration= 50673 loss= 0.011366330087184906\n",
      "test_data MSELoss:(pred-real)/real= 0.002384631868658794\n",
      "epoch= 545 iteration= 50686 loss= 0.00047513924073427916\n",
      "epoch= 545 iteration= 50706 loss= 0.0019201738759875298\n",
      "epoch= 545 iteration= 50726 loss= 0.0008017215877771378\n",
      "epoch= 545 iteration= 50746 loss= 0.0009948137449100614\n",
      "epoch= 545 iteration= 50766 loss= 0.011344950646162033\n",
      "test_data MSELoss:(pred-real)/real= 0.002379888561942304\n",
      "epoch= 546 iteration= 50779 loss= 0.00047299033030867577\n",
      "epoch= 546 iteration= 50799 loss= 0.0019148483406752348\n",
      "epoch= 546 iteration= 50819 loss= 0.0007999073714017868\n",
      "epoch= 546 iteration= 50839 loss= 0.0009941202588379383\n",
      "epoch= 546 iteration= 50859 loss= 0.011323686689138412\n",
      "test_data MSELoss:(pred-real)/real= 0.0023745668748031473\n",
      "epoch= 547 iteration= 50872 loss= 0.0004750587686430663\n",
      "epoch= 547 iteration= 50892 loss= 0.0019093516748398542\n",
      "epoch= 547 iteration= 50912 loss= 0.0007993819308467209\n",
      "epoch= 547 iteration= 50932 loss= 0.0009932785760611296\n",
      "epoch= 547 iteration= 50952 loss= 0.011300697922706604\n",
      "test_data MSELoss:(pred-real)/real= 0.0023680404087321628\n",
      "epoch= 548 iteration= 50965 loss= 0.0004741039592772722\n",
      "epoch= 548 iteration= 50985 loss= 0.0019035300938412547\n",
      "epoch= 548 iteration= 51005 loss= 0.0007983554969541728\n",
      "epoch= 548 iteration= 51025 loss= 0.000992074143141508\n",
      "epoch= 548 iteration= 51045 loss= 0.011279798112809658\n",
      "test_data MSELoss:(pred-real)/real= 0.0023629758668701267\n",
      "epoch= 549 iteration= 51058 loss= 0.0004748170031234622\n",
      "epoch= 549 iteration= 51078 loss= 0.0018971181707456708\n",
      "epoch= 549 iteration= 51098 loss= 0.0007987468270584941\n",
      "epoch= 549 iteration= 51118 loss= 0.0009912509704008698\n",
      "epoch= 549 iteration= 51138 loss= 0.011260157451033592\n",
      "test_data MSELoss:(pred-real)/real= 0.0023577114851731393\n",
      "epoch= 550 iteration= 51151 loss= 0.0004752081003971398\n",
      "epoch= 550 iteration= 51171 loss= 0.0018932429375126958\n",
      "epoch= 550 iteration= 51191 loss= 0.0007982614333741367\n",
      "epoch= 550 iteration= 51211 loss= 0.0009903782047331333\n",
      "epoch= 550 iteration= 51231 loss= 0.011239358223974705\n",
      "test_data MSELoss:(pred-real)/real= 0.0023520091522691976\n",
      "epoch= 551 iteration= 51244 loss= 0.00047582684783264995\n",
      "epoch= 551 iteration= 51264 loss= 0.0018860151758417487\n",
      "epoch= 551 iteration= 51284 loss= 0.0007972518214955926\n",
      "epoch= 551 iteration= 51304 loss= 0.000989779131487012\n",
      "epoch= 551 iteration= 51324 loss= 0.011218634434044361\n",
      "test_data MSELoss:(pred-real)/real= 0.0023466346707815924\n",
      "epoch= 552 iteration= 51337 loss= 0.00047498155618086457\n",
      "epoch= 552 iteration= 51357 loss= 0.0018784217536449432\n",
      "epoch= 552 iteration= 51377 loss= 0.0007956146728247404\n",
      "epoch= 552 iteration= 51397 loss= 0.0009886057814583182\n",
      "epoch= 552 iteration= 51417 loss= 0.011197822168469429\n",
      "test_data MSELoss:(pred-real)/real= 0.0023416190395235187\n",
      "epoch= 553 iteration= 51430 loss= 0.00047341562458314\n",
      "epoch= 553 iteration= 51450 loss= 0.0018743944820016623\n",
      "epoch= 553 iteration= 51470 loss= 0.0007956569315865636\n",
      "epoch= 553 iteration= 51490 loss= 0.0009876752737909555\n",
      "epoch= 553 iteration= 51510 loss= 0.011178532615303993\n",
      "test_data MSELoss:(pred-real)/real= 0.00233608921795773\n",
      "epoch= 554 iteration= 51523 loss= 0.0004730883229058236\n",
      "epoch= 554 iteration= 51543 loss= 0.001866544596850872\n",
      "epoch= 554 iteration= 51563 loss= 0.0007950636208988726\n",
      "epoch= 554 iteration= 51583 loss= 0.0009866503532975912\n",
      "epoch= 554 iteration= 51603 loss= 0.011155594140291214\n",
      "test_data MSELoss:(pred-real)/real= 0.002331452246936452\n",
      "epoch= 555 iteration= 51616 loss= 0.0004724151804111898\n",
      "epoch= 555 iteration= 51636 loss= 0.0018645464442670345\n",
      "epoch= 555 iteration= 51656 loss= 0.000793974963016808\n",
      "epoch= 555 iteration= 51676 loss= 0.0009860557038336992\n",
      "epoch= 555 iteration= 51696 loss= 0.01113651692867279\n",
      "test_data MSELoss:(pred-real)/real= 0.0023264854163345364\n",
      "epoch= 556 iteration= 51709 loss= 0.00047101383097469807\n",
      "epoch= 556 iteration= 51729 loss= 0.0018570846877992153\n",
      "epoch= 556 iteration= 51749 loss= 0.0007933684391900897\n",
      "epoch= 556 iteration= 51769 loss= 0.0009854244999587536\n",
      "epoch= 556 iteration= 51789 loss= 0.0111146941781044\n",
      "test_data MSELoss:(pred-real)/real= 0.0023211043024073457\n",
      "epoch= 557 iteration= 51802 loss= 0.0004710148205049336\n",
      "epoch= 557 iteration= 51822 loss= 0.0018524107290431857\n",
      "epoch= 557 iteration= 51842 loss= 0.0007925872341729701\n",
      "epoch= 557 iteration= 51862 loss= 0.000984058016911149\n",
      "epoch= 557 iteration= 51882 loss= 0.011093087494373322\n",
      "test_data MSELoss:(pred-real)/real= 0.0023160541540063503\n",
      "epoch= 558 iteration= 51895 loss= 0.0004712669469881803\n",
      "epoch= 558 iteration= 51915 loss= 0.0018463830929249525\n",
      "epoch= 558 iteration= 51935 loss= 0.0007915273308753967\n",
      "epoch= 558 iteration= 51955 loss= 0.0009833151707425714\n",
      "epoch= 558 iteration= 51975 loss= 0.01107209175825119\n",
      "test_data MSELoss:(pred-real)/real= 0.002310554328788486\n",
      "epoch= 559 iteration= 51988 loss= 0.0004706667095888406\n",
      "epoch= 559 iteration= 52008 loss= 0.0018402161076664925\n",
      "epoch= 559 iteration= 52028 loss= 0.0007906907121650875\n",
      "epoch= 559 iteration= 52048 loss= 0.0009824014268815517\n",
      "epoch= 559 iteration= 52068 loss= 0.011053613387048244\n",
      "test_data MSELoss:(pred-real)/real= 0.002305322281447136\n",
      "epoch= 560 iteration= 52081 loss= 0.000470975530333817\n",
      "epoch= 560 iteration= 52101 loss= 0.0018354522762820125\n",
      "epoch= 560 iteration= 52121 loss= 0.0007899987976998091\n",
      "epoch= 560 iteration= 52141 loss= 0.0009811276104301214\n",
      "epoch= 560 iteration= 52161 loss= 0.011036068201065063\n",
      "test_data MSELoss:(pred-real)/real= 0.0023004902565541365\n",
      "epoch= 561 iteration= 52174 loss= 0.0004709583008661866\n",
      "epoch= 561 iteration= 52194 loss= 0.0018283464014530182\n",
      "epoch= 561 iteration= 52214 loss= 0.0007893795846030116\n",
      "epoch= 561 iteration= 52234 loss= 0.0009807159658521414\n",
      "epoch= 561 iteration= 52254 loss= 0.011014465242624283\n",
      "test_data MSELoss:(pred-real)/real= 0.00229521056300857\n",
      "epoch= 562 iteration= 52267 loss= 0.0004711114161182195\n",
      "epoch= 562 iteration= 52287 loss= 0.0018231667345389724\n",
      "epoch= 562 iteration= 52307 loss= 0.0007886679377406836\n",
      "epoch= 562 iteration= 52327 loss= 0.0009790831245481968\n",
      "epoch= 562 iteration= 52347 loss= 0.010992968454957008\n",
      "test_data MSELoss:(pred-real)/real= 0.0022904049740949026\n",
      "epoch= 563 iteration= 52360 loss= 0.0004700268036685884\n",
      "epoch= 563 iteration= 52380 loss= 0.0018173579592257738\n",
      "epoch= 563 iteration= 52400 loss= 0.0007882926147431135\n",
      "epoch= 563 iteration= 52420 loss= 0.0009779647225514054\n",
      "epoch= 563 iteration= 52440 loss= 0.010973215103149414\n",
      "test_data MSELoss:(pred-real)/real= 0.0022843862049436816\n",
      "epoch= 564 iteration= 52453 loss= 0.00046924652997404337\n",
      "epoch= 564 iteration= 52473 loss= 0.001811770722270012\n",
      "epoch= 564 iteration= 52493 loss= 0.0007870097178965807\n",
      "epoch= 564 iteration= 52513 loss= 0.0009775811340659857\n",
      "epoch= 564 iteration= 52533 loss= 0.010956233367323875\n",
      "test_data MSELoss:(pred-real)/real= 0.0022796114693240574\n",
      "epoch= 565 iteration= 52546 loss= 0.00046965727233327925\n",
      "epoch= 565 iteration= 52566 loss= 0.0018080405425280333\n",
      "epoch= 565 iteration= 52586 loss= 0.0007866743253543973\n",
      "epoch= 565 iteration= 52606 loss= 0.000976185838226229\n",
      "epoch= 565 iteration= 52626 loss= 0.010933445766568184\n",
      "test_data MSELoss:(pred-real)/real= 0.002274843531065724\n",
      "epoch= 566 iteration= 52639 loss= 0.0004688581102527678\n",
      "epoch= 566 iteration= 52659 loss= 0.0018043392337858677\n",
      "epoch= 566 iteration= 52679 loss= 0.0007859837496653199\n",
      "epoch= 566 iteration= 52699 loss= 0.0009755922947078943\n",
      "epoch= 566 iteration= 52719 loss= 0.010912067256867886\n",
      "test_data MSELoss:(pred-real)/real= 0.00226949646215265\n",
      "epoch= 567 iteration= 52732 loss= 0.00046767498133704066\n",
      "epoch= 567 iteration= 52752 loss= 0.001797673525288701\n",
      "epoch= 567 iteration= 52772 loss= 0.000785705866292119\n",
      "epoch= 567 iteration= 52792 loss= 0.0009745458955876529\n",
      "epoch= 567 iteration= 52812 loss= 0.01089029386639595\n",
      "test_data MSELoss:(pred-real)/real= 0.002264347341325548\n",
      "epoch= 568 iteration= 52825 loss= 0.00046841311268508434\n",
      "epoch= 568 iteration= 52845 loss= 0.0017916286597028375\n",
      "epoch= 568 iteration= 52865 loss= 0.0007842621998861432\n",
      "epoch= 568 iteration= 52885 loss= 0.0009734932100400329\n",
      "epoch= 568 iteration= 52905 loss= 0.010873477905988693\n",
      "test_data MSELoss:(pred-real)/real= 0.002259206039727562\n",
      "epoch= 569 iteration= 52918 loss= 0.00046817451948300004\n",
      "epoch= 569 iteration= 52938 loss= 0.0017855814658105373\n",
      "epoch= 569 iteration= 52958 loss= 0.0007837070734240115\n",
      "epoch= 569 iteration= 52978 loss= 0.0009724830742925406\n",
      "epoch= 569 iteration= 52998 loss= 0.01085389219224453\n",
      "test_data MSELoss:(pred-real)/real= 0.0022538504757297537\n",
      "epoch= 570 iteration= 53011 loss= 0.00046939949970692396\n",
      "epoch= 570 iteration= 53031 loss= 0.0017795778112486005\n",
      "epoch= 570 iteration= 53051 loss= 0.0007835447904653847\n",
      "epoch= 570 iteration= 53071 loss= 0.0009714506450109184\n",
      "epoch= 570 iteration= 53091 loss= 0.010837601497769356\n",
      "test_data MSELoss:(pred-real)/real= 0.002249272718068419\n",
      "epoch= 571 iteration= 53104 loss= 0.00046907455543987453\n",
      "epoch= 571 iteration= 53124 loss= 0.0017741277115419507\n",
      "epoch= 571 iteration= 53144 loss= 0.0007826588116586208\n",
      "epoch= 571 iteration= 53164 loss= 0.0009706193231977522\n",
      "epoch= 571 iteration= 53184 loss= 0.010817290283739567\n",
      "test_data MSELoss:(pred-real)/real= 0.0022441064033450354\n",
      "epoch= 572 iteration= 53197 loss= 0.0004669095214921981\n",
      "epoch= 572 iteration= 53217 loss= 0.001767612062394619\n",
      "epoch= 572 iteration= 53237 loss= 0.0007817058358341455\n",
      "epoch= 572 iteration= 53257 loss= 0.0009698030189611018\n",
      "epoch= 572 iteration= 53277 loss= 0.010793725028634071\n",
      "test_data MSELoss:(pred-real)/real= 0.002239580977604621\n",
      "epoch= 573 iteration= 53290 loss= 0.00046646330156363547\n",
      "epoch= 573 iteration= 53310 loss= 0.0017631201772019267\n",
      "epoch= 573 iteration= 53330 loss= 0.0007814000127837062\n",
      "epoch= 573 iteration= 53350 loss= 0.0009685999248176813\n",
      "epoch= 573 iteration= 53370 loss= 0.010780565440654755\n",
      "test_data MSELoss:(pred-real)/real= 0.002234583503902993\n",
      "epoch= 574 iteration= 53383 loss= 0.00046708076843060553\n",
      "epoch= 574 iteration= 53403 loss= 0.0017563975416123867\n",
      "epoch= 574 iteration= 53423 loss= 0.0007808118243701756\n",
      "epoch= 574 iteration= 53443 loss= 0.0009675916517153382\n",
      "epoch= 574 iteration= 53463 loss= 0.010758697986602783\n",
      "test_data MSELoss:(pred-real)/real= 0.0022302765121114338\n",
      "epoch= 575 iteration= 53476 loss= 0.00046616679173894227\n",
      "epoch= 575 iteration= 53496 loss= 0.0017525970470160246\n",
      "epoch= 575 iteration= 53516 loss= 0.0007793460972607136\n",
      "epoch= 575 iteration= 53536 loss= 0.000966629246249795\n",
      "epoch= 575 iteration= 53556 loss= 0.010738717392086983\n",
      "test_data MSELoss:(pred-real)/real= 0.0022252804871338108\n",
      "epoch= 576 iteration= 53569 loss= 0.00046671650488860905\n",
      "epoch= 576 iteration= 53589 loss= 0.0017491745529696345\n",
      "epoch= 576 iteration= 53609 loss= 0.0007788597140461206\n",
      "epoch= 576 iteration= 53629 loss= 0.000965891988016665\n",
      "epoch= 576 iteration= 53649 loss= 0.010719073936343193\n",
      "test_data MSELoss:(pred-real)/real= 0.002220319241233584\n",
      "epoch= 577 iteration= 53662 loss= 0.0004653853247873485\n",
      "epoch= 577 iteration= 53682 loss= 0.0017410952132195234\n",
      "epoch= 577 iteration= 53702 loss= 0.0007778869476169348\n",
      "epoch= 577 iteration= 53722 loss= 0.0009643579833209515\n",
      "epoch= 577 iteration= 53742 loss= 0.010700996033847332\n",
      "test_data MSELoss:(pred-real)/real= 0.0022144375446562967\n",
      "epoch= 578 iteration= 53755 loss= 0.00046605285024270415\n",
      "epoch= 578 iteration= 53775 loss= 0.0017380531644448638\n",
      "epoch= 578 iteration= 53795 loss= 0.0007765033515170217\n",
      "epoch= 578 iteration= 53815 loss= 0.0009635408059693873\n",
      "epoch= 578 iteration= 53835 loss= 0.010681085288524628\n",
      "test_data MSELoss:(pred-real)/real= 0.002210541291990214\n",
      "epoch= 579 iteration= 53848 loss= 0.0004650350601878017\n",
      "epoch= 579 iteration= 53868 loss= 0.0017308983951807022\n",
      "epoch= 579 iteration= 53888 loss= 0.0007758450810797513\n",
      "epoch= 579 iteration= 53908 loss= 0.0009625596576370299\n",
      "epoch= 579 iteration= 53928 loss= 0.010660155676305294\n",
      "test_data MSELoss:(pred-real)/real= 0.002205910959876039\n",
      "epoch= 580 iteration= 53941 loss= 0.0004647239111363888\n",
      "epoch= 580 iteration= 53961 loss= 0.0017290503019466996\n",
      "epoch= 580 iteration= 53981 loss= 0.0007752588717266917\n",
      "epoch= 580 iteration= 54001 loss= 0.0009611895075067878\n",
      "epoch= 580 iteration= 54021 loss= 0.010645641013979912\n",
      "test_data MSELoss:(pred-real)/real= 0.0022008392229003627\n",
      "epoch= 581 iteration= 54034 loss= 0.0004643747233785689\n",
      "epoch= 581 iteration= 54054 loss= 0.0017209877260029316\n",
      "epoch= 581 iteration= 54074 loss= 0.000775936699938029\n",
      "epoch= 581 iteration= 54094 loss= 0.0009608063264749944\n",
      "epoch= 581 iteration= 54114 loss= 0.010626113042235374\n",
      "test_data MSELoss:(pred-real)/real= 0.0021959042836291096\n",
      "epoch= 582 iteration= 54127 loss= 0.0004633456701412797\n",
      "epoch= 582 iteration= 54147 loss= 0.0017158740665763617\n",
      "epoch= 582 iteration= 54167 loss= 0.0007738553686067462\n",
      "epoch= 582 iteration= 54187 loss= 0.0009598215110599995\n",
      "epoch= 582 iteration= 54207 loss= 0.01060779020190239\n",
      "test_data MSELoss:(pred-real)/real= 0.002190811326727271\n",
      "epoch= 583 iteration= 54220 loss= 0.00046449463116005063\n",
      "epoch= 583 iteration= 54240 loss= 0.0017113236244767904\n",
      "epoch= 583 iteration= 54260 loss= 0.0007730104262009263\n",
      "epoch= 583 iteration= 54280 loss= 0.0009586845408193767\n",
      "epoch= 583 iteration= 54300 loss= 0.01059045735746622\n",
      "test_data MSELoss:(pred-real)/real= 0.0021859826584760514\n",
      "epoch= 584 iteration= 54313 loss= 0.00046420597936958075\n",
      "epoch= 584 iteration= 54333 loss= 0.001705824164673686\n",
      "epoch= 584 iteration= 54353 loss= 0.0007725943578407168\n",
      "epoch= 584 iteration= 54373 loss= 0.0009579532779753208\n",
      "epoch= 584 iteration= 54393 loss= 0.010571649298071861\n",
      "test_data MSELoss:(pred-real)/real= 0.0021817792245807746\n",
      "epoch= 585 iteration= 54406 loss= 0.00046309345634654164\n",
      "epoch= 585 iteration= 54426 loss= 0.001701625995337963\n",
      "epoch= 585 iteration= 54446 loss= 0.0007725523319095373\n",
      "epoch= 585 iteration= 54466 loss= 0.0009564667125232518\n",
      "epoch= 585 iteration= 54486 loss= 0.010550955310463905\n",
      "test_data MSELoss:(pred-real)/real= 0.002177306273046674\n",
      "epoch= 586 iteration= 54499 loss= 0.000460886862128973\n",
      "epoch= 586 iteration= 54519 loss= 0.001698469975963235\n",
      "epoch= 586 iteration= 54539 loss= 0.0007712678052484989\n",
      "epoch= 586 iteration= 54559 loss= 0.0009556281147524714\n",
      "epoch= 586 iteration= 54579 loss= 0.01053503155708313\n",
      "test_data MSELoss:(pred-real)/real= 0.0021718339962212164\n",
      "epoch= 587 iteration= 54592 loss= 0.00045928082545287907\n",
      "epoch= 587 iteration= 54612 loss= 0.0016895011067390442\n",
      "epoch= 587 iteration= 54632 loss= 0.0007709924830123782\n",
      "epoch= 587 iteration= 54652 loss= 0.0009543672204017639\n",
      "epoch= 587 iteration= 54672 loss= 0.010516420938074589\n",
      "test_data MSELoss:(pred-real)/real= 0.002167415449447516\n",
      "epoch= 588 iteration= 54685 loss= 0.0004595370846800506\n",
      "epoch= 588 iteration= 54705 loss= 0.0016866642981767654\n",
      "epoch= 588 iteration= 54725 loss= 0.0007707682671025395\n",
      "epoch= 588 iteration= 54745 loss= 0.0009531577816233039\n",
      "epoch= 588 iteration= 54765 loss= 0.010495730675756931\n",
      "test_data MSELoss:(pred-real)/real= 0.0021631945985265905\n",
      "epoch= 589 iteration= 54778 loss= 0.0004586521827150136\n",
      "epoch= 589 iteration= 54798 loss= 0.0016800742596387863\n",
      "epoch= 589 iteration= 54818 loss= 0.0007692457875236869\n",
      "epoch= 589 iteration= 54838 loss= 0.0009523418266326189\n",
      "epoch= 589 iteration= 54858 loss= 0.010479653254151344\n",
      "test_data MSELoss:(pred-real)/real= 0.002158753994283163\n",
      "epoch= 590 iteration= 54871 loss= 0.00045887252781540155\n",
      "epoch= 590 iteration= 54891 loss= 0.001675100065767765\n",
      "epoch= 590 iteration= 54911 loss= 0.0007691126083955169\n",
      "epoch= 590 iteration= 54931 loss= 0.0009513500845059752\n",
      "epoch= 590 iteration= 54951 loss= 0.010460840538144112\n",
      "test_data MSELoss:(pred-real)/real= 0.0021544278586386805\n",
      "epoch= 591 iteration= 54964 loss= 0.00045810750452801585\n",
      "epoch= 591 iteration= 54984 loss= 0.0016696000238880515\n",
      "epoch= 591 iteration= 55004 loss= 0.0007677357061766088\n",
      "epoch= 591 iteration= 55024 loss= 0.0009501747554168105\n",
      "epoch= 591 iteration= 55044 loss= 0.010441229678690434\n",
      "test_data MSELoss:(pred-real)/real= 0.002149533079419699\n",
      "epoch= 592 iteration= 55057 loss= 0.0004579987726174295\n",
      "epoch= 592 iteration= 55077 loss= 0.0016643665730953217\n",
      "epoch= 592 iteration= 55097 loss= 0.0007667372701689601\n",
      "epoch= 592 iteration= 55117 loss= 0.0009494784753769636\n",
      "epoch= 592 iteration= 55137 loss= 0.01042172871530056\n",
      "test_data MSELoss:(pred-real)/real= 0.0021456950505833244\n",
      "epoch= 593 iteration= 55150 loss= 0.0004582305555231869\n",
      "epoch= 593 iteration= 55170 loss= 0.001660081441514194\n",
      "epoch= 593 iteration= 55190 loss= 0.0007662294083274901\n",
      "epoch= 593 iteration= 55210 loss= 0.0009482366149313748\n",
      "epoch= 593 iteration= 55230 loss= 0.010402470827102661\n",
      "test_data MSELoss:(pred-real)/real= 0.00214079143673492\n",
      "epoch= 594 iteration= 55243 loss= 0.0004554134211502969\n",
      "epoch= 594 iteration= 55263 loss= 0.0016560597578063607\n",
      "epoch= 594 iteration= 55283 loss= 0.0007649470353499055\n",
      "epoch= 594 iteration= 55303 loss= 0.0009472353267483413\n",
      "epoch= 594 iteration= 55323 loss= 0.010389501228928566\n",
      "test_data MSELoss:(pred-real)/real= 0.0021359355628697407\n",
      "epoch= 595 iteration= 55336 loss= 0.0004569411394186318\n",
      "epoch= 595 iteration= 55356 loss= 0.0016489230329170823\n",
      "epoch= 595 iteration= 55376 loss= 0.0007651369087398052\n",
      "epoch= 595 iteration= 55396 loss= 0.000946373853366822\n",
      "epoch= 595 iteration= 55416 loss= 0.010371594689786434\n",
      "test_data MSELoss:(pred-real)/real= 0.0021314072073437274\n",
      "epoch= 596 iteration= 55429 loss= 0.00045523146400228143\n",
      "epoch= 596 iteration= 55449 loss= 0.0016452257987111807\n",
      "epoch= 596 iteration= 55469 loss= 0.0007639374816790223\n",
      "epoch= 596 iteration= 55489 loss= 0.0009450700017623603\n",
      "epoch= 596 iteration= 55509 loss= 0.01035259012132883\n",
      "test_data MSELoss:(pred-real)/real= 0.0021266805447844993\n",
      "epoch= 597 iteration= 55522 loss= 0.00045493184006772935\n",
      "epoch= 597 iteration= 55542 loss= 0.00163971446454525\n",
      "epoch= 597 iteration= 55562 loss= 0.0007628798484802246\n",
      "epoch= 597 iteration= 55582 loss= 0.0009437620756216347\n",
      "epoch= 597 iteration= 55602 loss= 0.010337281972169876\n",
      "test_data MSELoss:(pred-real)/real= 0.0021231276429413506\n",
      "epoch= 598 iteration= 55615 loss= 0.000452128064353019\n",
      "epoch= 598 iteration= 55635 loss= 0.0016351134981960058\n",
      "epoch= 598 iteration= 55655 loss= 0.0007620869437232614\n",
      "epoch= 598 iteration= 55675 loss= 0.0009426787728443742\n",
      "epoch= 598 iteration= 55695 loss= 0.010316899046301842\n",
      "test_data MSELoss:(pred-real)/real= 0.002119033823772851\n",
      "epoch= 599 iteration= 55708 loss= 0.0004500677459873259\n",
      "epoch= 599 iteration= 55728 loss= 0.001629712525755167\n",
      "epoch= 599 iteration= 55748 loss= 0.0007618985837325454\n",
      "epoch= 599 iteration= 55768 loss= 0.0009420572896488011\n",
      "epoch= 599 iteration= 55788 loss= 0.010301988571882248\n",
      "test_data MSELoss:(pred-real)/real= 0.0021142741898074746\n",
      "epoch= 600 iteration= 55801 loss= 0.00044901706860400736\n",
      "epoch= 600 iteration= 55821 loss= 0.0016259539406746626\n",
      "epoch= 600 iteration= 55841 loss= 0.0007608194719068706\n",
      "epoch= 600 iteration= 55861 loss= 0.0009409405756741762\n",
      "epoch= 600 iteration= 55881 loss= 0.010284416377544403\n",
      "test_data MSELoss:(pred-real)/real= 0.002110192112417685\n",
      "epoch= 601 iteration= 55894 loss= 0.0004491204454097897\n",
      "epoch= 601 iteration= 55914 loss= 0.0016211270121857524\n",
      "epoch= 601 iteration= 55934 loss= 0.0007603717967867851\n",
      "epoch= 601 iteration= 55954 loss= 0.0009391597122885287\n",
      "epoch= 601 iteration= 55974 loss= 0.010264664888381958\n",
      "test_data MSELoss:(pred-real)/real= 0.0021058761178412372\n",
      "epoch= 602 iteration= 55987 loss= 0.0004466129466891289\n",
      "epoch= 602 iteration= 56007 loss= 0.0016151394229382277\n",
      "epoch= 602 iteration= 56027 loss= 0.0007588079897686839\n",
      "epoch= 602 iteration= 56047 loss= 0.0009381966083310544\n",
      "epoch= 602 iteration= 56067 loss= 0.010248057544231415\n",
      "test_data MSELoss:(pred-real)/real= 0.0021015339362848964\n",
      "epoch= 603 iteration= 56080 loss= 0.00044786298531107605\n",
      "epoch= 603 iteration= 56100 loss= 0.0016130844596773386\n",
      "epoch= 603 iteration= 56120 loss= 0.0007574633928015828\n",
      "epoch= 603 iteration= 56140 loss= 0.000937385018914938\n",
      "epoch= 603 iteration= 56160 loss= 0.010234598070383072\n",
      "test_data MSELoss:(pred-real)/real= 0.0020971798868332473\n",
      "epoch= 604 iteration= 56173 loss= 0.0004469011619221419\n",
      "epoch= 604 iteration= 56193 loss= 0.001607904676347971\n",
      "epoch= 604 iteration= 56213 loss= 0.0007573860930278897\n",
      "epoch= 604 iteration= 56233 loss= 0.0009361023548990488\n",
      "epoch= 604 iteration= 56253 loss= 0.010216988623142242\n",
      "test_data MSELoss:(pred-real)/real= 0.0020929190310804793\n",
      "epoch= 605 iteration= 56266 loss= 0.00044445658568292856\n",
      "epoch= 605 iteration= 56286 loss= 0.001599953044205904\n",
      "epoch= 605 iteration= 56306 loss= 0.0007567806169390678\n",
      "epoch= 605 iteration= 56326 loss= 0.0009353526402264833\n",
      "epoch= 605 iteration= 56346 loss= 0.010198220610618591\n",
      "test_data MSELoss:(pred-real)/real= 0.0020886315162190134\n",
      "epoch= 606 iteration= 56359 loss= 0.0004453744331840426\n",
      "epoch= 606 iteration= 56379 loss= 0.0015951761743053794\n",
      "epoch= 606 iteration= 56399 loss= 0.0007556320633739233\n",
      "epoch= 606 iteration= 56419 loss= 0.0009345610742457211\n",
      "epoch= 606 iteration= 56439 loss= 0.010180039331316948\n",
      "test_data MSELoss:(pred-real)/real= 0.002084244369244617\n",
      "epoch= 607 iteration= 56452 loss= 0.00044677508412860334\n",
      "epoch= 607 iteration= 56472 loss= 0.0015916177071630955\n",
      "epoch= 607 iteration= 56492 loss= 0.0007548253634013236\n",
      "epoch= 607 iteration= 56512 loss= 0.0009328476735390723\n",
      "epoch= 607 iteration= 56532 loss= 0.010164175182580948\n",
      "test_data MSELoss:(pred-real)/real= 0.0020796648491846603\n",
      "epoch= 608 iteration= 56545 loss= 0.00044537894427776337\n",
      "epoch= 608 iteration= 56565 loss= 0.0015866231406107545\n",
      "epoch= 608 iteration= 56585 loss= 0.0007543511455878615\n",
      "epoch= 608 iteration= 56605 loss= 0.0009321027901023626\n",
      "epoch= 608 iteration= 56625 loss= 0.010148156434297562\n",
      "test_data MSELoss:(pred-real)/real= 0.002075914565163354\n",
      "epoch= 609 iteration= 56638 loss= 0.00044243165757507086\n",
      "epoch= 609 iteration= 56658 loss= 0.0015812984202057123\n",
      "epoch= 609 iteration= 56678 loss= 0.0007542009698227048\n",
      "epoch= 609 iteration= 56698 loss= 0.0009309002198278904\n",
      "epoch= 609 iteration= 56718 loss= 0.010129869915544987\n",
      "test_data MSELoss:(pred-real)/real= 0.002072561347934728\n",
      "epoch= 610 iteration= 56731 loss= 0.00044102335232309997\n",
      "epoch= 610 iteration= 56751 loss= 0.001576719107106328\n",
      "epoch= 610 iteration= 56771 loss= 0.0007528586429543793\n",
      "epoch= 610 iteration= 56791 loss= 0.0009303493425250053\n",
      "epoch= 610 iteration= 56811 loss= 0.010115922428667545\n",
      "test_data MSELoss:(pred-real)/real= 0.0020680687416137922\n",
      "epoch= 611 iteration= 56824 loss= 0.00043989127152599394\n",
      "epoch= 611 iteration= 56844 loss= 0.0015728609869256616\n",
      "epoch= 611 iteration= 56864 loss= 0.0007518658530898392\n",
      "epoch= 611 iteration= 56884 loss= 0.0009285167325288057\n",
      "epoch= 611 iteration= 56904 loss= 0.01009940542280674\n",
      "test_data MSELoss:(pred-real)/real= 0.0020635167473099297\n",
      "epoch= 612 iteration= 56917 loss= 0.00043910162639804184\n",
      "epoch= 612 iteration= 56937 loss= 0.0015668823616579175\n",
      "epoch= 612 iteration= 56957 loss= 0.0007516033365391195\n",
      "epoch= 612 iteration= 56977 loss= 0.0009272861061617732\n",
      "epoch= 612 iteration= 56997 loss= 0.010084530338644981\n",
      "test_data MSELoss:(pred-real)/real= 0.0020605211126773306\n",
      "epoch= 613 iteration= 57010 loss= 0.0004374569980427623\n",
      "epoch= 613 iteration= 57030 loss= 0.0015638757031410933\n",
      "epoch= 613 iteration= 57050 loss= 0.0007497984915971756\n",
      "epoch= 613 iteration= 57070 loss= 0.0009263218380510807\n",
      "epoch= 613 iteration= 57090 loss= 0.010064695961773396\n",
      "test_data MSELoss:(pred-real)/real= 0.002056070875066022\n",
      "epoch= 614 iteration= 57103 loss= 0.0004366812063381076\n",
      "epoch= 614 iteration= 57123 loss= 0.0015578469028696418\n",
      "epoch= 614 iteration= 57143 loss= 0.0007490391144528985\n",
      "epoch= 614 iteration= 57163 loss= 0.000925077882129699\n",
      "epoch= 614 iteration= 57183 loss= 0.010052068158984184\n",
      "test_data MSELoss:(pred-real)/real= 0.0020513899443257186\n",
      "epoch= 615 iteration= 57196 loss= 0.00043517406447790563\n",
      "epoch= 615 iteration= 57216 loss= 0.0015543176559731364\n",
      "epoch= 615 iteration= 57236 loss= 0.000748789869248867\n",
      "epoch= 615 iteration= 57256 loss= 0.000924487947486341\n",
      "epoch= 615 iteration= 57276 loss= 0.010032251477241516\n",
      "test_data MSELoss:(pred-real)/real= 0.002047853531419403\n",
      "epoch= 616 iteration= 57289 loss= 0.00043357390677556396\n",
      "epoch= 616 iteration= 57309 loss= 0.0015482086455449462\n",
      "epoch= 616 iteration= 57329 loss= 0.0007479084888473153\n",
      "epoch= 616 iteration= 57349 loss= 0.000922432285733521\n",
      "epoch= 616 iteration= 57369 loss= 0.010015261359512806\n",
      "test_data MSELoss:(pred-real)/real= 0.002043809289009207\n",
      "epoch= 617 iteration= 57382 loss= 0.0004329647636041045\n",
      "epoch= 617 iteration= 57402 loss= 0.0015445610042661428\n",
      "epoch= 617 iteration= 57422 loss= 0.0007472332217730582\n",
      "epoch= 617 iteration= 57442 loss= 0.0009213474113494158\n",
      "epoch= 617 iteration= 57462 loss= 0.009997554123401642\n",
      "test_data MSELoss:(pred-real)/real= 0.0020394343462410485\n",
      "epoch= 618 iteration= 57475 loss= 0.0004332509124651551\n",
      "epoch= 618 iteration= 57495 loss= 0.0015404593432322145\n",
      "epoch= 618 iteration= 57515 loss= 0.0007465884555131197\n",
      "epoch= 618 iteration= 57535 loss= 0.0009206802351400256\n",
      "epoch= 618 iteration= 57555 loss= 0.009982218965888023\n",
      "test_data MSELoss:(pred-real)/real= 0.002035984083906644\n",
      "epoch= 619 iteration= 57568 loss= 0.00043284561252221465\n",
      "epoch= 619 iteration= 57588 loss= 0.0015359136741608381\n",
      "epoch= 619 iteration= 57608 loss= 0.0007456775638274848\n",
      "epoch= 619 iteration= 57628 loss= 0.0009199404739774764\n",
      "epoch= 619 iteration= 57648 loss= 0.009965447708964348\n",
      "test_data MSELoss:(pred-real)/real= 0.0020320102468960816\n",
      "epoch= 620 iteration= 57661 loss= 0.00042965682223439217\n",
      "epoch= 620 iteration= 57681 loss= 0.0015325506683439016\n",
      "epoch= 620 iteration= 57701 loss= 0.0007443743525072932\n",
      "epoch= 620 iteration= 57721 loss= 0.0009185667731799185\n",
      "epoch= 620 iteration= 57741 loss= 0.009949635714292526\n",
      "test_data MSELoss:(pred-real)/real= 0.0020278202331004045\n",
      "epoch= 621 iteration= 57754 loss= 0.0004302242014091462\n",
      "epoch= 621 iteration= 57774 loss= 0.0015269076684489846\n",
      "epoch= 621 iteration= 57794 loss= 0.000743378303013742\n",
      "epoch= 621 iteration= 57814 loss= 0.000916867982596159\n",
      "epoch= 621 iteration= 57834 loss= 0.009938343428075314\n",
      "test_data MSELoss:(pred-real)/real= 0.0020250475887830057\n",
      "epoch= 622 iteration= 57847 loss= 0.0004269335186108947\n",
      "epoch= 622 iteration= 57867 loss= 0.0015219885390251875\n",
      "epoch= 622 iteration= 57887 loss= 0.0007430406403727829\n",
      "epoch= 622 iteration= 57907 loss= 0.0009159313631244004\n",
      "epoch= 622 iteration= 57927 loss= 0.00991890486329794\n",
      "test_data MSELoss:(pred-real)/real= 0.0020207109531232468\n",
      "epoch= 623 iteration= 57940 loss= 0.0004247355100233108\n",
      "epoch= 623 iteration= 57960 loss= 0.0015162078198045492\n",
      "epoch= 623 iteration= 57980 loss= 0.0007420751499012113\n",
      "epoch= 623 iteration= 58000 loss= 0.0009148065000772476\n",
      "epoch= 623 iteration= 58020 loss= 0.009901167824864388\n",
      "test_data MSELoss:(pred-real)/real= 0.0020168496606250605\n",
      "epoch= 624 iteration= 58033 loss= 0.0004237622779328376\n",
      "epoch= 624 iteration= 58053 loss= 0.0015119722811505198\n",
      "epoch= 624 iteration= 58073 loss= 0.000740820774808526\n",
      "epoch= 624 iteration= 58093 loss= 0.0009135454893112183\n",
      "epoch= 624 iteration= 58113 loss= 0.009886489249765873\n",
      "test_data MSELoss:(pred-real)/real= 0.0020138645195402205\n",
      "epoch= 625 iteration= 58126 loss= 0.00042237358866259456\n",
      "epoch= 625 iteration= 58146 loss= 0.0015074912225827575\n",
      "epoch= 625 iteration= 58166 loss= 0.0007406257791444659\n",
      "epoch= 625 iteration= 58186 loss= 0.0009124272619374096\n",
      "epoch= 625 iteration= 58206 loss= 0.009871626272797585\n",
      "test_data MSELoss:(pred-real)/real= 0.002010496423786713\n",
      "epoch= 626 iteration= 58219 loss= 0.00042075925739482045\n",
      "epoch= 626 iteration= 58239 loss= 0.0015039574354887009\n",
      "epoch= 626 iteration= 58259 loss= 0.0007390054524876177\n",
      "epoch= 626 iteration= 58279 loss= 0.0009114589774981141\n",
      "epoch= 626 iteration= 58299 loss= 0.009858481585979462\n",
      "test_data MSELoss:(pred-real)/real= 0.0020055299165606913\n",
      "epoch= 627 iteration= 58312 loss= 0.0004187091253697872\n",
      "epoch= 627 iteration= 58332 loss= 0.0014991930220276117\n",
      "epoch= 627 iteration= 58352 loss= 0.0007378436275757849\n",
      "epoch= 627 iteration= 58372 loss= 0.0009101759642362595\n",
      "epoch= 627 iteration= 58392 loss= 0.00984228402376175\n",
      "test_data MSELoss:(pred-real)/real= 0.002003003862531235\n",
      "epoch= 628 iteration= 58405 loss= 0.00041802204214036465\n",
      "epoch= 628 iteration= 58425 loss= 0.0014940174296498299\n",
      "epoch= 628 iteration= 58445 loss= 0.0007374247652478516\n",
      "epoch= 628 iteration= 58465 loss= 0.0009092949912883341\n",
      "epoch= 628 iteration= 58485 loss= 0.009824523702263832\n",
      "test_data MSELoss:(pred-real)/real= 0.002000155283086416\n",
      "epoch= 629 iteration= 58498 loss= 0.0004160016542300582\n",
      "epoch= 629 iteration= 58518 loss= 0.0014923522248864174\n",
      "epoch= 629 iteration= 58538 loss= 0.0007360000163316727\n",
      "epoch= 629 iteration= 58558 loss= 0.0009077662252821028\n",
      "epoch= 629 iteration= 58578 loss= 0.00980960763990879\n",
      "test_data MSELoss:(pred-real)/real= 0.0019957807978951475\n",
      "epoch= 630 iteration= 58591 loss= 0.0004171907203271985\n",
      "epoch= 630 iteration= 58611 loss= 0.00148646323941648\n",
      "epoch= 630 iteration= 58631 loss= 0.0007353541441261768\n",
      "epoch= 630 iteration= 58651 loss= 0.0009067088249139488\n",
      "epoch= 630 iteration= 58671 loss= 0.009790736250579357\n",
      "test_data MSELoss:(pred-real)/real= 0.0019926665376664866\n",
      "epoch= 631 iteration= 58684 loss= 0.00041516023338772357\n",
      "epoch= 631 iteration= 58704 loss= 0.001481498358771205\n",
      "epoch= 631 iteration= 58724 loss= 0.0007347587961703539\n",
      "epoch= 631 iteration= 58744 loss= 0.0009049325017258525\n",
      "epoch= 631 iteration= 58764 loss= 0.009777486324310303\n",
      "test_data MSELoss:(pred-real)/real= 0.0019884603259722805\n",
      "epoch= 632 iteration= 58777 loss= 0.00041280468576587737\n",
      "epoch= 632 iteration= 58797 loss= 0.0014772311551496387\n",
      "epoch= 632 iteration= 58817 loss= 0.0007335497648455203\n",
      "epoch= 632 iteration= 58837 loss= 0.000904357060790062\n",
      "epoch= 632 iteration= 58857 loss= 0.009767511859536171\n",
      "test_data MSELoss:(pred-real)/real= 0.0019841969672900936\n",
      "epoch= 633 iteration= 58870 loss= 0.00041032969602383673\n",
      "epoch= 633 iteration= 58890 loss= 0.0014727108646184206\n",
      "epoch= 633 iteration= 58910 loss= 0.0007328499923460186\n",
      "epoch= 633 iteration= 58930 loss= 0.0009033129899762571\n",
      "epoch= 633 iteration= 58950 loss= 0.009747840464115143\n",
      "test_data MSELoss:(pred-real)/real= 0.0019815803308220995\n",
      "epoch= 634 iteration= 58963 loss= 0.0004088990390300751\n",
      "epoch= 634 iteration= 58983 loss= 0.0014695150312036276\n",
      "epoch= 634 iteration= 59003 loss= 0.0007326359627768397\n",
      "epoch= 634 iteration= 59023 loss= 0.0009018253767862916\n",
      "epoch= 634 iteration= 59043 loss= 0.00973473209887743\n",
      "test_data MSELoss:(pred-real)/real= 0.001978200176381506\n",
      "epoch= 635 iteration= 59056 loss= 0.0004069986171089113\n",
      "epoch= 635 iteration= 59076 loss= 0.0014639561995863914\n",
      "epoch= 635 iteration= 59096 loss= 0.0007325065671466291\n",
      "epoch= 635 iteration= 59116 loss= 0.0009005226893350482\n",
      "epoch= 635 iteration= 59136 loss= 0.009723241440951824\n",
      "test_data MSELoss:(pred-real)/real= 0.0019743203172563678\n",
      "epoch= 636 iteration= 59149 loss= 0.0004046677495352924\n",
      "epoch= 636 iteration= 59169 loss= 0.001460496336221695\n",
      "epoch= 636 iteration= 59189 loss= 0.0007311519584618509\n",
      "epoch= 636 iteration= 59209 loss= 0.000899155274964869\n",
      "epoch= 636 iteration= 59229 loss= 0.009704679250717163\n",
      "test_data MSELoss:(pred-real)/real= 0.0019710099117623437\n",
      "epoch= 637 iteration= 59242 loss= 0.0004034978337585926\n",
      "epoch= 637 iteration= 59262 loss= 0.001454868819564581\n",
      "epoch= 637 iteration= 59282 loss= 0.0007301000878214836\n",
      "epoch= 637 iteration= 59302 loss= 0.0008980421116575599\n",
      "epoch= 637 iteration= 59322 loss= 0.009688204154372215\n",
      "test_data MSELoss:(pred-real)/real= 0.001967700282370465\n",
      "epoch= 638 iteration= 59335 loss= 0.0004013073048554361\n",
      "epoch= 638 iteration= 59355 loss= 0.0014504188438877463\n",
      "epoch= 638 iteration= 59375 loss= 0.0007285180035978556\n",
      "epoch= 638 iteration= 59395 loss= 0.0008969255723059177\n",
      "epoch= 638 iteration= 59415 loss= 0.009672563523054123\n",
      "test_data MSELoss:(pred-real)/real= 0.0019649333310856796\n",
      "epoch= 639 iteration= 59428 loss= 0.0003995741135440767\n",
      "epoch= 639 iteration= 59448 loss= 0.001447523245587945\n",
      "epoch= 639 iteration= 59468 loss= 0.000727596168871969\n",
      "epoch= 639 iteration= 59488 loss= 0.0008954337099567056\n",
      "epoch= 639 iteration= 59508 loss= 0.009659009985625744\n",
      "test_data MSELoss:(pred-real)/real= 0.0019610690796980634\n",
      "epoch= 640 iteration= 59521 loss= 0.00039740398642607033\n",
      "epoch= 640 iteration= 59541 loss= 0.001443049986846745\n",
      "epoch= 640 iteration= 59561 loss= 0.0007263135048560798\n",
      "epoch= 640 iteration= 59581 loss= 0.0008943536086007953\n",
      "epoch= 640 iteration= 59601 loss= 0.009642665274441242\n",
      "test_data MSELoss:(pred-real)/real= 0.001957635052450415\n",
      "epoch= 641 iteration= 59614 loss= 0.0003961705369874835\n",
      "epoch= 641 iteration= 59634 loss= 0.0014381688088178635\n",
      "epoch= 641 iteration= 59654 loss= 0.0007250764174386859\n",
      "epoch= 641 iteration= 59674 loss= 0.000893455813638866\n",
      "epoch= 641 iteration= 59694 loss= 0.009628525003790855\n",
      "test_data MSELoss:(pred-real)/real= 0.001954307433657555\n",
      "epoch= 642 iteration= 59707 loss= 0.0003940439783036709\n",
      "epoch= 642 iteration= 59727 loss= 0.0014334794832393527\n",
      "epoch= 642 iteration= 59747 loss= 0.0007248874753713608\n",
      "epoch= 642 iteration= 59767 loss= 0.0008924518479034305\n",
      "epoch= 642 iteration= 59787 loss= 0.009611701592803001\n",
      "test_data MSELoss:(pred-real)/real= 0.0019524925171733936\n",
      "epoch= 643 iteration= 59800 loss= 0.00039172920514829457\n",
      "epoch= 643 iteration= 59820 loss= 0.0014288354432210326\n",
      "epoch= 643 iteration= 59840 loss= 0.0007243396248668432\n",
      "epoch= 643 iteration= 59860 loss= 0.0008906804141588509\n",
      "epoch= 643 iteration= 59880 loss= 0.009597649797797203\n",
      "test_data MSELoss:(pred-real)/real= 0.0019482106597731924\n",
      "epoch= 644 iteration= 59893 loss= 0.0003892797976732254\n",
      "epoch= 644 iteration= 59913 loss= 0.00142570398747921\n",
      "epoch= 644 iteration= 59933 loss= 0.0007238073740154505\n",
      "epoch= 644 iteration= 59953 loss= 0.0008895245264284313\n",
      "epoch= 644 iteration= 59973 loss= 0.009581372141838074\n",
      "test_data MSELoss:(pred-real)/real= 0.0019451593204090993\n",
      "epoch= 645 iteration= 59986 loss= 0.00038569941534660757\n",
      "epoch= 645 iteration= 60006 loss= 0.0014196120901033282\n",
      "epoch= 645 iteration= 60026 loss= 0.0007221460109576583\n",
      "epoch= 645 iteration= 60046 loss= 0.0008880816167220473\n",
      "epoch= 645 iteration= 60066 loss= 0.009568434208631516\n",
      "test_data MSELoss:(pred-real)/real= 0.001943306358751013\n",
      "epoch= 646 iteration= 60079 loss= 0.000384121376555413\n",
      "epoch= 646 iteration= 60099 loss= 0.001416817307472229\n",
      "epoch= 646 iteration= 60119 loss= 0.000721558986697346\n",
      "epoch= 646 iteration= 60139 loss= 0.0008869176963344216\n",
      "epoch= 646 iteration= 60159 loss= 0.00955238752067089\n",
      "test_data MSELoss:(pred-real)/real= 0.0019400368603075752\n",
      "epoch= 647 iteration= 60172 loss= 0.00038057565689086914\n",
      "epoch= 647 iteration= 60192 loss= 0.0014135484816506505\n",
      "epoch= 647 iteration= 60212 loss= 0.0007205315632745624\n",
      "epoch= 647 iteration= 60232 loss= 0.0008857405046001077\n",
      "epoch= 647 iteration= 60252 loss= 0.009536500088870525\n",
      "test_data MSELoss:(pred-real)/real= 0.0019363116945088324\n",
      "epoch= 648 iteration= 60265 loss= 0.00037841760786250234\n",
      "epoch= 648 iteration= 60285 loss= 0.0014094896614551544\n",
      "epoch= 648 iteration= 60305 loss= 0.0007195989601314068\n",
      "epoch= 648 iteration= 60325 loss= 0.0008845201227813959\n",
      "epoch= 648 iteration= 60345 loss= 0.009521620348095894\n",
      "test_data MSELoss:(pred-real)/real= 0.0019332871904932996\n",
      "epoch= 649 iteration= 60358 loss= 0.0003754948847927153\n",
      "epoch= 649 iteration= 60378 loss= 0.0014042365364730358\n",
      "epoch= 649 iteration= 60398 loss= 0.0007183132693171501\n",
      "epoch= 649 iteration= 60418 loss= 0.0008832864696159959\n",
      "epoch= 649 iteration= 60438 loss= 0.009512141346931458\n",
      "test_data MSELoss:(pred-real)/real= 0.0019310878318113585\n",
      "epoch= 650 iteration= 60451 loss= 0.0003722108085639775\n",
      "epoch= 650 iteration= 60471 loss= 0.0013999758521094918\n",
      "epoch= 650 iteration= 60491 loss= 0.0007172019686549902\n",
      "epoch= 650 iteration= 60511 loss= 0.0008820150978863239\n",
      "epoch= 650 iteration= 60531 loss= 0.009493328630924225\n",
      "test_data MSELoss:(pred-real)/real= 0.0019282494985317397\n",
      "epoch= 651 iteration= 60544 loss= 0.00037103804061189294\n",
      "epoch= 651 iteration= 60564 loss= 0.0013956957263872027\n",
      "epoch= 651 iteration= 60584 loss= 0.0007164218695834279\n",
      "epoch= 651 iteration= 60604 loss= 0.0008812937885522842\n",
      "epoch= 651 iteration= 60624 loss= 0.009483523666858673\n",
      "test_data MSELoss:(pred-real)/real= 0.001925289989837135\n",
      "epoch= 652 iteration= 60637 loss= 0.0003684409020934254\n",
      "epoch= 652 iteration= 60657 loss= 0.001390454126521945\n",
      "epoch= 652 iteration= 60677 loss= 0.0007157470681704581\n",
      "epoch= 652 iteration= 60697 loss= 0.0008793673478066921\n",
      "epoch= 652 iteration= 60717 loss= 0.00946469884365797\n",
      "test_data MSELoss:(pred-real)/real= 0.0019230511518091792\n",
      "epoch= 653 iteration= 60730 loss= 0.00036648736568167806\n",
      "epoch= 653 iteration= 60750 loss= 0.0013875197619199753\n",
      "epoch= 653 iteration= 60770 loss= 0.0007144389091990888\n",
      "epoch= 653 iteration= 60790 loss= 0.0008775694295763969\n",
      "epoch= 653 iteration= 60810 loss= 0.009450783021748066\n",
      "test_data MSELoss:(pred-real)/real= 0.0019192859617760405\n",
      "epoch= 654 iteration= 60823 loss= 0.0003642367955762893\n",
      "epoch= 654 iteration= 60843 loss= 0.001382644404657185\n",
      "epoch= 654 iteration= 60863 loss= 0.0007132540922611952\n",
      "epoch= 654 iteration= 60883 loss= 0.0008769277483224869\n",
      "epoch= 654 iteration= 60903 loss= 0.009437340311706066\n",
      "test_data MSELoss:(pred-real)/real= 0.0019160742150335056\n",
      "epoch= 655 iteration= 60916 loss= 0.0003630076244007796\n",
      "epoch= 655 iteration= 60936 loss= 0.0013780881417915225\n",
      "epoch= 655 iteration= 60956 loss= 0.00071266561280936\n",
      "epoch= 655 iteration= 60976 loss= 0.0008754757000133395\n",
      "epoch= 655 iteration= 60996 loss= 0.009420283138751984\n",
      "test_data MSELoss:(pred-real)/real= 0.0019134870025381031\n",
      "epoch= 656 iteration= 61009 loss= 0.0003615751047618687\n",
      "epoch= 656 iteration= 61029 loss= 0.0013739399146288633\n",
      "epoch= 656 iteration= 61049 loss= 0.0007116396445780993\n",
      "epoch= 656 iteration= 61069 loss= 0.0008743048529140651\n",
      "epoch= 656 iteration= 61089 loss= 0.009408622980117798\n",
      "test_data MSELoss:(pred-real)/real= 0.0019102784022430165\n",
      "epoch= 657 iteration= 61102 loss= 0.0003586884995456785\n",
      "epoch= 657 iteration= 61122 loss= 0.0013718768022954464\n",
      "epoch= 657 iteration= 61142 loss= 0.0007100802613422275\n",
      "epoch= 657 iteration= 61162 loss= 0.0008729349938221276\n",
      "epoch= 657 iteration= 61182 loss= 0.009390747174620628\n",
      "test_data MSELoss:(pred-real)/real= 0.0019083554927621866\n",
      "epoch= 658 iteration= 61195 loss= 0.0003559256438165903\n",
      "epoch= 658 iteration= 61215 loss= 0.0013662144774571061\n",
      "epoch= 658 iteration= 61235 loss= 0.0007098914356902242\n",
      "epoch= 658 iteration= 61255 loss= 0.0008716421434655786\n",
      "epoch= 658 iteration= 61275 loss= 0.009377162903547287\n",
      "test_data MSELoss:(pred-real)/real= 0.001906339011232679\n",
      "epoch= 659 iteration= 61288 loss= 0.00035331471008248627\n",
      "epoch= 659 iteration= 61308 loss= 0.001362439477816224\n",
      "epoch= 659 iteration= 61328 loss= 0.0007086560362949967\n",
      "epoch= 659 iteration= 61348 loss= 0.0008706191438250244\n",
      "epoch= 659 iteration= 61368 loss= 0.009361816570162773\n",
      "test_data MSELoss:(pred-real)/real= 0.0019034326186455372\n",
      "epoch= 660 iteration= 61381 loss= 0.000350057176547125\n",
      "epoch= 660 iteration= 61401 loss= 0.0013587106950581074\n",
      "epoch= 660 iteration= 61421 loss= 0.000707854749634862\n",
      "epoch= 660 iteration= 61441 loss= 0.0008691665134392679\n",
      "epoch= 660 iteration= 61461 loss= 0.009350543841719627\n",
      "test_data MSELoss:(pred-real)/real= 0.0019011260867248187\n",
      "epoch= 661 iteration= 61474 loss= 0.00034770695492625237\n",
      "epoch= 661 iteration= 61494 loss= 0.001354298205114901\n",
      "epoch= 661 iteration= 61514 loss= 0.0007063632365316153\n",
      "epoch= 661 iteration= 61534 loss= 0.0008678552112542093\n",
      "epoch= 661 iteration= 61554 loss= 0.00933532789349556\n",
      "test_data MSELoss:(pred-real)/real= 0.0018986239738296717\n",
      "epoch= 662 iteration= 61567 loss= 0.00034546590177342296\n",
      "epoch= 662 iteration= 61587 loss= 0.0013494679005816579\n",
      "epoch= 662 iteration= 61607 loss= 0.0007050047861412168\n",
      "epoch= 662 iteration= 61627 loss= 0.0008672354742884636\n",
      "epoch= 662 iteration= 61647 loss= 0.009320207871496677\n",
      "test_data MSELoss:(pred-real)/real= 0.0018953430375808643\n",
      "epoch= 663 iteration= 61660 loss= 0.00034132442669942975\n",
      "epoch= 663 iteration= 61680 loss= 0.0013450307305902243\n",
      "epoch= 663 iteration= 61700 loss= 0.000704396516084671\n",
      "epoch= 663 iteration= 61720 loss= 0.0008655356359668076\n",
      "epoch= 663 iteration= 61740 loss= 0.009306373074650764\n",
      "test_data MSELoss:(pred-real)/real= 0.001892653020655012\n",
      "epoch= 664 iteration= 61753 loss= 0.0003384089795872569\n",
      "epoch= 664 iteration= 61773 loss= 0.0013410686515271664\n",
      "epoch= 664 iteration= 61793 loss= 0.0007034881273284554\n",
      "epoch= 664 iteration= 61813 loss= 0.0008640398737043142\n",
      "epoch= 664 iteration= 61833 loss= 0.009291538968682289\n",
      "test_data MSELoss:(pred-real)/real= 0.0018905023591489429\n",
      "epoch= 665 iteration= 61846 loss= 0.00033535074908286333\n",
      "epoch= 665 iteration= 61866 loss= 0.0013381835306063294\n",
      "epoch= 665 iteration= 61886 loss= 0.000702203600667417\n",
      "epoch= 665 iteration= 61906 loss= 0.0008624360198155046\n",
      "epoch= 665 iteration= 61926 loss= 0.009275486692786217\n",
      "test_data MSELoss:(pred-real)/real= 0.0018881555248905595\n",
      "epoch= 666 iteration= 61939 loss= 0.0003304751589894295\n",
      "epoch= 666 iteration= 61959 loss= 0.001333711319603026\n",
      "epoch= 666 iteration= 61979 loss= 0.0007017877069301903\n",
      "epoch= 666 iteration= 61999 loss= 0.0008614734397269785\n",
      "epoch= 666 iteration= 62019 loss= 0.009263758547604084\n",
      "test_data MSELoss:(pred-real)/real= 0.001885439348471765\n",
      "epoch= 667 iteration= 62032 loss= 0.00032862904481589794\n",
      "epoch= 667 iteration= 62052 loss= 0.0013286436442285776\n",
      "epoch= 667 iteration= 62072 loss= 0.0007005478255450726\n",
      "epoch= 667 iteration= 62092 loss= 0.0008600899600423872\n",
      "epoch= 667 iteration= 62112 loss= 0.009253717958927155\n",
      "test_data MSELoss:(pred-real)/real= 0.0018828616965846675\n",
      "epoch= 668 iteration= 62125 loss= 0.0003266700077801943\n",
      "epoch= 668 iteration= 62145 loss= 0.001325556542724371\n",
      "epoch= 668 iteration= 62165 loss= 0.0006995966541580856\n",
      "epoch= 668 iteration= 62185 loss= 0.0008589240605942905\n",
      "epoch= 668 iteration= 62205 loss= 0.009233489632606506\n",
      "test_data MSELoss:(pred-real)/real= 0.0018806250267920808\n",
      "epoch= 669 iteration= 62218 loss= 0.00032358255703002214\n",
      "epoch= 669 iteration= 62238 loss= 0.0013213587226346135\n",
      "epoch= 669 iteration= 62258 loss= 0.000698942574672401\n",
      "epoch= 669 iteration= 62278 loss= 0.0008577118860557675\n",
      "epoch= 669 iteration= 62298 loss= 0.009218771010637283\n",
      "test_data MSELoss:(pred-real)/real= 0.0018785028336828367\n",
      "epoch= 670 iteration= 62311 loss= 0.00032112866756506264\n",
      "epoch= 670 iteration= 62331 loss= 0.0013195938663557172\n",
      "epoch= 670 iteration= 62351 loss= 0.0006975748692639172\n",
      "epoch= 670 iteration= 62371 loss= 0.0008555687963962555\n",
      "epoch= 670 iteration= 62391 loss= 0.009201940149068832\n",
      "test_data MSELoss:(pred-real)/real= 0.001875296779972915\n",
      "epoch= 671 iteration= 62404 loss= 0.00031862882315181196\n",
      "epoch= 671 iteration= 62424 loss= 0.0013128805439919233\n",
      "epoch= 671 iteration= 62444 loss= 0.0006964874919503927\n",
      "epoch= 671 iteration= 62464 loss= 0.0008544142474420369\n",
      "epoch= 671 iteration= 62484 loss= 0.009189095348119736\n",
      "test_data MSELoss:(pred-real)/real= 0.0018730517929523354\n",
      "epoch= 672 iteration= 62497 loss= 0.0003159782208967954\n",
      "epoch= 672 iteration= 62517 loss= 0.0013095599133521318\n",
      "epoch= 672 iteration= 62537 loss= 0.0006955101271159947\n",
      "epoch= 672 iteration= 62557 loss= 0.0008532073115929961\n",
      "epoch= 672 iteration= 62577 loss= 0.009176332503557205\n",
      "test_data MSELoss:(pred-real)/real= 0.0018711196607910097\n",
      "epoch= 673 iteration= 62590 loss= 0.00031273928470909595\n",
      "epoch= 673 iteration= 62610 loss= 0.001304403878748417\n",
      "epoch= 673 iteration= 62630 loss= 0.0006934826960787177\n",
      "epoch= 673 iteration= 62650 loss= 0.0008518383256159723\n",
      "epoch= 673 iteration= 62670 loss= 0.009160295128822327\n",
      "test_data MSELoss:(pred-real)/real= 0.0018686586473551062\n",
      "epoch= 674 iteration= 62683 loss= 0.0003093076520599425\n",
      "epoch= 674 iteration= 62703 loss= 0.0012998362071812153\n",
      "epoch= 674 iteration= 62723 loss= 0.0006930370000191033\n",
      "epoch= 674 iteration= 62743 loss= 0.0008510365150868893\n",
      "epoch= 674 iteration= 62763 loss= 0.009145686402916908\n",
      "test_data MSELoss:(pred-real)/real= 0.0018663925826937582\n",
      "epoch= 675 iteration= 62776 loss= 0.000306186790112406\n",
      "epoch= 675 iteration= 62796 loss= 0.001297831884585321\n",
      "epoch= 675 iteration= 62816 loss= 0.000691864057444036\n",
      "epoch= 675 iteration= 62836 loss= 0.0008493188652209938\n",
      "epoch= 675 iteration= 62856 loss= 0.009134660474956036\n",
      "test_data MSELoss:(pred-real)/real= 0.0018640832875260254\n",
      "epoch= 676 iteration= 62869 loss= 0.00030156795401126146\n",
      "epoch= 676 iteration= 62889 loss= 0.0012950178934261203\n",
      "epoch= 676 iteration= 62909 loss= 0.0006910977535881102\n",
      "epoch= 676 iteration= 62929 loss= 0.0008478336967527866\n",
      "epoch= 676 iteration= 62949 loss= 0.009118273854255676\n",
      "test_data MSELoss:(pred-real)/real= 0.0018622346429361238\n",
      "epoch= 677 iteration= 62962 loss= 0.00029902366804890335\n",
      "epoch= 677 iteration= 62982 loss= 0.0012900982983410358\n",
      "epoch= 677 iteration= 63002 loss= 0.0006904697511345148\n",
      "epoch= 677 iteration= 63022 loss= 0.0008466479484923184\n",
      "epoch= 677 iteration= 63042 loss= 0.009101686999201775\n",
      "test_data MSELoss:(pred-real)/real= 0.0018600903438507682\n",
      "epoch= 678 iteration= 63055 loss= 0.00029597902903333306\n",
      "epoch= 678 iteration= 63075 loss= 0.0012841608840972185\n",
      "epoch= 678 iteration= 63095 loss= 0.0006889856304042041\n",
      "epoch= 678 iteration= 63115 loss= 0.0008456709911115468\n",
      "epoch= 678 iteration= 63135 loss= 0.009089712053537369\n",
      "test_data MSELoss:(pred-real)/real= 0.0018572261259477171\n",
      "epoch= 679 iteration= 63148 loss= 0.0002932982170023024\n",
      "epoch= 679 iteration= 63168 loss= 0.0012809259351342916\n",
      "epoch= 679 iteration= 63188 loss= 0.0006881950539536774\n",
      "epoch= 679 iteration= 63208 loss= 0.0008436775533482432\n",
      "epoch= 679 iteration= 63228 loss= 0.00907357968389988\n",
      "test_data MSELoss:(pred-real)/real= 0.0018542231668511198\n",
      "epoch= 680 iteration= 63241 loss= 0.0002911897609010339\n",
      "epoch= 680 iteration= 63261 loss= 0.0012765389401465654\n",
      "epoch= 680 iteration= 63281 loss= 0.0006873142556287348\n",
      "epoch= 680 iteration= 63301 loss= 0.0008425285341218114\n",
      "epoch= 680 iteration= 63321 loss= 0.009059401229023933\n",
      "test_data MSELoss:(pred-real)/real= 0.001853641795201434\n",
      "epoch= 681 iteration= 63334 loss= 0.000286732247332111\n",
      "epoch= 681 iteration= 63354 loss= 0.0012719199294224381\n",
      "epoch= 681 iteration= 63374 loss= 0.0006862195441499352\n",
      "epoch= 681 iteration= 63394 loss= 0.0008410941227339208\n",
      "epoch= 681 iteration= 63414 loss= 0.009045258164405823\n",
      "test_data MSELoss:(pred-real)/real= 0.001850764317269851\n",
      "epoch= 682 iteration= 63427 loss= 0.0002836437779478729\n",
      "epoch= 682 iteration= 63447 loss= 0.0012691132724285126\n",
      "epoch= 682 iteration= 63467 loss= 0.0006848751218058169\n",
      "epoch= 682 iteration= 63487 loss= 0.0008398913778364658\n",
      "epoch= 682 iteration= 63507 loss= 0.009025489911437035\n",
      "test_data MSELoss:(pred-real)/real= 0.0018481385195627809\n",
      "epoch= 683 iteration= 63520 loss= 0.0002802977105602622\n",
      "epoch= 683 iteration= 63540 loss= 0.001264292048290372\n",
      "epoch= 683 iteration= 63560 loss= 0.0006844364106655121\n",
      "epoch= 683 iteration= 63580 loss= 0.0008386647095903754\n",
      "epoch= 683 iteration= 63600 loss= 0.00901247188448906\n",
      "test_data MSELoss:(pred-real)/real= 0.0018457343944141434\n",
      "epoch= 684 iteration= 63613 loss= 0.00027671290445141494\n",
      "epoch= 684 iteration= 63633 loss= 0.0012612021528184414\n",
      "epoch= 684 iteration= 63653 loss= 0.0006837069522589445\n",
      "epoch= 684 iteration= 63673 loss= 0.0008373208693228662\n",
      "epoch= 684 iteration= 63693 loss= 0.008994942530989647\n",
      "test_data MSELoss:(pred-real)/real= 0.0018435430586881314\n",
      "epoch= 685 iteration= 63706 loss= 0.00027161696925759315\n",
      "epoch= 685 iteration= 63726 loss= 0.001255397917702794\n",
      "epoch= 685 iteration= 63746 loss= 0.0006817085668444633\n",
      "epoch= 685 iteration= 63766 loss= 0.0008360056672245264\n",
      "epoch= 685 iteration= 63786 loss= 0.008981763385236263\n",
      "test_data MSELoss:(pred-real)/real= 0.0018419001878808355\n",
      "epoch= 686 iteration= 63799 loss= 0.00026996718952432275\n",
      "epoch= 686 iteration= 63819 loss= 0.0012523740297183394\n",
      "epoch= 686 iteration= 63839 loss= 0.0006804862059652805\n",
      "epoch= 686 iteration= 63859 loss= 0.0008343376684933901\n",
      "epoch= 686 iteration= 63879 loss= 0.00896911509335041\n",
      "test_data MSELoss:(pred-real)/real= 0.0018397383925427373\n",
      "epoch= 687 iteration= 63892 loss= 0.0002676316653378308\n",
      "epoch= 687 iteration= 63912 loss= 0.0012476551346480846\n",
      "epoch= 687 iteration= 63932 loss= 0.0006797394016757607\n",
      "epoch= 687 iteration= 63952 loss= 0.0008334161248058081\n",
      "epoch= 687 iteration= 63972 loss= 0.0089524295181036\n",
      "test_data MSELoss:(pred-real)/real= 0.0018377319825554474\n",
      "epoch= 688 iteration= 63985 loss= 0.0002652288239914924\n",
      "epoch= 688 iteration= 64005 loss= 0.001245478866621852\n",
      "epoch= 688 iteration= 64025 loss= 0.0006780869443900883\n",
      "epoch= 688 iteration= 64045 loss= 0.000832062098197639\n",
      "epoch= 688 iteration= 64065 loss= 0.00893632136285305\n",
      "test_data MSELoss:(pred-real)/real= 0.0018352318632726867\n",
      "epoch= 689 iteration= 64078 loss= 0.0002612869720906019\n",
      "epoch= 689 iteration= 64098 loss= 0.0012415931560099125\n",
      "epoch= 689 iteration= 64118 loss= 0.0006766209844499826\n",
      "epoch= 689 iteration= 64138 loss= 0.0008305434603244066\n",
      "epoch= 689 iteration= 64158 loss= 0.00891774520277977\n",
      "test_data MSELoss:(pred-real)/real= 0.001832791494153854\n",
      "epoch= 690 iteration= 64171 loss= 0.0002579184656497091\n",
      "epoch= 690 iteration= 64191 loss= 0.0012373578501865268\n",
      "epoch= 690 iteration= 64211 loss= 0.0006760722608305514\n",
      "epoch= 690 iteration= 64231 loss= 0.0008286674274131656\n",
      "epoch= 690 iteration= 64251 loss= 0.008905814029276371\n",
      "test_data MSELoss:(pred-real)/real= 0.001831550951465033\n",
      "epoch= 691 iteration= 64264 loss= 0.00025541672948747873\n",
      "epoch= 691 iteration= 64284 loss= 0.0012338126543909311\n",
      "epoch= 691 iteration= 64304 loss= 0.0006746361032128334\n",
      "epoch= 691 iteration= 64324 loss= 0.0008272990817204118\n",
      "epoch= 691 iteration= 64344 loss= 0.008890601806342602\n",
      "test_data MSELoss:(pred-real)/real= 0.0018282235897560087\n",
      "epoch= 692 iteration= 64357 loss= 0.0002512237406335771\n",
      "epoch= 692 iteration= 64377 loss= 0.0012289761798456311\n",
      "epoch= 692 iteration= 64397 loss= 0.0006737338844686747\n",
      "epoch= 692 iteration= 64417 loss= 0.0008268259698525071\n",
      "epoch= 692 iteration= 64437 loss= 0.00887131318449974\n",
      "test_data MSELoss:(pred-real)/real= 0.001827263195688526\n",
      "epoch= 693 iteration= 64450 loss= 0.0002477336092852056\n",
      "epoch= 693 iteration= 64470 loss= 0.0012251294683665037\n",
      "epoch= 693 iteration= 64490 loss= 0.0006726192077621818\n",
      "epoch= 693 iteration= 64510 loss= 0.0008249920792877674\n",
      "epoch= 693 iteration= 64530 loss= 0.008861351758241653\n",
      "test_data MSELoss:(pred-real)/real= 0.0018255679476876846\n",
      "epoch= 694 iteration= 64543 loss= 0.0002429891173960641\n",
      "epoch= 694 iteration= 64563 loss= 0.0012204449158161879\n",
      "epoch= 694 iteration= 64583 loss= 0.0006717845099046826\n",
      "epoch= 694 iteration= 64603 loss= 0.0008232058025896549\n",
      "epoch= 694 iteration= 64623 loss= 0.008842740207910538\n",
      "test_data MSELoss:(pred-real)/real= 0.0018241012779374917\n",
      "epoch= 695 iteration= 64636 loss= 0.0002394178882241249\n",
      "epoch= 695 iteration= 64656 loss= 0.0012168394168838859\n",
      "epoch= 695 iteration= 64676 loss= 0.0006707856082357466\n",
      "epoch= 695 iteration= 64696 loss= 0.0008221757016144693\n",
      "epoch= 695 iteration= 64716 loss= 0.008823687210679054\n",
      "test_data MSELoss:(pred-real)/real= 0.0018210132240912775\n",
      "epoch= 696 iteration= 64729 loss= 0.00023593689547851682\n",
      "epoch= 696 iteration= 64749 loss= 0.001212427276186645\n",
      "epoch= 696 iteration= 64769 loss= 0.0006693341420032084\n",
      "epoch= 696 iteration= 64789 loss= 0.0008205842459574342\n",
      "epoch= 696 iteration= 64809 loss= 0.0088095273822546\n",
      "test_data MSELoss:(pred-real)/real= 0.0018192846457370454\n",
      "epoch= 697 iteration= 64822 loss= 0.0002336915349587798\n",
      "epoch= 697 iteration= 64842 loss= 0.0012075629783794284\n",
      "epoch= 697 iteration= 64862 loss= 0.000669167609885335\n",
      "epoch= 697 iteration= 64882 loss= 0.0008196655544452369\n",
      "epoch= 697 iteration= 64902 loss= 0.008791336789727211\n",
      "test_data MSELoss:(pred-real)/real= 0.0018168997695384962\n",
      "epoch= 698 iteration= 64915 loss= 0.0002304677909705788\n",
      "epoch= 698 iteration= 64935 loss= 0.001205601030960679\n",
      "epoch= 698 iteration= 64955 loss= 0.0006677300552837551\n",
      "epoch= 698 iteration= 64975 loss= 0.0008179296273738146\n",
      "epoch= 698 iteration= 64995 loss= 0.008777322247624397\n",
      "test_data MSELoss:(pred-real)/real= 0.0018155194379182325\n",
      "epoch= 699 iteration= 65008 loss= 0.00022616104979533702\n",
      "epoch= 699 iteration= 65028 loss= 0.0012011289363726974\n",
      "epoch= 699 iteration= 65048 loss= 0.0006665049586445093\n",
      "epoch= 699 iteration= 65068 loss= 0.0008165411418303847\n",
      "epoch= 699 iteration= 65088 loss= 0.008760194294154644\n",
      "test_data MSELoss:(pred-real)/real= 0.0018128417302957838\n",
      "epoch= 700 iteration= 65101 loss= 0.00022232296760194004\n",
      "epoch= 700 iteration= 65121 loss= 0.0011958859395235777\n",
      "epoch= 700 iteration= 65141 loss= 0.000665251340251416\n",
      "epoch= 700 iteration= 65161 loss= 0.0008153141825459898\n",
      "epoch= 700 iteration= 65181 loss= 0.00874271709471941\n",
      "test_data MSELoss:(pred-real)/real= 0.0018111639947164804\n",
      "epoch= 701 iteration= 65194 loss= 0.0002194409753428772\n",
      "epoch= 701 iteration= 65214 loss= 0.001192357623949647\n",
      "epoch= 701 iteration= 65234 loss= 0.0006647196132689714\n",
      "epoch= 701 iteration= 65254 loss= 0.00081438577035442\n",
      "epoch= 701 iteration= 65274 loss= 0.008727889508008957\n",
      "test_data MSELoss:(pred-real)/real= 0.0018088707761813162\n",
      "epoch= 702 iteration= 65287 loss= 0.00021543119510170072\n",
      "epoch= 702 iteration= 65307 loss= 0.0011870174203068018\n",
      "epoch= 702 iteration= 65327 loss= 0.0006634175078943372\n",
      "epoch= 702 iteration= 65347 loss= 0.0008127704495564103\n",
      "epoch= 702 iteration= 65367 loss= 0.00870948750525713\n",
      "test_data MSELoss:(pred-real)/real= 0.001808222279780441\n",
      "epoch= 703 iteration= 65380 loss= 0.00021220087364781648\n",
      "epoch= 703 iteration= 65400 loss= 0.0011829520808532834\n",
      "epoch= 703 iteration= 65420 loss= 0.0006621161010116339\n",
      "epoch= 703 iteration= 65440 loss= 0.0008118060068227351\n",
      "epoch= 703 iteration= 65460 loss= 0.008693566545844078\n",
      "test_data MSELoss:(pred-real)/real= 0.0018049810282213406\n",
      "epoch= 704 iteration= 65473 loss= 0.0002078248799080029\n",
      "epoch= 704 iteration= 65493 loss= 0.0011781684588640928\n",
      "epoch= 704 iteration= 65513 loss= 0.000661744736135006\n",
      "epoch= 704 iteration= 65533 loss= 0.0008100513950921595\n",
      "epoch= 704 iteration= 65553 loss= 0.008675583638250828\n",
      "test_data MSELoss:(pred-real)/real= 0.0018027835515871022\n",
      "epoch= 705 iteration= 65566 loss= 0.00020520131511148065\n",
      "epoch= 705 iteration= 65586 loss= 0.0011768543627113104\n",
      "epoch= 705 iteration= 65606 loss= 0.0006603380898013711\n",
      "epoch= 705 iteration= 65626 loss= 0.0008084556320682168\n",
      "epoch= 705 iteration= 65646 loss= 0.008654155768454075\n",
      "test_data MSELoss:(pred-real)/real= 0.0018006415258342815\n",
      "epoch= 706 iteration= 65659 loss= 0.00020054937340319157\n",
      "epoch= 706 iteration= 65679 loss= 0.0011735631851479411\n",
      "epoch= 706 iteration= 65699 loss= 0.0006582454079762101\n",
      "epoch= 706 iteration= 65719 loss= 0.0008068748866207898\n",
      "epoch= 706 iteration= 65739 loss= 0.008638217113912106\n",
      "test_data MSELoss:(pred-real)/real= 0.0017999861916501282\n",
      "epoch= 707 iteration= 65752 loss= 0.00019800456357188523\n",
      "epoch= 707 iteration= 65772 loss= 0.0011684749042615294\n",
      "epoch= 707 iteration= 65792 loss= 0.0006573736318387091\n",
      "epoch= 707 iteration= 65812 loss= 0.0008062077686190605\n",
      "epoch= 707 iteration= 65832 loss= 0.008623681962490082\n",
      "test_data MSELoss:(pred-real)/real= 0.0017980334620612364\n",
      "epoch= 708 iteration= 65845 loss= 0.00019314377277623862\n",
      "epoch= 708 iteration= 65865 loss= 0.0011626204941421747\n",
      "epoch= 708 iteration= 65885 loss= 0.0006570157129317522\n",
      "epoch= 708 iteration= 65905 loss= 0.0008047752780839801\n",
      "epoch= 708 iteration= 65925 loss= 0.008601265959441662\n",
      "test_data MSELoss:(pred-real)/real= 0.0017957453075925717\n",
      "epoch= 709 iteration= 65938 loss= 0.0001900219067465514\n",
      "epoch= 709 iteration= 65958 loss= 0.0011600771686062217\n",
      "epoch= 709 iteration= 65978 loss= 0.0006555451545864344\n",
      "epoch= 709 iteration= 65998 loss= 0.0008034759666770697\n",
      "epoch= 709 iteration= 66018 loss= 0.008584552444517612\n",
      "test_data MSELoss:(pred-real)/real= 0.0017935260402737185\n",
      "epoch= 710 iteration= 66031 loss= 0.000186377321369946\n",
      "epoch= 710 iteration= 66051 loss= 0.001156155369244516\n",
      "epoch= 710 iteration= 66071 loss= 0.000654868024867028\n",
      "epoch= 710 iteration= 66091 loss= 0.0008018019143491983\n",
      "epoch= 710 iteration= 66111 loss= 0.008564524352550507\n",
      "test_data MSELoss:(pred-real)/real= 0.0017928797169588506\n",
      "epoch= 711 iteration= 66124 loss= 0.00018268519488628954\n",
      "epoch= 711 iteration= 66144 loss= 0.001151830074377358\n",
      "epoch= 711 iteration= 66164 loss= 0.0006538117886520922\n",
      "epoch= 711 iteration= 66184 loss= 0.0008007038850337267\n",
      "epoch= 711 iteration= 66204 loss= 0.00855189748108387\n",
      "test_data MSELoss:(pred-real)/real= 0.0017899268342363131\n",
      "epoch= 712 iteration= 66217 loss= 0.00017906315042637289\n",
      "epoch= 712 iteration= 66237 loss= 0.0011491857003420591\n",
      "epoch= 712 iteration= 66257 loss= 0.0006519051967188716\n",
      "epoch= 712 iteration= 66277 loss= 0.0007992363534867764\n",
      "epoch= 712 iteration= 66297 loss= 0.008528061211109161\n",
      "test_data MSELoss:(pred-real)/real= 0.0017885472108092573\n",
      "epoch= 713 iteration= 66310 loss= 0.00017575079982634634\n",
      "epoch= 713 iteration= 66330 loss= 0.0011432600440457463\n",
      "epoch= 713 iteration= 66350 loss= 0.0006510053062811494\n",
      "epoch= 713 iteration= 66370 loss= 0.000797581800725311\n",
      "epoch= 713 iteration= 66390 loss= 0.008510688319802284\n",
      "test_data MSELoss:(pred-real)/real= 0.0017861911037471145\n",
      "epoch= 714 iteration= 66403 loss= 0.00017274467973038554\n",
      "epoch= 714 iteration= 66423 loss= 0.0011404246324673295\n",
      "epoch= 714 iteration= 66443 loss= 0.000649726833216846\n",
      "epoch= 714 iteration= 66463 loss= 0.0007963345851749182\n",
      "epoch= 714 iteration= 66483 loss= 0.008492856286466122\n",
      "test_data MSELoss:(pred-real)/real= 0.0017825404421374616\n",
      "epoch= 715 iteration= 66496 loss= 0.00016960903303697705\n",
      "epoch= 715 iteration= 66516 loss= 0.0011370006250217557\n",
      "epoch= 715 iteration= 66536 loss= 0.0006486080237664282\n",
      "epoch= 715 iteration= 66556 loss= 0.0007955862674862146\n",
      "epoch= 715 iteration= 66576 loss= 0.008469613268971443\n",
      "test_data MSELoss:(pred-real)/real= 0.0017815082667059163\n",
      "epoch= 716 iteration= 66589 loss= 0.00016669729666318744\n",
      "epoch= 716 iteration= 66609 loss= 0.0011328218970447779\n",
      "epoch= 716 iteration= 66629 loss= 0.000647472683340311\n",
      "epoch= 716 iteration= 66649 loss= 0.0007938673952594399\n",
      "epoch= 716 iteration= 66669 loss= 0.00845220498740673\n",
      "test_data MSELoss:(pred-real)/real= 0.0017794480663724244\n",
      "epoch= 717 iteration= 66682 loss= 0.00016314104141201824\n",
      "epoch= 717 iteration= 66702 loss= 0.0011288509704172611\n",
      "epoch= 717 iteration= 66722 loss= 0.0006465251790359616\n",
      "epoch= 717 iteration= 66742 loss= 0.0007926105754449964\n",
      "epoch= 717 iteration= 66762 loss= 0.008433842100203037\n",
      "test_data MSELoss:(pred-real)/real= 0.0017783708236594168\n",
      "epoch= 718 iteration= 66775 loss= 0.00015940144658088684\n",
      "epoch= 718 iteration= 66795 loss= 0.0011255397694185376\n",
      "epoch= 718 iteration= 66815 loss= 0.0006455563707277179\n",
      "epoch= 718 iteration= 66835 loss= 0.0007916053291410208\n",
      "epoch= 718 iteration= 66855 loss= 0.008412188850343227\n",
      "test_data MSELoss:(pred-real)/real= 0.0017762205211005898\n",
      "epoch= 719 iteration= 66868 loss= 0.0001557703799335286\n",
      "epoch= 719 iteration= 66888 loss= 0.0011202775640413165\n",
      "epoch= 719 iteration= 66908 loss= 0.0006442720768973231\n",
      "epoch= 719 iteration= 66928 loss= 0.000789559562690556\n",
      "epoch= 719 iteration= 66948 loss= 0.008393499068915844\n",
      "test_data MSELoss:(pred-real)/real= 0.0017739769051938008\n",
      "epoch= 720 iteration= 66961 loss= 0.00015355100913438946\n",
      "epoch= 720 iteration= 66981 loss= 0.0011148923076689243\n",
      "epoch= 720 iteration= 67001 loss= 0.0006434356910176575\n",
      "epoch= 720 iteration= 67021 loss= 0.0007881136843934655\n",
      "epoch= 720 iteration= 67041 loss= 0.008372772485017776\n",
      "test_data MSELoss:(pred-real)/real= 0.0017725163472480038\n",
      "epoch= 721 iteration= 67054 loss= 0.00015008231275714934\n",
      "epoch= 721 iteration= 67074 loss= 0.0011121387360617518\n",
      "epoch= 721 iteration= 67094 loss= 0.0006422075675800443\n",
      "epoch= 721 iteration= 67114 loss= 0.0007868096581660211\n",
      "epoch= 721 iteration= 67134 loss= 0.008353671990334988\n",
      "test_data MSELoss:(pred-real)/real= 0.0017700021643476146\n",
      "epoch= 722 iteration= 67147 loss= 0.00014723624917678535\n",
      "epoch= 722 iteration= 67167 loss= 0.0011085758451372385\n",
      "epoch= 722 iteration= 67187 loss= 0.000641393184196204\n",
      "epoch= 722 iteration= 67207 loss= 0.0007862488855607808\n",
      "epoch= 722 iteration= 67227 loss= 0.008331017568707466\n",
      "test_data MSELoss:(pred-real)/real= 0.0017671481214670672\n",
      "epoch= 723 iteration= 67240 loss= 0.00014354327868204564\n",
      "epoch= 723 iteration= 67260 loss= 0.001104022259823978\n",
      "epoch= 723 iteration= 67280 loss= 0.0006396524840965867\n",
      "epoch= 723 iteration= 67300 loss= 0.0007851027185097337\n",
      "epoch= 723 iteration= 67320 loss= 0.008306476287543774\n",
      "test_data MSELoss:(pred-real)/real= 0.001765701635223296\n",
      "epoch= 724 iteration= 67333 loss= 0.0001403792412020266\n",
      "epoch= 724 iteration= 67353 loss= 0.0011010018642991781\n",
      "epoch= 724 iteration= 67373 loss= 0.0006382490391843021\n",
      "epoch= 724 iteration= 67393 loss= 0.0007831942639313638\n",
      "epoch= 724 iteration= 67413 loss= 0.00828936044126749\n",
      "test_data MSELoss:(pred-real)/real= 0.0017631837884740282\n",
      "epoch= 725 iteration= 67426 loss= 0.0001375591818941757\n",
      "epoch= 725 iteration= 67446 loss= 0.0010953390737995505\n",
      "epoch= 725 iteration= 67466 loss= 0.0006372410571202636\n",
      "epoch= 725 iteration= 67486 loss= 0.000781369220931083\n",
      "epoch= 725 iteration= 67506 loss= 0.008270861580967903\n",
      "test_data MSELoss:(pred-real)/real= 0.0017594267184095872\n",
      "epoch= 726 iteration= 67519 loss= 0.00013487454270944\n",
      "epoch= 726 iteration= 67539 loss= 0.0010918902698904276\n",
      "epoch= 726 iteration= 67559 loss= 0.0006363458232954144\n",
      "epoch= 726 iteration= 67579 loss= 0.0007803600165061653\n",
      "epoch= 726 iteration= 67599 loss= 0.008248995989561081\n",
      "test_data MSELoss:(pred-real)/real= 0.0017582269453365977\n",
      "epoch= 727 iteration= 67612 loss= 0.00013133596803527325\n",
      "epoch= 727 iteration= 67632 loss= 0.0010890071280300617\n",
      "epoch= 727 iteration= 67652 loss= 0.0006346742738969624\n",
      "epoch= 727 iteration= 67672 loss= 0.0007794908597134054\n",
      "epoch= 727 iteration= 67692 loss= 0.00822484865784645\n",
      "test_data MSELoss:(pred-real)/real= 0.001754944954882376\n",
      "epoch= 728 iteration= 67705 loss= 0.00012903149763587862\n",
      "epoch= 728 iteration= 67725 loss= 0.001083053881302476\n",
      "epoch= 728 iteration= 67745 loss= 0.0006343122804537416\n",
      "epoch= 728 iteration= 67765 loss= 0.0007778063882142305\n",
      "epoch= 728 iteration= 67785 loss= 0.008203641511499882\n",
      "test_data MSELoss:(pred-real)/real= 0.0017538499555990307\n",
      "epoch= 729 iteration= 67798 loss= 0.0001260283461306244\n",
      "epoch= 729 iteration= 67818 loss= 0.001079389126971364\n",
      "epoch= 729 iteration= 67838 loss= 0.0006336895748972893\n",
      "epoch= 729 iteration= 67858 loss= 0.0007770108641125262\n",
      "epoch= 729 iteration= 67878 loss= 0.008181742392480373\n",
      "test_data MSELoss:(pred-real)/real= 0.0017506112894302027\n",
      "epoch= 730 iteration= 67891 loss= 0.00012286897981539369\n",
      "epoch= 730 iteration= 67911 loss= 0.0010767070343717933\n",
      "epoch= 730 iteration= 67931 loss= 0.0006321342661976814\n",
      "epoch= 730 iteration= 67951 loss= 0.0007755541009828448\n",
      "epoch= 730 iteration= 67971 loss= 0.008160288445651531\n",
      "test_data MSELoss:(pred-real)/real= 0.001747432957118791\n",
      "epoch= 731 iteration= 67984 loss= 0.00012004875316051766\n",
      "epoch= 731 iteration= 68004 loss= 0.0010718099074438214\n",
      "epoch= 731 iteration= 68024 loss= 0.0006307788426056504\n",
      "epoch= 731 iteration= 68044 loss= 0.0007743609603494406\n",
      "epoch= 731 iteration= 68064 loss= 0.008137183263897896\n",
      "test_data MSELoss:(pred-real)/real= 0.0017449187370301741\n",
      "epoch= 732 iteration= 68077 loss= 0.00011748418910428882\n",
      "epoch= 732 iteration= 68097 loss= 0.001066271448507905\n",
      "epoch= 732 iteration= 68117 loss= 0.0006299266824498773\n",
      "epoch= 732 iteration= 68137 loss= 0.0007729621138423681\n",
      "epoch= 732 iteration= 68157 loss= 0.008111486211419106\n",
      "test_data MSELoss:(pred-real)/real= 0.001743987047423919\n",
      "epoch= 733 iteration= 68170 loss= 0.00011426775017753243\n",
      "epoch= 733 iteration= 68190 loss= 0.0010624012211337686\n",
      "epoch= 733 iteration= 68210 loss= 0.0006288236472755671\n",
      "epoch= 733 iteration= 68230 loss= 0.0007715711835771799\n",
      "epoch= 733 iteration= 68250 loss= 0.008085762150585651\n",
      "test_data MSELoss:(pred-real)/real= 0.0017411622522761011\n",
      "epoch= 734 iteration= 68263 loss= 0.00011215711128897965\n",
      "epoch= 734 iteration= 68283 loss= 0.001058883499354124\n",
      "epoch= 734 iteration= 68303 loss= 0.0006271835300140083\n",
      "epoch= 734 iteration= 68323 loss= 0.0007704412564635277\n",
      "epoch= 734 iteration= 68343 loss= 0.008066120557487011\n",
      "test_data MSELoss:(pred-real)/real= 0.0017374016816676077\n",
      "epoch= 735 iteration= 68356 loss= 0.00011003936378983781\n",
      "epoch= 735 iteration= 68376 loss= 0.001055269269272685\n",
      "epoch= 735 iteration= 68396 loss= 0.0006263378309085965\n",
      "epoch= 735 iteration= 68416 loss= 0.0007693513762205839\n",
      "epoch= 735 iteration= 68436 loss= 0.00804482214152813\n",
      "test_data MSELoss:(pred-real)/real= 0.0017338651280927782\n",
      "epoch= 736 iteration= 68449 loss= 0.00010776473209261894\n",
      "epoch= 736 iteration= 68469 loss= 0.0010520641226321459\n",
      "epoch= 736 iteration= 68489 loss= 0.0006249547586776316\n",
      "epoch= 736 iteration= 68509 loss= 0.0007681985734961927\n",
      "epoch= 736 iteration= 68529 loss= 0.00801560003310442\n",
      "test_data MSELoss:(pred-real)/real= 0.0017324903359015782\n",
      "epoch= 737 iteration= 68542 loss= 0.00010590750025585294\n",
      "epoch= 737 iteration= 68562 loss= 0.0010478314943611622\n",
      "epoch= 737 iteration= 68582 loss= 0.0006232628948055208\n",
      "epoch= 737 iteration= 68602 loss= 0.000766682846006006\n",
      "epoch= 737 iteration= 68622 loss= 0.007993709295988083\n",
      "test_data MSELoss:(pred-real)/real= 0.0017291576239383882\n",
      "epoch= 738 iteration= 68635 loss= 0.00010414455027785152\n",
      "epoch= 738 iteration= 68655 loss= 0.0010445164516568184\n",
      "epoch= 738 iteration= 68675 loss= 0.0006215341854840517\n",
      "epoch= 738 iteration= 68695 loss= 0.0007655899971723557\n",
      "epoch= 738 iteration= 68715 loss= 0.007965723983943462\n",
      "test_data MSELoss:(pred-real)/real= 0.0017269875550280428\n",
      "epoch= 739 iteration= 68728 loss= 0.00010240788105875254\n",
      "epoch= 739 iteration= 68748 loss= 0.0010408189846202731\n",
      "epoch= 739 iteration= 68768 loss= 0.0006205624667927623\n",
      "epoch= 739 iteration= 68788 loss= 0.0007643083808943629\n",
      "epoch= 739 iteration= 68808 loss= 0.007944191806018353\n",
      "test_data MSELoss:(pred-real)/real= 0.0017223432660102844\n",
      "epoch= 740 iteration= 68821 loss= 0.00010060773638542742\n",
      "epoch= 740 iteration= 68841 loss= 0.0010382552864030004\n",
      "epoch= 740 iteration= 68861 loss= 0.0006197954062372446\n",
      "epoch= 740 iteration= 68881 loss= 0.0007632940541952848\n",
      "epoch= 740 iteration= 68901 loss= 0.007919254712760448\n",
      "test_data MSELoss:(pred-real)/real= 0.0017183024384495285\n",
      "epoch= 741 iteration= 68914 loss= 9.903790487442166e-05\n",
      "epoch= 741 iteration= 68934 loss= 0.001034254557453096\n",
      "epoch= 741 iteration= 68954 loss= 0.000618528516497463\n",
      "epoch= 741 iteration= 68974 loss= 0.000762191426474601\n",
      "epoch= 741 iteration= 68994 loss= 0.007896924391388893\n",
      "test_data MSELoss:(pred-real)/real= 0.0017159813594642198\n",
      "epoch= 742 iteration= 69007 loss= 9.764888091012836e-05\n",
      "epoch= 742 iteration= 69027 loss= 0.0010306176263839006\n",
      "epoch= 742 iteration= 69047 loss= 0.0006174633745104074\n",
      "epoch= 742 iteration= 69067 loss= 0.0007609677850268781\n",
      "epoch= 742 iteration= 69087 loss= 0.007868475280702114\n",
      "test_data MSELoss:(pred-real)/real= 0.0017123322080199916\n",
      "epoch= 743 iteration= 69100 loss= 9.592760034138337e-05\n",
      "epoch= 743 iteration= 69120 loss= 0.001024599769152701\n",
      "epoch= 743 iteration= 69140 loss= 0.000616709585301578\n",
      "epoch= 743 iteration= 69160 loss= 0.0007598953088745475\n",
      "epoch= 743 iteration= 69180 loss= 0.0078448336571455\n",
      "test_data MSELoss:(pred-real)/real= 0.0017084057746817255\n",
      "epoch= 744 iteration= 69193 loss= 9.45424908422865e-05\n",
      "epoch= 744 iteration= 69213 loss= 0.001021701144054532\n",
      "epoch= 744 iteration= 69233 loss= 0.000614605494774878\n",
      "epoch= 744 iteration= 69253 loss= 0.0007590404129587114\n",
      "epoch= 744 iteration= 69273 loss= 0.007815847173333168\n",
      "test_data MSELoss:(pred-real)/real= 0.0017038895910243606\n",
      "epoch= 745 iteration= 69286 loss= 9.297934593632817e-05\n",
      "epoch= 745 iteration= 69306 loss= 0.0010191472247242928\n",
      "epoch= 745 iteration= 69326 loss= 0.0006133548449724913\n",
      "epoch= 745 iteration= 69346 loss= 0.0007575339986942708\n",
      "epoch= 745 iteration= 69366 loss= 0.007790568750351667\n",
      "test_data MSELoss:(pred-real)/real= 0.0017005561029590252\n",
      "epoch= 746 iteration= 69379 loss= 9.215524914907292e-05\n",
      "epoch= 746 iteration= 69399 loss= 0.0010156307835131884\n",
      "epoch= 746 iteration= 69419 loss= 0.0006124044302850962\n",
      "epoch= 746 iteration= 69439 loss= 0.0007563779363408685\n",
      "epoch= 746 iteration= 69459 loss= 0.007761224173009396\n",
      "test_data MSELoss:(pred-real)/real= 0.001698074155784626\n",
      "epoch= 747 iteration= 69472 loss= 9.105569188250229e-05\n",
      "epoch= 747 iteration= 69492 loss= 0.0010122095700353384\n",
      "epoch= 747 iteration= 69512 loss= 0.0006112964474596083\n",
      "epoch= 747 iteration= 69532 loss= 0.0007551711751148105\n",
      "epoch= 747 iteration= 69552 loss= 0.007734545972198248\n",
      "test_data MSELoss:(pred-real)/real= 0.0016937101592904786\n",
      "epoch= 748 iteration= 69565 loss= 9.005088213598356e-05\n",
      "epoch= 748 iteration= 69585 loss= 0.0010084735695272684\n",
      "epoch= 748 iteration= 69605 loss= 0.0006100707105360925\n",
      "epoch= 748 iteration= 69625 loss= 0.0007544630207121372\n",
      "epoch= 748 iteration= 69645 loss= 0.007709289435297251\n",
      "test_data MSELoss:(pred-real)/real= 0.001689529774012044\n",
      "epoch= 749 iteration= 69658 loss= 8.939147664932534e-05\n",
      "epoch= 749 iteration= 69678 loss= 0.0010035868035629392\n",
      "epoch= 749 iteration= 69698 loss= 0.0006088289665058255\n",
      "epoch= 749 iteration= 69718 loss= 0.0007531419396400452\n",
      "epoch= 749 iteration= 69738 loss= 0.007681362330913544\n",
      "test_data MSELoss:(pred-real)/real= 0.0016843595464403431\n",
      "epoch= 750 iteration= 69751 loss= 8.862746472004801e-05\n",
      "epoch= 750 iteration= 69771 loss= 0.0009994334541261196\n",
      "epoch= 750 iteration= 69791 loss= 0.0006068821530789137\n",
      "epoch= 750 iteration= 69811 loss= 0.0007523184176534414\n",
      "epoch= 750 iteration= 69831 loss= 0.007652244996279478\n",
      "test_data MSELoss:(pred-real)/real= 0.0016812986594029805\n",
      "epoch= 751 iteration= 69844 loss= 8.812359010335058e-05\n",
      "epoch= 751 iteration= 69864 loss= 0.0009963978081941605\n",
      "epoch= 751 iteration= 69884 loss= 0.0006065776105970144\n",
      "epoch= 751 iteration= 69904 loss= 0.0007509209681302309\n",
      "epoch= 751 iteration= 69924 loss= 0.007623331621289253\n",
      "test_data MSELoss:(pred-real)/real= 0.0016756847633385202\n",
      "epoch= 752 iteration= 69937 loss= 8.767359395278618e-05\n",
      "epoch= 752 iteration= 69957 loss= 0.0009928044164553285\n",
      "epoch= 752 iteration= 69977 loss= 0.0006051827222108841\n",
      "epoch= 752 iteration= 69997 loss= 0.000750555656850338\n",
      "epoch= 752 iteration= 70017 loss= 0.007594726048409939\n",
      "test_data MSELoss:(pred-real)/real= 0.0016721264077609198\n",
      "epoch= 753 iteration= 70030 loss= 8.75945552252233e-05\n",
      "epoch= 753 iteration= 70050 loss= 0.000989927677437663\n",
      "epoch= 753 iteration= 70070 loss= 0.00060383410891518\n",
      "epoch= 753 iteration= 70090 loss= 0.0007489949930459261\n",
      "epoch= 753 iteration= 70110 loss= 0.007565273903310299\n",
      "test_data MSELoss:(pred-real)/real= 0.0016671908912636961\n",
      "epoch= 754 iteration= 70123 loss= 8.746114326640964e-05\n",
      "epoch= 754 iteration= 70143 loss= 0.000986594008281827\n",
      "epoch= 754 iteration= 70163 loss= 0.0006025347393006086\n",
      "epoch= 754 iteration= 70183 loss= 0.0007488303817808628\n",
      "epoch= 754 iteration= 70203 loss= 0.007535766810178757\n",
      "test_data MSELoss:(pred-real)/real= 0.0016624409682764155\n",
      "epoch= 755 iteration= 70216 loss= 8.76942795002833e-05\n",
      "epoch= 755 iteration= 70236 loss= 0.000982079771347344\n",
      "epoch= 755 iteration= 70256 loss= 0.0006010233191773295\n",
      "epoch= 755 iteration= 70276 loss= 0.0007473294390365481\n",
      "epoch= 755 iteration= 70296 loss= 0.007506680674850941\n",
      "test_data MSELoss:(pred-real)/real= 0.001656871780546175\n",
      "epoch= 756 iteration= 70309 loss= 8.818387141218409e-05\n",
      "epoch= 756 iteration= 70329 loss= 0.0009782242123037577\n",
      "epoch= 756 iteration= 70349 loss= 0.0005992879159748554\n",
      "epoch= 756 iteration= 70369 loss= 0.0007463883957825601\n",
      "epoch= 756 iteration= 70389 loss= 0.007474229205399752\n",
      "test_data MSELoss:(pred-real)/real= 0.0016519126155698258\n",
      "epoch= 757 iteration= 70402 loss= 8.863630500854924e-05\n",
      "epoch= 757 iteration= 70422 loss= 0.000974557246081531\n",
      "epoch= 757 iteration= 70442 loss= 0.0005990295903757215\n",
      "epoch= 757 iteration= 70462 loss= 0.0007451450219377875\n",
      "epoch= 757 iteration= 70482 loss= 0.0074473414570093155\n",
      "test_data MSELoss:(pred-real)/real= 0.001646792221840264\n",
      "epoch= 758 iteration= 70495 loss= 8.925763540901244e-05\n",
      "epoch= 758 iteration= 70515 loss= 0.0009707877179607749\n",
      "epoch= 758 iteration= 70535 loss= 0.0005975032690912485\n",
      "epoch= 758 iteration= 70555 loss= 0.0007442571222782135\n",
      "epoch= 758 iteration= 70575 loss= 0.00741681270301342\n",
      "test_data MSELoss:(pred-real)/real= 0.0016409485963069731\n",
      "epoch= 759 iteration= 70588 loss= 8.998215344035998e-05\n",
      "epoch= 759 iteration= 70608 loss= 0.0009674411849118769\n",
      "epoch= 759 iteration= 70628 loss= 0.0005966651951894164\n",
      "epoch= 759 iteration= 70648 loss= 0.00074312201468274\n",
      "epoch= 759 iteration= 70668 loss= 0.007381299510598183\n",
      "test_data MSELoss:(pred-real)/real= 0.0016356163250748068\n",
      "epoch= 760 iteration= 70681 loss= 9.109507664106786e-05\n",
      "epoch= 760 iteration= 70701 loss= 0.0009641548385843635\n",
      "epoch= 760 iteration= 70721 loss= 0.0005952950450591743\n",
      "epoch= 760 iteration= 70741 loss= 0.0007423281203955412\n",
      "epoch= 760 iteration= 70761 loss= 0.007349786348640919\n",
      "test_data MSELoss:(pred-real)/real= 0.0016297426191158593\n",
      "epoch= 761 iteration= 70774 loss= 9.203522495226935e-05\n",
      "epoch= 761 iteration= 70794 loss= 0.0009604215156286955\n",
      "epoch= 761 iteration= 70814 loss= 0.0005937303649261594\n",
      "epoch= 761 iteration= 70834 loss= 0.0007412536069750786\n",
      "epoch= 761 iteration= 70854 loss= 0.0073231784626841545\n",
      "test_data MSELoss:(pred-real)/real= 0.0016240642758526115\n",
      "epoch= 762 iteration= 70867 loss= 9.300508827436715e-05\n",
      "epoch= 762 iteration= 70887 loss= 0.0009561493061482906\n",
      "epoch= 762 iteration= 70907 loss= 0.0005928154569119215\n",
      "epoch= 762 iteration= 70927 loss= 0.0007404182106256485\n",
      "epoch= 762 iteration= 70947 loss= 0.007289452478289604\n",
      "test_data MSELoss:(pred-real)/real= 0.0016178523235592162\n",
      "epoch= 763 iteration= 70960 loss= 9.451717778574675e-05\n",
      "epoch= 763 iteration= 70980 loss= 0.00095069978851825\n",
      "epoch= 763 iteration= 71000 loss= 0.0005919575924053788\n",
      "epoch= 763 iteration= 71020 loss= 0.0007393270498141646\n",
      "epoch= 763 iteration= 71040 loss= 0.007259478326886892\n",
      "test_data MSELoss:(pred-real)/real= 0.0016122072896299262\n",
      "epoch= 764 iteration= 71053 loss= 9.627659164834768e-05\n",
      "epoch= 764 iteration= 71073 loss= 0.0009486460476182401\n",
      "epoch= 764 iteration= 71093 loss= 0.0005907650338485837\n",
      "epoch= 764 iteration= 71113 loss= 0.0007388592348434031\n",
      "epoch= 764 iteration= 71133 loss= 0.007230749353766441\n",
      "test_data MSELoss:(pred-real)/real= 0.0016059883216965115\n",
      "epoch= 765 iteration= 71146 loss= 9.823054278967902e-05\n",
      "epoch= 765 iteration= 71166 loss= 0.0009441344300284982\n",
      "epoch= 765 iteration= 71186 loss= 0.0005900587420910597\n",
      "epoch= 765 iteration= 71206 loss= 0.0007379392045550048\n",
      "epoch= 765 iteration= 71226 loss= 0.007198436185717583\n",
      "test_data MSELoss:(pred-real)/real= 0.0016001843776191687\n",
      "epoch= 766 iteration= 71239 loss= 0.00010029687837231904\n",
      "epoch= 766 iteration= 71259 loss= 0.0009413697989657521\n",
      "epoch= 766 iteration= 71279 loss= 0.0005882836412638426\n",
      "epoch= 766 iteration= 71299 loss= 0.0007371506653726101\n",
      "epoch= 766 iteration= 71319 loss= 0.007166263181716204\n",
      "test_data MSELoss:(pred-real)/real= 0.0015937528256068213\n",
      "epoch= 767 iteration= 71332 loss= 0.00010261889838147908\n",
      "epoch= 767 iteration= 71352 loss= 0.0009384046425111592\n",
      "epoch= 767 iteration= 71372 loss= 0.0005871664616279304\n",
      "epoch= 767 iteration= 71392 loss= 0.0007360957097262144\n",
      "epoch= 767 iteration= 71412 loss= 0.007130901794880629\n",
      "test_data MSELoss:(pred-real)/real= 0.0015860227399065883\n",
      "epoch= 768 iteration= 71425 loss= 0.00010400160681456327\n",
      "epoch= 768 iteration= 71445 loss= 0.0009350564796477556\n",
      "epoch= 768 iteration= 71465 loss= 0.000585631700232625\n",
      "epoch= 768 iteration= 71485 loss= 0.0007354009430855513\n",
      "epoch= 768 iteration= 71505 loss= 0.007100670598447323\n",
      "test_data MSELoss:(pred-real)/real= 0.0015795148728001448\n",
      "epoch= 769 iteration= 71518 loss= 0.00010609431774355471\n",
      "epoch= 769 iteration= 71538 loss= 0.000933770090341568\n",
      "epoch= 769 iteration= 71558 loss= 0.0005841319216415286\n",
      "epoch= 769 iteration= 71578 loss= 0.0007344256155192852\n",
      "epoch= 769 iteration= 71598 loss= 0.007068927399814129\n",
      "test_data MSELoss:(pred-real)/real= 0.0015728951790758099\n",
      "epoch= 770 iteration= 71611 loss= 0.00010859653411898762\n",
      "epoch= 770 iteration= 71631 loss= 0.0009306519059464335\n",
      "epoch= 770 iteration= 71651 loss= 0.0005826725391671062\n",
      "epoch= 770 iteration= 71671 loss= 0.0007335193804465234\n",
      "epoch= 770 iteration= 71691 loss= 0.007040715776383877\n",
      "test_data MSELoss:(pred-real)/real= 0.0015648257427124514\n",
      "epoch= 771 iteration= 71704 loss= 0.00011086590529885143\n",
      "epoch= 771 iteration= 71724 loss= 0.0009272035676985979\n",
      "epoch= 771 iteration= 71744 loss= 0.0005817353958263993\n",
      "epoch= 771 iteration= 71764 loss= 0.0007322381716221571\n",
      "epoch= 771 iteration= 71784 loss= 0.0070046307519078255\n",
      "test_data MSELoss:(pred-real)/real= 0.0015578415833361861\n",
      "epoch= 772 iteration= 71797 loss= 0.00011390748841222376\n",
      "epoch= 772 iteration= 71817 loss= 0.0009230600553564727\n",
      "epoch= 772 iteration= 71837 loss= 0.0005802807863801718\n",
      "epoch= 772 iteration= 71857 loss= 0.0007314926479011774\n",
      "epoch= 772 iteration= 71877 loss= 0.006975085474550724\n",
      "test_data MSELoss:(pred-real)/real= 0.0015497822831902239\n",
      "epoch= 773 iteration= 71890 loss= 0.00011532268399605528\n",
      "epoch= 773 iteration= 71910 loss= 0.0009207943803630769\n",
      "epoch= 773 iteration= 71930 loss= 0.0005796184996142983\n",
      "epoch= 773 iteration= 71950 loss= 0.0007303967140614986\n",
      "epoch= 773 iteration= 71970 loss= 0.006941598374396563\n",
      "test_data MSELoss:(pred-real)/real= 0.0015420281755117078\n",
      "epoch= 774 iteration= 71983 loss= 0.00011752007412724197\n",
      "epoch= 774 iteration= 72003 loss= 0.0009170082048512995\n",
      "epoch= 774 iteration= 72023 loss= 0.0005780863575637341\n",
      "epoch= 774 iteration= 72043 loss= 0.0007287274929694831\n",
      "epoch= 774 iteration= 72063 loss= 0.006909474730491638\n",
      "test_data MSELoss:(pred-real)/real= 0.001533903880044818\n",
      "epoch= 775 iteration= 72076 loss= 0.00011994542728643864\n",
      "epoch= 775 iteration= 72096 loss= 0.00091287971008569\n",
      "epoch= 775 iteration= 72116 loss= 0.0005763829685747623\n",
      "epoch= 775 iteration= 72136 loss= 0.0007278869161382318\n",
      "epoch= 775 iteration= 72156 loss= 0.0068763243034482\n",
      "test_data MSELoss:(pred-real)/real= 0.0015262679727230635\n",
      "epoch= 776 iteration= 72169 loss= 0.00012176283780718222\n",
      "epoch= 776 iteration= 72189 loss= 0.0009093496482819319\n",
      "epoch= 776 iteration= 72209 loss= 0.0005756116006523371\n",
      "epoch= 776 iteration= 72229 loss= 0.0007271617650985718\n",
      "epoch= 776 iteration= 72249 loss= 0.006844102405011654\n",
      "test_data MSELoss:(pred-real)/real= 0.0015186582491474433\n",
      "epoch= 777 iteration= 72262 loss= 0.00012412059004418552\n",
      "epoch= 777 iteration= 72282 loss= 0.0009067442151717842\n",
      "epoch= 777 iteration= 72302 loss= 0.0005744526861235499\n",
      "epoch= 777 iteration= 72322 loss= 0.0007256480166688561\n",
      "epoch= 777 iteration= 72342 loss= 0.00680986512452364\n",
      "test_data MSELoss:(pred-real)/real= 0.0015114270823283328\n",
      "epoch= 778 iteration= 72355 loss= 0.0001262535370187834\n",
      "epoch= 778 iteration= 72375 loss= 0.0009027583291754127\n",
      "epoch= 778 iteration= 72395 loss= 0.0005732849822379649\n",
      "epoch= 778 iteration= 72415 loss= 0.0007241465500555933\n",
      "epoch= 778 iteration= 72435 loss= 0.006778879091143608\n",
      "test_data MSELoss:(pred-real)/real= 0.0015035915938723418\n",
      "epoch= 779 iteration= 72448 loss= 0.00012888279161415994\n",
      "epoch= 779 iteration= 72468 loss= 0.0008986130123957992\n",
      "epoch= 779 iteration= 72488 loss= 0.0005718639586120844\n",
      "epoch= 779 iteration= 72508 loss= 0.0007225244771689177\n",
      "epoch= 779 iteration= 72528 loss= 0.006746439263224602\n",
      "test_data MSELoss:(pred-real)/real= 0.0014951737604052243\n",
      "epoch= 780 iteration= 72541 loss= 0.00013121688971295953\n",
      "epoch= 780 iteration= 72561 loss= 0.0008960593258962035\n",
      "epoch= 780 iteration= 72581 loss= 0.0005713599966838956\n",
      "epoch= 780 iteration= 72601 loss= 0.0007212081109173596\n",
      "epoch= 780 iteration= 72621 loss= 0.006711636669933796\n",
      "test_data MSELoss:(pred-real)/real= 0.0014873180092157174\n",
      "epoch= 781 iteration= 72634 loss= 0.00013317866250872612\n",
      "epoch= 781 iteration= 72654 loss= 0.0008932021446526051\n",
      "epoch= 781 iteration= 72674 loss= 0.0005688541568815708\n",
      "epoch= 781 iteration= 72694 loss= 0.0007198692765086889\n",
      "epoch= 781 iteration= 72714 loss= 0.0066828252747654915\n",
      "test_data MSELoss:(pred-real)/real= 0.001479903087278621\n",
      "epoch= 782 iteration= 72727 loss= 0.00013478286564350128\n",
      "epoch= 782 iteration= 72747 loss= 0.0008894813363440335\n",
      "epoch= 782 iteration= 72767 loss= 0.0005691667320206761\n",
      "epoch= 782 iteration= 72787 loss= 0.000719031784683466\n",
      "epoch= 782 iteration= 72807 loss= 0.006652012467384338\n",
      "test_data MSELoss:(pred-real)/real= 0.0014727611301673783\n",
      "epoch= 783 iteration= 72820 loss= 0.00013664108701050282\n",
      "epoch= 783 iteration= 72840 loss= 0.0008864685660228133\n",
      "epoch= 783 iteration= 72860 loss= 0.0005677943117916584\n",
      "epoch= 783 iteration= 72880 loss= 0.000717868679203093\n",
      "epoch= 783 iteration= 72900 loss= 0.006618477404117584\n",
      "test_data MSELoss:(pred-real)/real= 0.0014648133122439806\n",
      "epoch= 784 iteration= 72913 loss= 0.0001396122679580003\n",
      "epoch= 784 iteration= 72933 loss= 0.0008817095076665282\n",
      "epoch= 784 iteration= 72953 loss= 0.0005666259676218033\n",
      "epoch= 784 iteration= 72973 loss= 0.0007162246038205922\n",
      "epoch= 784 iteration= 72993 loss= 0.006584871094673872\n",
      "test_data MSELoss:(pred-real)/real= 0.0014571352949133143\n",
      "epoch= 785 iteration= 73006 loss= 0.00014122483844403177\n",
      "epoch= 785 iteration= 73026 loss= 0.000879011582583189\n",
      "epoch= 785 iteration= 73046 loss= 0.0005661314353346825\n",
      "epoch= 785 iteration= 73066 loss= 0.0007144378032535315\n",
      "epoch= 785 iteration= 73086 loss= 0.0065539865754544735\n",
      "test_data MSELoss:(pred-real)/real= 0.0014486802004588146\n",
      "epoch= 786 iteration= 73099 loss= 0.00014369601558428258\n",
      "epoch= 786 iteration= 73119 loss= 0.0008756032329984009\n",
      "epoch= 786 iteration= 73139 loss= 0.0005649528466165066\n",
      "epoch= 786 iteration= 73159 loss= 0.0007127850549295545\n",
      "epoch= 786 iteration= 73179 loss= 0.006522279232740402\n",
      "test_data MSELoss:(pred-real)/real= 0.0014405374305271027\n",
      "epoch= 787 iteration= 73192 loss= 0.00014621144509874284\n",
      "epoch= 787 iteration= 73212 loss= 0.000871557742357254\n",
      "epoch= 787 iteration= 73232 loss= 0.0005639954470098019\n",
      "epoch= 787 iteration= 73252 loss= 0.000711194472387433\n",
      "epoch= 787 iteration= 73272 loss= 0.006488158833235502\n",
      "test_data MSELoss:(pred-real)/real= 0.0014329369684370856\n",
      "epoch= 788 iteration= 73285 loss= 0.0001470922288717702\n",
      "epoch= 788 iteration= 73305 loss= 0.0008697282173670828\n",
      "epoch= 788 iteration= 73325 loss= 0.0005626522470265627\n",
      "epoch= 788 iteration= 73345 loss= 0.0007085929391905665\n",
      "epoch= 788 iteration= 73365 loss= 0.006456073839217424\n",
      "test_data MSELoss:(pred-real)/real= 0.0014245832232215132\n",
      "epoch= 789 iteration= 73378 loss= 0.00014758491306565702\n",
      "epoch= 789 iteration= 73398 loss= 0.0008649968658573925\n",
      "epoch= 789 iteration= 73418 loss= 0.0005619502044282854\n",
      "epoch= 789 iteration= 73438 loss= 0.0007068755803629756\n",
      "epoch= 789 iteration= 73458 loss= 0.006422590930014849\n",
      "test_data MSELoss:(pred-real)/real= 0.0014163353108516377\n",
      "epoch= 790 iteration= 73471 loss= 0.00014880491653457284\n",
      "epoch= 790 iteration= 73491 loss= 0.0008617680869065225\n",
      "epoch= 790 iteration= 73511 loss= 0.000560689892154187\n",
      "epoch= 790 iteration= 73531 loss= 0.0007051400025375187\n",
      "epoch= 790 iteration= 73551 loss= 0.006395353469997644\n",
      "test_data MSELoss:(pred-real)/real= 0.0014080368855502456\n",
      "epoch= 791 iteration= 73564 loss= 0.00014771739370189607\n",
      "epoch= 791 iteration= 73584 loss= 0.0008579376735724509\n",
      "epoch= 791 iteration= 73604 loss= 0.000559487147256732\n",
      "epoch= 791 iteration= 73624 loss= 0.0007029694970697165\n",
      "epoch= 791 iteration= 73644 loss= 0.006363595835864544\n",
      "test_data MSELoss:(pred-real)/real= 0.0013998085261037988\n",
      "epoch= 792 iteration= 73657 loss= 0.00014798299525864422\n",
      "epoch= 792 iteration= 73677 loss= 0.0008547183824703097\n",
      "epoch= 792 iteration= 73697 loss= 0.0005593373207375407\n",
      "epoch= 792 iteration= 73717 loss= 0.0007011195411905646\n",
      "epoch= 792 iteration= 73737 loss= 0.006331146694719791\n",
      "test_data MSELoss:(pred-real)/real= 0.0013932336991678716\n",
      "epoch= 793 iteration= 73750 loss= 0.00014832886517979205\n",
      "epoch= 793 iteration= 73770 loss= 0.0008497651433572173\n",
      "epoch= 793 iteration= 73790 loss= 0.0005583146121352911\n",
      "epoch= 793 iteration= 73810 loss= 0.0006998666212894022\n",
      "epoch= 793 iteration= 73830 loss= 0.006296595558524132\n",
      "test_data MSELoss:(pred-real)/real= 0.0013852634130873615\n",
      "epoch= 794 iteration= 73843 loss= 0.00014923792332410812\n",
      "epoch= 794 iteration= 73863 loss= 0.000845621048938483\n",
      "epoch= 794 iteration= 73883 loss= 0.0005571073852479458\n",
      "epoch= 794 iteration= 73903 loss= 0.0006975636351853609\n",
      "epoch= 794 iteration= 73923 loss= 0.006266565062105656\n",
      "test_data MSELoss:(pred-real)/real= 0.0013773146541401122\n",
      "epoch= 795 iteration= 73936 loss= 0.00015050178626552224\n",
      "epoch= 795 iteration= 73956 loss= 0.0008417595527134836\n",
      "epoch= 795 iteration= 73976 loss= 0.0005567351472564042\n",
      "epoch= 795 iteration= 73996 loss= 0.00069524219725281\n",
      "epoch= 795 iteration= 74016 loss= 0.006235586944967508\n",
      "test_data MSELoss:(pred-real)/real= 0.0013710677740164101\n",
      "epoch= 796 iteration= 74029 loss= 0.00015071124653331935\n",
      "epoch= 796 iteration= 74049 loss= 0.0008377177873626351\n",
      "epoch= 796 iteration= 74069 loss= 0.000556206505279988\n",
      "epoch= 796 iteration= 74089 loss= 0.0006939689628779888\n",
      "epoch= 796 iteration= 74109 loss= 0.00620040949434042\n",
      "test_data MSELoss:(pred-real)/real= 0.0013630276662297547\n",
      "epoch= 797 iteration= 74122 loss= 0.0001512629387434572\n",
      "epoch= 797 iteration= 74142 loss= 0.0008341644424945116\n",
      "epoch= 797 iteration= 74162 loss= 0.000555243925191462\n",
      "epoch= 797 iteration= 74182 loss= 0.0006906865746714175\n",
      "epoch= 797 iteration= 74202 loss= 0.006172139663249254\n",
      "test_data MSELoss:(pred-real)/real= 0.001356007744713376\n",
      "epoch= 798 iteration= 74215 loss= 0.00015208689728751779\n",
      "epoch= 798 iteration= 74235 loss= 0.000829847005661577\n",
      "epoch= 798 iteration= 74255 loss= 0.0005549885681830347\n",
      "epoch= 798 iteration= 74275 loss= 0.0006887546624056995\n",
      "epoch= 798 iteration= 74295 loss= 0.006137946154922247\n",
      "test_data MSELoss:(pred-real)/real= 0.001348482755323251\n",
      "epoch= 799 iteration= 74308 loss= 0.00015192916907835752\n",
      "epoch= 799 iteration= 74328 loss= 0.000824293470941484\n",
      "epoch= 799 iteration= 74348 loss= 0.000554570578970015\n",
      "epoch= 799 iteration= 74368 loss= 0.0006865931791253388\n",
      "epoch= 799 iteration= 74388 loss= 0.006104051135480404\n",
      "test_data MSELoss:(pred-real)/real= 0.0013409056733103676\n",
      "epoch= 800 iteration= 74401 loss= 0.00015103406622074544\n",
      "epoch= 800 iteration= 74421 loss= 0.0008208727813325822\n",
      "epoch= 800 iteration= 74441 loss= 0.0005526802851818502\n",
      "epoch= 800 iteration= 74461 loss= 0.0006843450246378779\n",
      "epoch= 800 iteration= 74481 loss= 0.006074209697544575\n",
      "test_data MSELoss:(pred-real)/real= 0.001333209346436585\n",
      "epoch= 801 iteration= 74494 loss= 0.0001491708098910749\n",
      "epoch= 801 iteration= 74514 loss= 0.0008177575073204935\n",
      "epoch= 801 iteration= 74534 loss= 0.0005521632265299559\n",
      "epoch= 801 iteration= 74554 loss= 0.0006810842314735055\n",
      "epoch= 801 iteration= 74574 loss= 0.006037543062120676\n",
      "test_data MSELoss:(pred-real)/real= 0.0013244521623063418\n",
      "epoch= 802 iteration= 74587 loss= 0.00014979104162193835\n",
      "epoch= 802 iteration= 74607 loss= 0.0008124130545184016\n",
      "epoch= 802 iteration= 74627 loss= 0.0005512317875400186\n",
      "epoch= 802 iteration= 74647 loss= 0.0006797079113312066\n",
      "epoch= 802 iteration= 74667 loss= 0.006006860174238682\n",
      "test_data MSELoss:(pred-real)/real= 0.0013192099059880194\n",
      "epoch= 803 iteration= 74680 loss= 0.00014896916400175542\n",
      "epoch= 803 iteration= 74700 loss= 0.0008084771689027548\n",
      "epoch= 803 iteration= 74720 loss= 0.0005510699120350182\n",
      "epoch= 803 iteration= 74740 loss= 0.0006771483458578587\n",
      "epoch= 803 iteration= 74760 loss= 0.005976295098662376\n",
      "test_data MSELoss:(pred-real)/real= 0.0013111650655951558\n",
      "epoch= 804 iteration= 74773 loss= 0.00014907828881405294\n",
      "epoch= 804 iteration= 74793 loss= 0.00080352951772511\n",
      "epoch= 804 iteration= 74813 loss= 0.0005507355672307312\n",
      "epoch= 804 iteration= 74833 loss= 0.0006744182901456952\n",
      "epoch= 804 iteration= 74853 loss= 0.005943972617387772\n",
      "test_data MSELoss:(pred-real)/real= 0.0013045315491682333\n",
      "epoch= 805 iteration= 74866 loss= 0.00014741564518772066\n",
      "epoch= 805 iteration= 74886 loss= 0.0008013470796868205\n",
      "epoch= 805 iteration= 74906 loss= 0.0005501025589182973\n",
      "epoch= 805 iteration= 74926 loss= 0.000671546789817512\n",
      "epoch= 805 iteration= 74946 loss= 0.005912270396947861\n",
      "test_data MSELoss:(pred-real)/real= 0.0012970053358003497\n",
      "epoch= 806 iteration= 74959 loss= 0.00014805498358327895\n",
      "epoch= 806 iteration= 74979 loss= 0.0007967568235471845\n",
      "epoch= 806 iteration= 74999 loss= 0.0005491263582371175\n",
      "epoch= 806 iteration= 75019 loss= 0.0006694995099678636\n",
      "epoch= 806 iteration= 75039 loss= 0.0058790273033082485\n",
      "test_data MSELoss:(pred-real)/real= 0.0012908988865092397\n",
      "epoch= 807 iteration= 75052 loss= 0.00014548283070325851\n",
      "epoch= 807 iteration= 75072 loss= 0.0007917528855614364\n",
      "epoch= 807 iteration= 75092 loss= 0.0005486313020810485\n",
      "epoch= 807 iteration= 75112 loss= 0.0006662211380898952\n",
      "epoch= 807 iteration= 75132 loss= 0.005848495289683342\n",
      "test_data MSELoss:(pred-real)/real= 0.0012823279555757633\n",
      "epoch= 808 iteration= 75145 loss= 0.0001454906159779057\n",
      "epoch= 808 iteration= 75165 loss= 0.0007879712502472103\n",
      "epoch= 808 iteration= 75185 loss= 0.0005480930558405817\n",
      "epoch= 808 iteration= 75205 loss= 0.0006637789192609489\n",
      "epoch= 808 iteration= 75225 loss= 0.005813945550471544\n",
      "test_data MSELoss:(pred-real)/real= 0.001276580261118296\n",
      "epoch= 809 iteration= 75238 loss= 0.00014297629240900278\n",
      "epoch= 809 iteration= 75258 loss= 0.0007842538179829717\n",
      "epoch= 809 iteration= 75278 loss= 0.0005472775083035231\n",
      "epoch= 809 iteration= 75298 loss= 0.0006612029392272234\n",
      "epoch= 809 iteration= 75318 loss= 0.00578302051872015\n",
      "test_data MSELoss:(pred-real)/real= 0.0012701204751566467\n",
      "epoch= 810 iteration= 75331 loss= 0.0001428677496733144\n",
      "epoch= 810 iteration= 75351 loss= 0.0007793934200890362\n",
      "epoch= 810 iteration= 75371 loss= 0.0005465042777359486\n",
      "epoch= 810 iteration= 75391 loss= 0.0006585847586393356\n",
      "epoch= 810 iteration= 75411 loss= 0.005748360883444548\n",
      "test_data MSELoss:(pred-real)/real= 0.0012635461753234267\n",
      "epoch= 811 iteration= 75424 loss= 0.00014301255578175187\n",
      "epoch= 811 iteration= 75444 loss= 0.0007745267357677221\n",
      "epoch= 811 iteration= 75464 loss= 0.0005455247592180967\n",
      "epoch= 811 iteration= 75484 loss= 0.0006555562140420079\n",
      "epoch= 811 iteration= 75504 loss= 0.005713905673474073\n",
      "test_data MSELoss:(pred-real)/real= 0.0012570334123059486\n",
      "epoch= 812 iteration= 75517 loss= 0.0001419127656845376\n",
      "epoch= 812 iteration= 75537 loss= 0.0007708236807957292\n",
      "epoch= 812 iteration= 75557 loss= 0.000544989830814302\n",
      "epoch= 812 iteration= 75577 loss= 0.0006529074162244797\n",
      "epoch= 812 iteration= 75597 loss= 0.005684333387762308\n",
      "test_data MSELoss:(pred-real)/real= 0.0012502150752374695\n",
      "epoch= 813 iteration= 75610 loss= 0.00014131660282146186\n",
      "epoch= 813 iteration= 75630 loss= 0.0007670335471630096\n",
      "epoch= 813 iteration= 75650 loss= 0.000544208160135895\n",
      "epoch= 813 iteration= 75670 loss= 0.0006503078038804233\n",
      "epoch= 813 iteration= 75690 loss= 0.005652554798871279\n",
      "test_data MSELoss:(pred-real)/real= 0.0012437987542297277\n",
      "epoch= 814 iteration= 75703 loss= 0.0001390259130857885\n",
      "epoch= 814 iteration= 75723 loss= 0.0007633849163539708\n",
      "epoch= 814 iteration= 75743 loss= 0.0005429089069366455\n",
      "epoch= 814 iteration= 75763 loss= 0.0006465223850682378\n",
      "epoch= 814 iteration= 75783 loss= 0.0056183822453022\n",
      "test_data MSELoss:(pred-real)/real= 0.0012362340866174134\n",
      "epoch= 815 iteration= 75796 loss= 0.00013617741933558136\n",
      "epoch= 815 iteration= 75816 loss= 0.0007599401287734509\n",
      "epoch= 815 iteration= 75836 loss= 0.0005430331220850348\n",
      "epoch= 815 iteration= 75856 loss= 0.0006439781282097101\n",
      "epoch= 815 iteration= 75876 loss= 0.005586417857557535\n",
      "test_data MSELoss:(pred-real)/real= 0.0012296150493461431\n",
      "epoch= 816 iteration= 75889 loss= 0.0001326992060057819\n",
      "epoch= 816 iteration= 75909 loss= 0.0007548224530182779\n",
      "epoch= 816 iteration= 75929 loss= 0.0005424568662419915\n",
      "epoch= 816 iteration= 75949 loss= 0.0006413905066438019\n",
      "epoch= 816 iteration= 75969 loss= 0.005549069028347731\n",
      "test_data MSELoss:(pred-real)/real= 0.0012238348491438148\n",
      "epoch= 817 iteration= 75982 loss= 0.00013255822705104947\n",
      "epoch= 817 iteration= 76002 loss= 0.0007517914054915309\n",
      "epoch= 817 iteration= 76022 loss= 0.0005410588346421719\n",
      "epoch= 817 iteration= 76042 loss= 0.0006376088131219149\n",
      "epoch= 817 iteration= 76062 loss= 0.005519946105778217\n",
      "test_data MSELoss:(pred-real)/real= 0.0012164011988918194\n",
      "epoch= 818 iteration= 76075 loss= 0.00013090721040498465\n",
      "epoch= 818 iteration= 76095 loss= 0.0007468303083442152\n",
      "epoch= 818 iteration= 76115 loss= 0.0005405743140727282\n",
      "epoch= 818 iteration= 76135 loss= 0.0006350217154249549\n",
      "epoch= 818 iteration= 76155 loss= 0.00548466295003891\n",
      "test_data MSELoss:(pred-real)/real= 0.0012105534309133266\n",
      "epoch= 819 iteration= 76168 loss= 0.00012911585508845747\n",
      "epoch= 819 iteration= 76188 loss= 0.0007429224788211286\n",
      "epoch= 819 iteration= 76208 loss= 0.0005405569681897759\n",
      "epoch= 819 iteration= 76228 loss= 0.0006323097040876746\n",
      "epoch= 819 iteration= 76248 loss= 0.005454410798847675\n",
      "test_data MSELoss:(pred-real)/real= 0.0012043392618781784\n",
      "epoch= 820 iteration= 76261 loss= 0.00012725067790597677\n",
      "epoch= 820 iteration= 76281 loss= 0.0007386461365967989\n",
      "epoch= 820 iteration= 76301 loss= 0.0005393532919697464\n",
      "epoch= 820 iteration= 76321 loss= 0.0006285427371039987\n",
      "epoch= 820 iteration= 76341 loss= 0.0054215723648667336\n",
      "test_data MSELoss:(pred-real)/real= 0.001198360982622641\n",
      "epoch= 821 iteration= 76354 loss= 0.00012541317846626043\n",
      "epoch= 821 iteration= 76374 loss= 0.0007343333563767374\n",
      "epoch= 821 iteration= 76394 loss= 0.0005387922283262014\n",
      "epoch= 821 iteration= 76414 loss= 0.0006260930094867945\n",
      "epoch= 821 iteration= 76434 loss= 0.005388664081692696\n",
      "test_data MSELoss:(pred-real)/real= 0.0011918935342691839\n",
      "epoch= 822 iteration= 76447 loss= 0.0001248064945684746\n",
      "epoch= 822 iteration= 76467 loss= 0.0007310637156479061\n",
      "epoch= 822 iteration= 76487 loss= 0.0005377334891818464\n",
      "epoch= 822 iteration= 76507 loss= 0.000623712781816721\n",
      "epoch= 822 iteration= 76527 loss= 0.005355389788746834\n",
      "test_data MSELoss:(pred-real)/real= 0.0011863786057801917\n",
      "epoch= 823 iteration= 76540 loss= 0.00012319056259002537\n",
      "epoch= 823 iteration= 76560 loss= 0.0007262117578648031\n",
      "epoch= 823 iteration= 76580 loss= 0.0005372047307901084\n",
      "epoch= 823 iteration= 76600 loss= 0.0006206533871591091\n",
      "epoch= 823 iteration= 76620 loss= 0.0053218938410282135\n",
      "test_data MSELoss:(pred-real)/real= 0.0011797273990749898\n",
      "epoch= 824 iteration= 76633 loss= 0.0001225440064445138\n",
      "epoch= 824 iteration= 76653 loss= 0.0007219678955152631\n",
      "epoch= 824 iteration= 76673 loss= 0.0005362721858546138\n",
      "epoch= 824 iteration= 76693 loss= 0.0006171169807203114\n",
      "epoch= 824 iteration= 76713 loss= 0.005292902700603008\n",
      "test_data MSELoss:(pred-real)/real= 0.0011734355575754307\n",
      "epoch= 825 iteration= 76726 loss= 0.00012132295523770154\n",
      "epoch= 825 iteration= 76746 loss= 0.0007184415007941425\n",
      "epoch= 825 iteration= 76766 loss= 0.0005357572808861732\n",
      "epoch= 825 iteration= 76786 loss= 0.0006141496123746037\n",
      "epoch= 825 iteration= 76806 loss= 0.0052549890242516994\n",
      "test_data MSELoss:(pred-real)/real= 0.0011677762886392884\n",
      "epoch= 826 iteration= 76819 loss= 0.00011978042311966419\n",
      "epoch= 826 iteration= 76839 loss= 0.0007151272147893906\n",
      "epoch= 826 iteration= 76859 loss= 0.0005346177495084703\n",
      "epoch= 826 iteration= 76879 loss= 0.0006115597207099199\n",
      "epoch= 826 iteration= 76899 loss= 0.005224931053817272\n",
      "test_data MSELoss:(pred-real)/real= 0.0011599446426973576\n",
      "epoch= 827 iteration= 76912 loss= 0.0001177431404357776\n",
      "epoch= 827 iteration= 76932 loss= 0.0007115978514775634\n",
      "epoch= 827 iteration= 76952 loss= 0.0005337756592780352\n",
      "epoch= 827 iteration= 76972 loss= 0.0006077213911339641\n",
      "epoch= 827 iteration= 76992 loss= 0.005191513802856207\n",
      "test_data MSELoss:(pred-real)/real= 0.001154857315769833\n",
      "epoch= 828 iteration= 77005 loss= 0.00011606943735387176\n",
      "epoch= 828 iteration= 77025 loss= 0.0007084788521751761\n",
      "epoch= 828 iteration= 77045 loss= 0.0005330566200427711\n",
      "epoch= 828 iteration= 77065 loss= 0.0006043220637366176\n",
      "epoch= 828 iteration= 77085 loss= 0.005160829983651638\n",
      "test_data MSELoss:(pred-real)/real= 0.0011489795376999406\n",
      "epoch= 829 iteration= 77098 loss= 0.00011506782902870327\n",
      "epoch= 829 iteration= 77118 loss= 0.0007046370301395655\n",
      "epoch= 829 iteration= 77138 loss= 0.0005320858326740563\n",
      "epoch= 829 iteration= 77158 loss= 0.0006020639557391405\n",
      "epoch= 829 iteration= 77178 loss= 0.005129579454660416\n",
      "test_data MSELoss:(pred-real)/real= 0.0011430894608363612\n",
      "epoch= 830 iteration= 77191 loss= 0.00011324991646688432\n",
      "epoch= 830 iteration= 77211 loss= 0.0007009234395809472\n",
      "epoch= 830 iteration= 77231 loss= 0.000530483084730804\n",
      "epoch= 830 iteration= 77251 loss= 0.0005985373281873763\n",
      "epoch= 830 iteration= 77271 loss= 0.00509357824921608\n",
      "test_data MSELoss:(pred-real)/real= 0.0011371286099246289\n",
      "epoch= 831 iteration= 77284 loss= 0.00010986052802763879\n",
      "epoch= 831 iteration= 77304 loss= 0.0006982373306527734\n",
      "epoch= 831 iteration= 77324 loss= 0.00053060008212924\n",
      "epoch= 831 iteration= 77344 loss= 0.000595004647038877\n",
      "epoch= 831 iteration= 77364 loss= 0.005060484632849693\n",
      "test_data MSELoss:(pred-real)/real= 0.0011305025254841894\n",
      "epoch= 832 iteration= 77377 loss= 0.00010739696153905243\n",
      "epoch= 832 iteration= 77397 loss= 0.0006951505201868713\n",
      "epoch= 832 iteration= 77417 loss= 0.0005293370923027396\n",
      "epoch= 832 iteration= 77437 loss= 0.0005925750592723489\n",
      "epoch= 832 iteration= 77457 loss= 0.005027038045227528\n",
      "test_data MSELoss:(pred-real)/real= 0.0011252029119835545\n",
      "epoch= 833 iteration= 77470 loss= 0.00010515491885598749\n",
      "epoch= 833 iteration= 77490 loss= 0.0006917099817655981\n",
      "epoch= 833 iteration= 77510 loss= 0.0005282560596242547\n",
      "epoch= 833 iteration= 77530 loss= 0.0005890539614483714\n",
      "epoch= 833 iteration= 77550 loss= 0.004994212184101343\n",
      "test_data MSELoss:(pred-real)/real= 0.001119874032964516\n",
      "epoch= 834 iteration= 77563 loss= 0.00010401784675195813\n",
      "epoch= 834 iteration= 77583 loss= 0.000687941734213382\n",
      "epoch= 834 iteration= 77603 loss= 0.0005275205476209521\n",
      "epoch= 834 iteration= 77623 loss= 0.0005858786753378808\n",
      "epoch= 834 iteration= 77643 loss= 0.004964246414601803\n",
      "test_data MSELoss:(pred-real)/real= 0.0011128670044450297\n",
      "epoch= 835 iteration= 77656 loss= 0.00010106197441928089\n",
      "epoch= 835 iteration= 77676 loss= 0.0006831309874542058\n",
      "epoch= 835 iteration= 77696 loss= 0.0005266385851427913\n",
      "epoch= 835 iteration= 77716 loss= 0.0005831290036439896\n",
      "epoch= 835 iteration= 77736 loss= 0.004930783063173294\n",
      "test_data MSELoss:(pred-real)/real= 0.0011067049514773923\n",
      "epoch= 836 iteration= 77749 loss= 9.849773778114468e-05\n",
      "epoch= 836 iteration= 77769 loss= 0.0006818055408075452\n",
      "epoch= 836 iteration= 77789 loss= 0.0005255118012428284\n",
      "epoch= 836 iteration= 77809 loss= 0.0005796772893518209\n",
      "epoch= 836 iteration= 77829 loss= 0.004894777201116085\n",
      "test_data MSELoss:(pred-real)/real= 0.0011023128754459321\n",
      "epoch= 837 iteration= 77842 loss= 9.695025801192969e-05\n",
      "epoch= 837 iteration= 77862 loss= 0.0006783713470213115\n",
      "epoch= 837 iteration= 77882 loss= 0.0005239285528659821\n",
      "epoch= 837 iteration= 77902 loss= 0.0005762772634625435\n",
      "epoch= 837 iteration= 77922 loss= 0.004861529916524887\n",
      "test_data MSELoss:(pred-real)/real= 0.001096527041227091\n",
      "epoch= 838 iteration= 77935 loss= 9.620904893381521e-05\n",
      "epoch= 838 iteration= 77955 loss= 0.0006748355808667839\n",
      "epoch= 838 iteration= 77975 loss= 0.0005225827917456627\n",
      "epoch= 838 iteration= 77995 loss= 0.000573343422729522\n",
      "epoch= 838 iteration= 78015 loss= 0.004829486832022667\n",
      "test_data MSELoss:(pred-real)/real= 0.0010903473092993307\n",
      "epoch= 839 iteration= 78028 loss= 9.431919897906482e-05\n",
      "epoch= 839 iteration= 78048 loss= 0.0006718292133882642\n",
      "epoch= 839 iteration= 78068 loss= 0.0005218116566538811\n",
      "epoch= 839 iteration= 78088 loss= 0.000569802476093173\n",
      "epoch= 839 iteration= 78108 loss= 0.004797595553100109\n",
      "test_data MSELoss:(pred-real)/real= 0.001085045237333462\n",
      "epoch= 840 iteration= 78121 loss= 9.303951810579747e-05\n",
      "epoch= 840 iteration= 78141 loss= 0.0006688427529297769\n",
      "epoch= 840 iteration= 78161 loss= 0.0005205344641581178\n",
      "epoch= 840 iteration= 78181 loss= 0.0005664904601871967\n",
      "epoch= 840 iteration= 78201 loss= 0.004763678181916475\n",
      "test_data MSELoss:(pred-real)/real= 0.0010788103528385465\n",
      "epoch= 841 iteration= 78214 loss= 8.988785702968016e-05\n",
      "epoch= 841 iteration= 78234 loss= 0.0006656195037066936\n",
      "epoch= 841 iteration= 78254 loss= 0.0005190719966776669\n",
      "epoch= 841 iteration= 78274 loss= 0.0005638674483634531\n",
      "epoch= 841 iteration= 78294 loss= 0.004731347318738699\n",
      "test_data MSELoss:(pred-real)/real= 0.0010738577936232712\n",
      "epoch= 842 iteration= 78307 loss= 8.740591874811798e-05\n",
      "epoch= 842 iteration= 78327 loss= 0.0006631602882407606\n",
      "epoch= 842 iteration= 78347 loss= 0.0005171301309019327\n",
      "epoch= 842 iteration= 78367 loss= 0.0005606216145679355\n",
      "epoch= 842 iteration= 78387 loss= 0.004700583405792713\n",
      "test_data MSELoss:(pred-real)/real= 0.0010676490928744897\n",
      "epoch= 843 iteration= 78400 loss= 8.555150270694867e-05\n",
      "epoch= 843 iteration= 78420 loss= 0.0006602468201890588\n",
      "epoch= 843 iteration= 78440 loss= 0.0005163598689250648\n",
      "epoch= 843 iteration= 78460 loss= 0.0005569108761847019\n",
      "epoch= 843 iteration= 78480 loss= 0.004666267428547144\n",
      "test_data MSELoss:(pred-real)/real= 0.0010624588806725417\n",
      "epoch= 844 iteration= 78493 loss= 8.374275057576597e-05\n",
      "epoch= 844 iteration= 78513 loss= 0.0006579572800546885\n",
      "epoch= 844 iteration= 78533 loss= 0.0005147428018972278\n",
      "epoch= 844 iteration= 78553 loss= 0.0005534713855013251\n",
      "epoch= 844 iteration= 78573 loss= 0.0046307663433253765\n",
      "test_data MSELoss:(pred-real)/real= 0.001057436722880488\n",
      "epoch= 845 iteration= 78586 loss= 8.176395203918219e-05\n",
      "epoch= 845 iteration= 78606 loss= 0.0006558307213708758\n",
      "epoch= 845 iteration= 78626 loss= 0.0005137907573953271\n",
      "epoch= 845 iteration= 78646 loss= 0.0005502459243871272\n",
      "epoch= 845 iteration= 78666 loss= 0.004599635023623705\n",
      "test_data MSELoss:(pred-real)/real= 0.0010515370522625744\n",
      "epoch= 846 iteration= 78679 loss= 7.997089414857328e-05\n",
      "epoch= 846 iteration= 78699 loss= 0.0006520019378513098\n",
      "epoch= 846 iteration= 78719 loss= 0.0005126601317897439\n",
      "epoch= 846 iteration= 78739 loss= 0.0005465022986754775\n",
      "epoch= 846 iteration= 78759 loss= 0.004567296709865332\n",
      "test_data MSELoss:(pred-real)/real= 0.0010448070695727235\n",
      "epoch= 847 iteration= 78772 loss= 7.801842002663761e-05\n",
      "epoch= 847 iteration= 78792 loss= 0.0006489235092885792\n",
      "epoch= 847 iteration= 78812 loss= 0.0005111955106258392\n",
      "epoch= 847 iteration= 78832 loss= 0.0005432722391560674\n",
      "epoch= 847 iteration= 78852 loss= 0.004533765837550163\n",
      "test_data MSELoss:(pred-real)/real= 0.0010387582280802438\n",
      "epoch= 848 iteration= 78865 loss= 7.600100798299536e-05\n",
      "epoch= 848 iteration= 78885 loss= 0.0006475619156844914\n",
      "epoch= 848 iteration= 78905 loss= 0.000509684206917882\n",
      "epoch= 848 iteration= 78925 loss= 0.0005403991090133786\n",
      "epoch= 848 iteration= 78945 loss= 0.004503334406763315\n",
      "test_data MSELoss:(pred-real)/real= 0.0010345034420930056\n",
      "epoch= 849 iteration= 78958 loss= 7.418791210511699e-05\n",
      "epoch= 849 iteration= 78978 loss= 0.000644500192720443\n",
      "epoch= 849 iteration= 78998 loss= 0.0005084022996015847\n",
      "epoch= 849 iteration= 79018 loss= 0.0005370649741962552\n",
      "epoch= 849 iteration= 79038 loss= 0.004467866383492947\n",
      "test_data MSELoss:(pred-real)/real= 0.0010282886715786946\n",
      "epoch= 850 iteration= 79051 loss= 7.317464041989297e-05\n",
      "epoch= 850 iteration= 79071 loss= 0.0006412914954125881\n",
      "epoch= 850 iteration= 79091 loss= 0.0005072108469903469\n",
      "epoch= 850 iteration= 79111 loss= 0.0005340821226127446\n",
      "epoch= 850 iteration= 79131 loss= 0.0044396547600626945\n",
      "test_data MSELoss:(pred-real)/real= 0.0010229677411391297\n",
      "epoch= 851 iteration= 79144 loss= 7.172177720349282e-05\n",
      "epoch= 851 iteration= 79164 loss= 0.0006404328742064536\n",
      "epoch= 851 iteration= 79184 loss= 0.0005056059453636408\n",
      "epoch= 851 iteration= 79204 loss= 0.000530368066392839\n",
      "epoch= 851 iteration= 79224 loss= 0.004404659382998943\n",
      "test_data MSELoss:(pred-real)/real= 0.0010178092585798975\n",
      "epoch= 852 iteration= 79237 loss= 6.976477743592113e-05\n",
      "epoch= 852 iteration= 79257 loss= 0.0006379535188898444\n",
      "epoch= 852 iteration= 79277 loss= 0.0005049239262007177\n",
      "epoch= 852 iteration= 79297 loss= 0.000527119729667902\n",
      "epoch= 852 iteration= 79317 loss= 0.004372084513306618\n",
      "test_data MSELoss:(pred-real)/real= 0.0010126379517411503\n",
      "epoch= 853 iteration= 79330 loss= 6.810958439018577e-05\n",
      "epoch= 853 iteration= 79350 loss= 0.0006361449486576021\n",
      "epoch= 853 iteration= 79370 loss= 0.0005019052769057453\n",
      "epoch= 853 iteration= 79390 loss= 0.0005237701698206365\n",
      "epoch= 853 iteration= 79410 loss= 0.004338137339800596\n",
      "test_data MSELoss:(pred-real)/real= 0.0010076565845843612\n",
      "epoch= 854 iteration= 79423 loss= 6.617697363253683e-05\n",
      "epoch= 854 iteration= 79443 loss= 0.0006345110014081001\n",
      "epoch= 854 iteration= 79463 loss= 0.0005005384446121752\n",
      "epoch= 854 iteration= 79483 loss= 0.0005207681097090244\n",
      "epoch= 854 iteration= 79503 loss= 0.00430349400267005\n",
      "test_data MSELoss:(pred-real)/real= 0.001001708099566814\n",
      "epoch= 855 iteration= 79516 loss= 6.471826782217249e-05\n",
      "epoch= 855 iteration= 79536 loss= 0.0006331052863970399\n",
      "epoch= 855 iteration= 79556 loss= 0.000498416309710592\n",
      "epoch= 855 iteration= 79576 loss= 0.0005169563228264451\n",
      "epoch= 855 iteration= 79596 loss= 0.004276311490684748\n",
      "test_data MSELoss:(pred-real)/real= 0.0009959629363341567\n",
      "epoch= 856 iteration= 79609 loss= 6.325196591205895e-05\n",
      "epoch= 856 iteration= 79629 loss= 0.0006315337377600372\n",
      "epoch= 856 iteration= 79649 loss= 0.000496796565130353\n",
      "epoch= 856 iteration= 79669 loss= 0.0005139465793035924\n",
      "epoch= 856 iteration= 79689 loss= 0.004238353110849857\n",
      "test_data MSELoss:(pred-real)/real= 0.0009909138025250286\n",
      "epoch= 857 iteration= 79702 loss= 6.232858140720055e-05\n",
      "epoch= 857 iteration= 79722 loss= 0.0006306812865659595\n",
      "epoch= 857 iteration= 79742 loss= 0.0004952463787049055\n",
      "epoch= 857 iteration= 79762 loss= 0.0005108462646603584\n",
      "epoch= 857 iteration= 79782 loss= 0.0042073605582118034\n",
      "test_data MSELoss:(pred-real)/real= 0.0009861387176594387\n",
      "epoch= 858 iteration= 79795 loss= 6.192761793499812e-05\n",
      "epoch= 858 iteration= 79815 loss= 0.0006277540815062821\n",
      "epoch= 858 iteration= 79835 loss= 0.0004934879252687097\n",
      "epoch= 858 iteration= 79855 loss= 0.000506863696500659\n",
      "epoch= 858 iteration= 79875 loss= 0.004175519570708275\n",
      "test_data MSELoss:(pred-real)/real= 0.0009807349981403807\n",
      "epoch= 859 iteration= 79888 loss= 6.118279998190701e-05\n",
      "epoch= 859 iteration= 79908 loss= 0.0006260009249672294\n",
      "epoch= 859 iteration= 79928 loss= 0.0004907263210043311\n",
      "epoch= 859 iteration= 79948 loss= 0.0005035072681494057\n",
      "epoch= 859 iteration= 79968 loss= 0.004144289530813694\n",
      "test_data MSELoss:(pred-real)/real= 0.0009739270220355442\n",
      "epoch= 860 iteration= 79981 loss= 6.080223465687595e-05\n",
      "epoch= 860 iteration= 80001 loss= 0.0006254300242289901\n",
      "epoch= 860 iteration= 80021 loss= 0.0004888674011453986\n",
      "epoch= 860 iteration= 80041 loss= 0.0005007092258892953\n",
      "epoch= 860 iteration= 80061 loss= 0.0041115665808320045\n",
      "test_data MSELoss:(pred-real)/real= 0.000969076961660499\n",
      "epoch= 861 iteration= 80074 loss= 6.118041346780956e-05\n",
      "epoch= 861 iteration= 80094 loss= 0.0006252939347177744\n",
      "epoch= 861 iteration= 80114 loss= 0.0004861881898250431\n",
      "epoch= 861 iteration= 80134 loss= 0.0004965965053997934\n",
      "epoch= 861 iteration= 80154 loss= 0.0040808250196278095\n",
      "test_data MSELoss:(pred-real)/real= 0.0009634615311774218\n",
      "epoch= 862 iteration= 80167 loss= 6.12628209637478e-05\n",
      "epoch= 862 iteration= 80187 loss= 0.0006237300694920123\n",
      "epoch= 862 iteration= 80207 loss= 0.000483428273582831\n",
      "epoch= 862 iteration= 80227 loss= 0.0004940201761201024\n",
      "epoch= 862 iteration= 80247 loss= 0.004051271826028824\n",
      "test_data MSELoss:(pred-real)/real= 0.0009574598540590765\n",
      "epoch= 863 iteration= 80260 loss= 6.196305912453681e-05\n",
      "epoch= 863 iteration= 80280 loss= 0.0006232163868844509\n",
      "epoch= 863 iteration= 80300 loss= 0.00048121571308001876\n",
      "epoch= 863 iteration= 80320 loss= 0.0004896020982414484\n",
      "epoch= 863 iteration= 80340 loss= 0.004016002640128136\n",
      "test_data MSELoss:(pred-real)/real= 0.000951878035266418\n",
      "epoch= 864 iteration= 80353 loss= 6.328553718049079e-05\n",
      "epoch= 864 iteration= 80373 loss= 0.0006217212066985667\n",
      "epoch= 864 iteration= 80393 loss= 0.0004791897372342646\n",
      "epoch= 864 iteration= 80413 loss= 0.0004859999753534794\n",
      "epoch= 864 iteration= 80433 loss= 0.003983653150498867\n",
      "test_data MSELoss:(pred-real)/real= 0.0009462965447002918\n",
      "epoch= 865 iteration= 80446 loss= 6.508294609375298e-05\n",
      "epoch= 865 iteration= 80466 loss= 0.000621491577476263\n",
      "epoch= 865 iteration= 80486 loss= 0.0004773116670548916\n",
      "epoch= 865 iteration= 80506 loss= 0.00048275350127369165\n",
      "epoch= 865 iteration= 80526 loss= 0.003951383288949728\n",
      "test_data MSELoss:(pred-real)/real= 0.0009408008845639415\n",
      "epoch= 866 iteration= 80539 loss= 6.780846888432279e-05\n",
      "epoch= 866 iteration= 80559 loss= 0.0006219511851668358\n",
      "epoch= 866 iteration= 80579 loss= 0.00047472823644056916\n",
      "epoch= 866 iteration= 80599 loss= 0.0004791485844179988\n",
      "epoch= 866 iteration= 80619 loss= 0.003917822614312172\n",
      "test_data MSELoss:(pred-real)/real= 0.0009363774936193497\n",
      "epoch= 867 iteration= 80632 loss= 7.10409294697456e-05\n",
      "epoch= 867 iteration= 80652 loss= 0.0006208549602888525\n",
      "epoch= 867 iteration= 80672 loss= 0.0004719901771750301\n",
      "epoch= 867 iteration= 80692 loss= 0.000476039364002645\n",
      "epoch= 867 iteration= 80712 loss= 0.0038863103836774826\n",
      "test_data MSELoss:(pred-real)/real= 0.0009316528562016578\n",
      "epoch= 868 iteration= 80725 loss= 7.492888835258782e-05\n",
      "epoch= 868 iteration= 80745 loss= 0.0006220589857548475\n",
      "epoch= 868 iteration= 80765 loss= 0.000468443933641538\n",
      "epoch= 868 iteration= 80785 loss= 0.00047293415991589427\n",
      "epoch= 868 iteration= 80805 loss= 0.0038534612394869328\n",
      "test_data MSELoss:(pred-real)/real= 0.0009277836797991768\n",
      "epoch= 869 iteration= 80818 loss= 7.92190694482997e-05\n",
      "epoch= 869 iteration= 80838 loss= 0.0006218944326974452\n",
      "epoch= 869 iteration= 80858 loss= 0.00046715850476175547\n",
      "epoch= 869 iteration= 80878 loss= 0.00046966999070718884\n",
      "epoch= 869 iteration= 80898 loss= 0.0038188749458640814\n",
      "test_data MSELoss:(pred-real)/real= 0.000922603082370996\n",
      "epoch= 870 iteration= 80911 loss= 8.422345854341984e-05\n",
      "epoch= 870 iteration= 80931 loss= 0.0006219634669832885\n",
      "epoch= 870 iteration= 80951 loss= 0.00046429483336396515\n",
      "epoch= 870 iteration= 80971 loss= 0.0004672299255616963\n",
      "epoch= 870 iteration= 80991 loss= 0.0037875494454056025\n",
      "test_data MSELoss:(pred-real)/real= 0.0009186848943095861\n",
      "epoch= 871 iteration= 81004 loss= 8.830537262838334e-05\n",
      "epoch= 871 iteration= 81024 loss= 0.0006206054240465164\n",
      "epoch= 871 iteration= 81044 loss= 0.000462006195448339\n",
      "epoch= 871 iteration= 81064 loss= 0.0004634594079107046\n",
      "epoch= 871 iteration= 81084 loss= 0.003756449557840824\n",
      "test_data MSELoss:(pred-real)/real= 0.0009134444020067652\n",
      "epoch= 872 iteration= 81097 loss= 9.473285172134638e-05\n",
      "epoch= 872 iteration= 81117 loss= 0.0006217071204446256\n",
      "epoch= 872 iteration= 81137 loss= 0.0004599110397975892\n",
      "epoch= 872 iteration= 81157 loss= 0.00045935175148770213\n",
      "epoch= 872 iteration= 81177 loss= 0.003726667258888483\n",
      "test_data MSELoss:(pred-real)/real= 0.0009084713836071185\n",
      "epoch= 873 iteration= 81190 loss= 0.00010106441914103925\n",
      "epoch= 873 iteration= 81210 loss= 0.0006198207847774029\n",
      "epoch= 873 iteration= 81230 loss= 0.000457345595350489\n",
      "epoch= 873 iteration= 81250 loss= 0.0004557583015412092\n",
      "epoch= 873 iteration= 81270 loss= 0.003689402714371681\n",
      "test_data MSELoss:(pred-real)/real= 0.0009039540972379553\n",
      "epoch= 874 iteration= 81283 loss= 0.0001126881834352389\n",
      "epoch= 874 iteration= 81303 loss= 0.000620904378592968\n",
      "epoch= 874 iteration= 81323 loss= 0.00045458710519596934\n",
      "epoch= 874 iteration= 81343 loss= 0.00045291968854144216\n",
      "epoch= 874 iteration= 81363 loss= 0.003658025059849024\n",
      "test_data MSELoss:(pred-real)/real= 0.0008981154807972618\n",
      "epoch= 875 iteration= 81376 loss= 0.00012227897241245955\n",
      "epoch= 875 iteration= 81396 loss= 0.0006202124059200287\n",
      "epoch= 875 iteration= 81416 loss= 0.0004512301238719374\n",
      "epoch= 875 iteration= 81436 loss= 0.0004499861679505557\n",
      "epoch= 875 iteration= 81456 loss= 0.003627312369644642\n",
      "test_data MSELoss:(pred-real)/real= 0.0008944011715357192\n",
      "epoch= 876 iteration= 81469 loss= 0.00013511022552847862\n",
      "epoch= 876 iteration= 81489 loss= 0.0006207241676747799\n",
      "epoch= 876 iteration= 81509 loss= 0.0004492597363423556\n",
      "epoch= 876 iteration= 81529 loss= 0.00044680244172923267\n",
      "epoch= 876 iteration= 81549 loss= 0.0035922478418797255\n",
      "test_data MSELoss:(pred-real)/real= 0.0008888915633886225\n",
      "epoch= 877 iteration= 81562 loss= 0.00014909864694345742\n",
      "epoch= 877 iteration= 81582 loss= 0.0006221140502020717\n",
      "epoch= 877 iteration= 81602 loss= 0.0004463018267415464\n",
      "epoch= 877 iteration= 81622 loss= 0.0004432836140040308\n",
      "epoch= 877 iteration= 81642 loss= 0.0035627628676593304\n",
      "test_data MSELoss:(pred-real)/real= 0.0008842102615744807\n",
      "epoch= 878 iteration= 81655 loss= 0.00016583112301304936\n",
      "epoch= 878 iteration= 81675 loss= 0.0006218357011675835\n",
      "epoch= 878 iteration= 81695 loss= 0.0004440975608304143\n",
      "epoch= 878 iteration= 81715 loss= 0.0004399495664983988\n",
      "epoch= 878 iteration= 81735 loss= 0.003532995702698827\n",
      "test_data MSELoss:(pred-real)/real= 0.0008806714758975431\n",
      "epoch= 879 iteration= 81748 loss= 0.00018421161803416908\n",
      "epoch= 879 iteration= 81768 loss= 0.0006231445586308837\n",
      "epoch= 879 iteration= 81788 loss= 0.000441531534306705\n",
      "epoch= 879 iteration= 81808 loss= 0.0004371799877844751\n",
      "epoch= 879 iteration= 81828 loss= 0.0034931832924485207\n",
      "test_data MSELoss:(pred-real)/real= 0.0008760248634239866\n",
      "epoch= 880 iteration= 81841 loss= 0.0002031901676673442\n",
      "epoch= 880 iteration= 81861 loss= 0.0006236191256903112\n",
      "epoch= 880 iteration= 81881 loss= 0.0004391236579976976\n",
      "epoch= 880 iteration= 81901 loss= 0.0004339605220593512\n",
      "epoch= 880 iteration= 81921 loss= 0.003463486675173044\n",
      "test_data MSELoss:(pred-real)/real= 0.0008721739690776707\n",
      "epoch= 881 iteration= 81934 loss= 0.00022558445925824344\n",
      "epoch= 881 iteration= 81954 loss= 0.0006246369448490441\n",
      "epoch= 881 iteration= 81974 loss= 0.000437171314842999\n",
      "epoch= 881 iteration= 81994 loss= 0.00043072091648355126\n",
      "epoch= 881 iteration= 82014 loss= 0.0034296223893761635\n",
      "test_data MSELoss:(pred-real)/real= 0.0008687335980034226\n",
      "epoch= 882 iteration= 82027 loss= 0.00024397988454438746\n",
      "epoch= 882 iteration= 82047 loss= 0.0006235876935534179\n",
      "epoch= 882 iteration= 82067 loss= 0.0004351228126324713\n",
      "epoch= 882 iteration= 82087 loss= 0.00042830570600926876\n",
      "epoch= 882 iteration= 82107 loss= 0.003397059626877308\n",
      "test_data MSELoss:(pred-real)/real= 0.0008651008821390052\n",
      "epoch= 883 iteration= 82120 loss= 0.000260219385381788\n",
      "epoch= 883 iteration= 82140 loss= 0.0006225149263627827\n",
      "epoch= 883 iteration= 82160 loss= 0.0004334726254455745\n",
      "epoch= 883 iteration= 82180 loss= 0.00042605813359841704\n",
      "epoch= 883 iteration= 82200 loss= 0.0033676468301564455\n",
      "test_data MSELoss:(pred-real)/real= 0.000861902258798687\n",
      "epoch= 884 iteration= 82213 loss= 0.000286110705928877\n",
      "epoch= 884 iteration= 82233 loss= 0.0006220338400453329\n",
      "epoch= 884 iteration= 82253 loss= 0.00043094289139844477\n",
      "epoch= 884 iteration= 82273 loss= 0.00042327045230194926\n",
      "epoch= 884 iteration= 82293 loss= 0.0033337255008518696\n",
      "test_data MSELoss:(pred-real)/real= 0.0008588662086468604\n",
      "epoch= 885 iteration= 82306 loss= 0.0003099920286331326\n",
      "epoch= 885 iteration= 82326 loss= 0.0006221283692866564\n",
      "epoch= 885 iteration= 82346 loss= 0.00042875081999227405\n",
      "epoch= 885 iteration= 82366 loss= 0.00041998730739578605\n",
      "epoch= 885 iteration= 82386 loss= 0.003305186051875353\n",
      "test_data MSELoss:(pred-real)/real= 0.0008551117789465934\n",
      "epoch= 886 iteration= 82399 loss= 0.0003363333235029131\n",
      "epoch= 886 iteration= 82419 loss= 0.0006191933644004166\n",
      "epoch= 886 iteration= 82439 loss= 0.0004262377042323351\n",
      "epoch= 886 iteration= 82459 loss= 0.0004173419438302517\n",
      "epoch= 886 iteration= 82479 loss= 0.0032716654241085052\n",
      "test_data MSELoss:(pred-real)/real= 0.0008513535342192174\n",
      "epoch= 887 iteration= 82492 loss= 0.0003698207437992096\n",
      "epoch= 887 iteration= 82512 loss= 0.0006205352256074548\n",
      "epoch= 887 iteration= 82532 loss= 0.0004240540729369968\n",
      "epoch= 887 iteration= 82552 loss= 0.0004147426225244999\n",
      "epoch= 887 iteration= 82572 loss= 0.0032406768295913935\n",
      "test_data MSELoss:(pred-real)/real= 0.0008485115265810034\n",
      "epoch= 888 iteration= 82585 loss= 0.0004119090735912323\n",
      "epoch= 888 iteration= 82605 loss= 0.0006220861687324941\n",
      "epoch= 888 iteration= 82625 loss= 0.0004227550816722214\n",
      "epoch= 888 iteration= 82645 loss= 0.00041215118835680187\n",
      "epoch= 888 iteration= 82665 loss= 0.003208679147064686\n",
      "test_data MSELoss:(pred-real)/real= 0.0008455269821246879\n",
      "epoch= 889 iteration= 82678 loss= 0.00044715090189129114\n",
      "epoch= 889 iteration= 82698 loss= 0.00062270846683532\n",
      "epoch= 889 iteration= 82718 loss= 0.00042063090950250626\n",
      "epoch= 889 iteration= 82738 loss= 0.0004099953221157193\n",
      "epoch= 889 iteration= 82758 loss= 0.0031765210442245007\n",
      "test_data MSELoss:(pred-real)/real= 0.0008424426325493389\n",
      "epoch= 890 iteration= 82771 loss= 0.0004815352149307728\n",
      "epoch= 890 iteration= 82791 loss= 0.0006214166060090065\n",
      "epoch= 890 iteration= 82811 loss= 0.00041866747778840363\n",
      "epoch= 890 iteration= 82831 loss= 0.0004076103796251118\n",
      "epoch= 890 iteration= 82851 loss= 0.003144969930872321\n",
      "test_data MSELoss:(pred-real)/real= 0.0008399818834732287\n",
      "epoch= 891 iteration= 82864 loss= 0.0005250172689557076\n",
      "epoch= 891 iteration= 82884 loss= 0.0006231910083442926\n",
      "epoch= 891 iteration= 82904 loss= 0.0004168359737377614\n",
      "epoch= 891 iteration= 82924 loss= 0.0004051782307215035\n",
      "epoch= 891 iteration= 82944 loss= 0.0031174770556390285\n",
      "test_data MSELoss:(pred-real)/real= 0.0008373710330084173\n",
      "epoch= 892 iteration= 82957 loss= 0.0005766991525888443\n",
      "epoch= 892 iteration= 82977 loss= 0.0006259935908019543\n",
      "epoch= 892 iteration= 82997 loss= 0.00041539210360497236\n",
      "epoch= 892 iteration= 83017 loss= 0.00040227349381893873\n",
      "epoch= 892 iteration= 83037 loss= 0.003088668454438448\n",
      "test_data MSELoss:(pred-real)/real= 0.0008350190514142418\n",
      "epoch= 893 iteration= 83050 loss= 0.000624875770881772\n",
      "epoch= 893 iteration= 83070 loss= 0.0006271973834373057\n",
      "epoch= 893 iteration= 83090 loss= 0.0004129106819164008\n",
      "epoch= 893 iteration= 83110 loss= 0.00040105206426233053\n",
      "epoch= 893 iteration= 83130 loss= 0.003058304311707616\n",
      "test_data MSELoss:(pred-real)/real= 0.0008327134338388634\n",
      "epoch= 894 iteration= 83143 loss= 0.0006875955150462687\n",
      "epoch= 894 iteration= 83163 loss= 0.0006300691748037934\n",
      "epoch= 894 iteration= 83183 loss= 0.00041155668441206217\n",
      "epoch= 894 iteration= 83203 loss= 0.0003994539729319513\n",
      "epoch= 894 iteration= 83223 loss= 0.0030253990553319454\n",
      "test_data MSELoss:(pred-real)/real= 0.0008313610095582488\n",
      "epoch= 895 iteration= 83236 loss= 0.0007425432559102774\n",
      "epoch= 895 iteration= 83256 loss= 0.000634142430499196\n",
      "epoch= 895 iteration= 83276 loss= 0.0004080815124325454\n",
      "epoch= 895 iteration= 83296 loss= 0.00039743073284626007\n",
      "epoch= 895 iteration= 83316 loss= 0.002999314572662115\n",
      "test_data MSELoss:(pred-real)/real= 0.0008296628463237236\n",
      "epoch= 896 iteration= 83329 loss= 0.0008135627722367644\n",
      "epoch= 896 iteration= 83349 loss= 0.0006394058000296354\n",
      "epoch= 896 iteration= 83369 loss= 0.00040607404662296176\n",
      "epoch= 896 iteration= 83389 loss= 0.00039631075924262404\n",
      "epoch= 896 iteration= 83409 loss= 0.0029717357829213142\n",
      "test_data MSELoss:(pred-real)/real= 0.0008284465778463831\n",
      "epoch= 897 iteration= 83422 loss= 0.0008841768139973283\n",
      "epoch= 897 iteration= 83442 loss= 0.0006463371682912111\n",
      "epoch= 897 iteration= 83462 loss= 0.0004040985368192196\n",
      "epoch= 897 iteration= 83482 loss= 0.0003941612667404115\n",
      "epoch= 897 iteration= 83502 loss= 0.0029434096068143845\n",
      "test_data MSELoss:(pred-real)/real= 0.0008273246259907157\n",
      "epoch= 898 iteration= 83515 loss= 0.0009573869174346328\n",
      "epoch= 898 iteration= 83535 loss= 0.0006541266920976341\n",
      "epoch= 898 iteration= 83555 loss= 0.0004010753473266959\n",
      "epoch= 898 iteration= 83575 loss= 0.00039329464198090136\n",
      "epoch= 898 iteration= 83595 loss= 0.0029169514309614897\n",
      "test_data MSELoss:(pred-real)/real= 0.0008262518482903639\n",
      "epoch= 899 iteration= 83608 loss= 0.0010347678326070309\n",
      "epoch= 899 iteration= 83628 loss= 0.000661392929032445\n",
      "epoch= 899 iteration= 83648 loss= 0.0003975315485149622\n",
      "epoch= 899 iteration= 83668 loss= 0.00039167486829683185\n",
      "epoch= 899 iteration= 83688 loss= 0.002893790602684021\n",
      "test_data MSELoss:(pred-real)/real= 0.0008252258436793151\n",
      "epoch= 900 iteration= 83701 loss= 0.0011122820433229208\n",
      "epoch= 900 iteration= 83721 loss= 0.0006712547037750483\n",
      "epoch= 900 iteration= 83741 loss= 0.0003936200519092381\n",
      "epoch= 900 iteration= 83761 loss= 0.00039023655699566007\n",
      "epoch= 900 iteration= 83781 loss= 0.0028687191661447287\n",
      "test_data MSELoss:(pred-real)/real= 0.0008249273903654992\n",
      "epoch= 901 iteration= 83794 loss= 0.0012089747469872236\n",
      "epoch= 901 iteration= 83814 loss= 0.0006820724811404943\n",
      "epoch= 901 iteration= 83834 loss= 0.00039081915747374296\n",
      "epoch= 901 iteration= 83854 loss= 0.00038919650251045823\n",
      "epoch= 901 iteration= 83874 loss= 0.002837930340319872\n",
      "test_data MSELoss:(pred-real)/real= 0.0008243326333791225\n",
      "epoch= 902 iteration= 83887 loss= 0.0013106763362884521\n",
      "epoch= 902 iteration= 83907 loss= 0.0006935547571629286\n",
      "epoch= 902 iteration= 83927 loss= 0.00038694002432748675\n",
      "epoch= 902 iteration= 83947 loss= 0.0003879736177623272\n",
      "epoch= 902 iteration= 83967 loss= 0.002815757179632783\n",
      "test_data MSELoss:(pred-real)/real= 0.0008233276999413243\n",
      "epoch= 903 iteration= 83980 loss= 0.001425259979441762\n",
      "epoch= 903 iteration= 84000 loss= 0.0007059986237436533\n",
      "epoch= 903 iteration= 84020 loss= 0.0003825401945505291\n",
      "epoch= 903 iteration= 84040 loss= 0.0003860627766698599\n",
      "epoch= 903 iteration= 84060 loss= 0.002792929532006383\n",
      "test_data MSELoss:(pred-real)/real= 0.0008233462989058656\n",
      "epoch= 904 iteration= 84073 loss= 0.0015295592602342367\n",
      "epoch= 904 iteration= 84093 loss= 0.0007170509197749197\n",
      "epoch= 904 iteration= 84113 loss= 0.0003785638837143779\n",
      "epoch= 904 iteration= 84133 loss= 0.0003850973444059491\n",
      "epoch= 904 iteration= 84153 loss= 0.0027656196616590023\n",
      "test_data MSELoss:(pred-real)/real= 0.0008238929416014192\n",
      "epoch= 905 iteration= 84166 loss= 0.0016468162648379803\n",
      "epoch= 905 iteration= 84186 loss= 0.0007294595707207918\n",
      "epoch= 905 iteration= 84206 loss= 0.000374416122213006\n",
      "epoch= 905 iteration= 84226 loss= 0.00038343059713952243\n",
      "epoch= 905 iteration= 84246 loss= 0.002744543831795454\n",
      "test_data MSELoss:(pred-real)/real= 0.0008229158896331986\n",
      "epoch= 906 iteration= 84259 loss= 0.0017545989248901606\n",
      "epoch= 906 iteration= 84279 loss= 0.000740136718377471\n",
      "epoch= 906 iteration= 84299 loss= 0.000371163128875196\n",
      "epoch= 906 iteration= 84319 loss= 0.0003819166449829936\n",
      "epoch= 906 iteration= 84339 loss= 0.0027246158570051193\n",
      "test_data MSELoss:(pred-real)/real= 0.0008230238494838381\n",
      "epoch= 907 iteration= 84352 loss= 0.0018722142558544874\n",
      "epoch= 907 iteration= 84372 loss= 0.0007483065710403025\n",
      "epoch= 907 iteration= 84392 loss= 0.0003672631864901632\n",
      "epoch= 907 iteration= 84412 loss= 0.0003798293473664671\n",
      "epoch= 907 iteration= 84432 loss= 0.0027009856421500444\n",
      "test_data MSELoss:(pred-real)/real= 0.0008224055644758563\n",
      "epoch= 908 iteration= 84445 loss= 0.001965540461242199\n",
      "epoch= 908 iteration= 84465 loss= 0.0007514165481552482\n",
      "epoch= 908 iteration= 84485 loss= 0.0003647100820671767\n",
      "epoch= 908 iteration= 84505 loss= 0.00037748500471934676\n",
      "epoch= 908 iteration= 84525 loss= 0.0026767197996377945\n",
      "test_data MSELoss:(pred-real)/real= 0.0008212982591228663\n",
      "epoch= 909 iteration= 84538 loss= 0.002063704188913107\n",
      "epoch= 909 iteration= 84558 loss= 0.0007529861177317798\n",
      "epoch= 909 iteration= 84578 loss= 0.00036274606827646494\n",
      "epoch= 909 iteration= 84598 loss= 0.00037506475928239524\n",
      "epoch= 909 iteration= 84618 loss= 0.002654113806784153\n",
      "test_data MSELoss:(pred-real)/real= 0.0008198962394898343\n",
      "epoch= 910 iteration= 84631 loss= 0.0021564532071352005\n",
      "epoch= 910 iteration= 84651 loss= 0.00075226288754493\n",
      "epoch= 910 iteration= 84671 loss= 0.00036159652518108487\n",
      "epoch= 910 iteration= 84691 loss= 0.00037235667696222663\n",
      "epoch= 910 iteration= 84711 loss= 0.0026319599710404873\n",
      "test_data MSELoss:(pred-real)/real= 0.000819582731411275\n",
      "epoch= 911 iteration= 84724 loss= 0.0022173291072249413\n",
      "epoch= 911 iteration= 84744 loss= 0.0007464909576810896\n",
      "epoch= 911 iteration= 84764 loss= 0.000361625017831102\n",
      "epoch= 911 iteration= 84784 loss= 0.0003695289196912199\n",
      "epoch= 911 iteration= 84804 loss= 0.002612047828733921\n",
      "test_data MSELoss:(pred-real)/real= 0.0008184123972670124\n",
      "epoch= 912 iteration= 84817 loss= 0.002287772251293063\n",
      "epoch= 912 iteration= 84837 loss= 0.0007400284521281719\n",
      "epoch= 912 iteration= 84857 loss= 0.00036150298546999693\n",
      "epoch= 912 iteration= 84877 loss= 0.0003680889494717121\n",
      "epoch= 912 iteration= 84897 loss= 0.0025898804888129234\n",
      "test_data MSELoss:(pred-real)/real= 0.0008175403882988677\n",
      "epoch= 913 iteration= 84910 loss= 0.002347825560718775\n",
      "epoch= 913 iteration= 84930 loss= 0.0007288415217772126\n",
      "epoch= 913 iteration= 84950 loss= 0.00036202307092025876\n",
      "epoch= 913 iteration= 84970 loss= 0.00036496500251814723\n",
      "epoch= 913 iteration= 84990 loss= 0.0025690041948109865\n",
      "test_data MSELoss:(pred-real)/real= 0.000815482014634957\n",
      "epoch= 914 iteration= 85003 loss= 0.0023958426900207996\n",
      "epoch= 914 iteration= 85023 loss= 0.000717810878995806\n",
      "epoch= 914 iteration= 85043 loss= 0.00036334508331492543\n",
      "epoch= 914 iteration= 85063 loss= 0.00036212726263329387\n",
      "epoch= 914 iteration= 85083 loss= 0.0025416784919798374\n",
      "test_data MSELoss:(pred-real)/real= 0.0008139039232951796\n",
      "epoch= 915 iteration= 85096 loss= 0.002429862506687641\n",
      "epoch= 915 iteration= 85116 loss= 0.0007043079822324216\n",
      "epoch= 915 iteration= 85136 loss= 0.00036577892024070024\n",
      "epoch= 915 iteration= 85156 loss= 0.0003607295220717788\n",
      "epoch= 915 iteration= 85176 loss= 0.002526438795030117\n",
      "test_data MSELoss:(pred-real)/real= 0.0008121706818605566\n",
      "epoch= 916 iteration= 85189 loss= 0.0024729794822633266\n",
      "epoch= 916 iteration= 85209 loss= 0.0006935179699212313\n",
      "epoch= 916 iteration= 85229 loss= 0.00036754494067281485\n",
      "epoch= 916 iteration= 85249 loss= 0.0003571806591935456\n",
      "epoch= 916 iteration= 85269 loss= 0.0025014649145305157\n",
      "test_data MSELoss:(pred-real)/real= 0.0008100746357134389\n",
      "epoch= 917 iteration= 85282 loss= 0.002479487331584096\n",
      "epoch= 917 iteration= 85302 loss= 0.0006783021963201463\n",
      "epoch= 917 iteration= 85322 loss= 0.0003702187677845359\n",
      "epoch= 917 iteration= 85342 loss= 0.00035553568159230053\n",
      "epoch= 917 iteration= 85362 loss= 0.0024827029556035995\n",
      "test_data MSELoss:(pred-real)/real= 0.0008075123381180068\n",
      "epoch= 918 iteration= 85375 loss= 0.0025060665793716908\n",
      "epoch= 918 iteration= 85395 loss= 0.0006670313887298107\n",
      "epoch= 918 iteration= 85415 loss= 0.0003727186704054475\n",
      "epoch= 918 iteration= 85435 loss= 0.00035279698204249144\n",
      "epoch= 918 iteration= 85455 loss= 0.0024572249967604876\n",
      "test_data MSELoss:(pred-real)/real= 0.0008056327561563295\n",
      "epoch= 919 iteration= 85468 loss= 0.0025204478297382593\n",
      "epoch= 919 iteration= 85488 loss= 0.0006532261613756418\n",
      "epoch= 919 iteration= 85508 loss= 0.0003747069276869297\n",
      "epoch= 919 iteration= 85528 loss= 0.00035098515218123794\n",
      "epoch= 919 iteration= 85548 loss= 0.002437898423522711\n",
      "test_data MSELoss:(pred-real)/real= 0.0008037993098292241\n",
      "epoch= 920 iteration= 85561 loss= 0.0025439108721911907\n",
      "epoch= 920 iteration= 85581 loss= 0.0006401911959983408\n",
      "epoch= 920 iteration= 85601 loss= 0.0003779403050430119\n",
      "epoch= 920 iteration= 85621 loss= 0.0003490815870463848\n",
      "epoch= 920 iteration= 85641 loss= 0.00241360766813159\n",
      "test_data MSELoss:(pred-real)/real= 0.0008015397235996918\n",
      "epoch= 921 iteration= 85654 loss= 0.0025577424094080925\n",
      "epoch= 921 iteration= 85674 loss= 0.0006290830206125975\n",
      "epoch= 921 iteration= 85694 loss= 0.0003802370047196746\n",
      "epoch= 921 iteration= 85714 loss= 0.00034736708039417863\n",
      "epoch= 921 iteration= 85734 loss= 0.0023925770074129105\n",
      "test_data MSELoss:(pred-real)/real= 0.0007995015471856783\n",
      "epoch= 922 iteration= 85747 loss= 0.0025535323657095432\n",
      "epoch= 922 iteration= 85767 loss= 0.0006163588259369135\n",
      "epoch= 922 iteration= 85787 loss= 0.0003836395335383713\n",
      "epoch= 922 iteration= 85807 loss= 0.00034539890475571156\n",
      "epoch= 922 iteration= 85827 loss= 0.002369432710111141\n",
      "test_data MSELoss:(pred-real)/real= 0.0007975548861496565\n",
      "epoch= 923 iteration= 85840 loss= 0.0025492345448583364\n",
      "epoch= 923 iteration= 85860 loss= 0.0006050617666915059\n",
      "epoch= 923 iteration= 85880 loss= 0.0003860497963614762\n",
      "epoch= 923 iteration= 85900 loss= 0.00034373061498627067\n",
      "epoch= 923 iteration= 85920 loss= 0.0023484304547309875\n",
      "test_data MSELoss:(pred-real)/real= 0.0007961438435207432\n",
      "epoch= 924 iteration= 85933 loss= 0.002534565981477499\n",
      "epoch= 924 iteration= 85953 loss= 0.0005943123251199722\n",
      "epoch= 924 iteration= 85973 loss= 0.00038737928844057024\n",
      "epoch= 924 iteration= 85993 loss= 0.0003418989363126457\n",
      "epoch= 924 iteration= 86013 loss= 0.0023261327296495438\n",
      "test_data MSELoss:(pred-real)/real= 0.0007940400448407874\n",
      "epoch= 925 iteration= 86026 loss= 0.0025037731975317\n",
      "epoch= 925 iteration= 86046 loss= 0.0005845769192092121\n",
      "epoch= 925 iteration= 86066 loss= 0.00038979208329692483\n",
      "epoch= 925 iteration= 86086 loss= 0.0003402912989258766\n",
      "epoch= 925 iteration= 86106 loss= 0.002305487636476755\n",
      "test_data MSELoss:(pred-real)/real= 0.0007929658741987725\n",
      "epoch= 926 iteration= 86119 loss= 0.0024979293812066317\n",
      "epoch= 926 iteration= 86139 loss= 0.0005772448494099081\n",
      "epoch= 926 iteration= 86159 loss= 0.0003919409937225282\n",
      "epoch= 926 iteration= 86179 loss= 0.000338110257871449\n",
      "epoch= 926 iteration= 86199 loss= 0.002285649534314871\n",
      "test_data MSELoss:(pred-real)/real= 0.0007911168601519117\n",
      "epoch= 927 iteration= 86212 loss= 0.002474660985171795\n",
      "epoch= 927 iteration= 86232 loss= 0.00057047582231462\n",
      "epoch= 927 iteration= 86252 loss= 0.00039287604158744216\n",
      "epoch= 927 iteration= 86272 loss= 0.00033651775447651744\n",
      "epoch= 927 iteration= 86292 loss= 0.002267809584736824\n",
      "test_data MSELoss:(pred-real)/real= 0.0007899945711768749\n",
      "epoch= 928 iteration= 86305 loss= 0.002440568059682846\n",
      "epoch= 928 iteration= 86325 loss= 0.0005636413698084652\n",
      "epoch= 928 iteration= 86345 loss= 0.0003939208691008389\n",
      "epoch= 928 iteration= 86365 loss= 0.00033493374940007925\n",
      "epoch= 928 iteration= 86385 loss= 0.0022460694890469313\n",
      "test_data MSELoss:(pred-real)/real= 0.0007879842968476522\n",
      "epoch= 929 iteration= 86398 loss= 0.00240358617156744\n",
      "epoch= 929 iteration= 86418 loss= 0.0005569053464569151\n",
      "epoch= 929 iteration= 86438 loss= 0.0003946636861655861\n",
      "epoch= 929 iteration= 86458 loss= 0.00033317826455459\n",
      "epoch= 929 iteration= 86478 loss= 0.0022236204240471125\n",
      "test_data MSELoss:(pred-real)/real= 0.0007867233143770136\n",
      "epoch= 930 iteration= 86491 loss= 0.002390007022768259\n",
      "epoch= 930 iteration= 86511 loss= 0.0005529032787308097\n",
      "epoch= 930 iteration= 86531 loss= 0.00039578007999807596\n",
      "epoch= 930 iteration= 86551 loss= 0.0003311554028186947\n",
      "epoch= 930 iteration= 86571 loss= 0.0022009594831615686\n",
      "test_data MSELoss:(pred-real)/real= 0.0007849181125089268\n",
      "epoch= 931 iteration= 86584 loss= 0.002351819071918726\n",
      "epoch= 931 iteration= 86604 loss= 0.0005480929394252598\n",
      "epoch= 931 iteration= 86624 loss= 0.0003955939901061356\n",
      "epoch= 931 iteration= 86644 loss= 0.0003304000128991902\n",
      "epoch= 931 iteration= 86664 loss= 0.0021826201118528843\n",
      "test_data MSELoss:(pred-real)/real= 0.0007843698011937602\n",
      "epoch= 932 iteration= 86677 loss= 0.0023285020142793655\n",
      "epoch= 932 iteration= 86697 loss= 0.0005440721870400012\n",
      "epoch= 932 iteration= 86717 loss= 0.00039545513573102653\n",
      "epoch= 932 iteration= 86737 loss= 0.0003285402781330049\n",
      "epoch= 932 iteration= 86757 loss= 0.002161242999136448\n",
      "test_data MSELoss:(pred-real)/real= 0.0007825384078993617\n",
      "epoch= 933 iteration= 86770 loss= 0.002282062778249383\n",
      "epoch= 933 iteration= 86790 loss= 0.0005401839734986424\n",
      "epoch= 933 iteration= 86810 loss= 0.0003954423009417951\n",
      "epoch= 933 iteration= 86830 loss= 0.0003264984698034823\n",
      "epoch= 933 iteration= 86850 loss= 0.002143851248547435\n",
      "test_data MSELoss:(pred-real)/real= 0.0007809130608317597\n",
      "epoch= 934 iteration= 86863 loss= 0.0022510283160954714\n",
      "epoch= 934 iteration= 86883 loss= 0.00053704425226897\n",
      "epoch= 934 iteration= 86903 loss= 0.0003953282139264047\n",
      "epoch= 934 iteration= 86923 loss= 0.0003254380135331303\n",
      "epoch= 934 iteration= 86943 loss= 0.002124941209331155\n",
      "test_data MSELoss:(pred-real)/real= 0.000780283522342668\n",
      "epoch= 935 iteration= 86956 loss= 0.0022280393168330193\n",
      "epoch= 935 iteration= 86976 loss= 0.0005354912718757987\n",
      "epoch= 935 iteration= 86996 loss= 0.00039469345938414335\n",
      "epoch= 935 iteration= 87016 loss= 0.0003235271433368325\n",
      "epoch= 935 iteration= 87036 loss= 0.0021026087924838066\n",
      "test_data MSELoss:(pred-real)/real= 0.00077845374390664\n",
      "epoch= 936 iteration= 87049 loss= 0.0021775614004582167\n",
      "epoch= 936 iteration= 87069 loss= 0.0005335877649486065\n",
      "epoch= 936 iteration= 87089 loss= 0.00039361300878226757\n",
      "epoch= 936 iteration= 87109 loss= 0.00032156752422451973\n",
      "epoch= 936 iteration= 87129 loss= 0.0020865038968622684\n",
      "test_data MSELoss:(pred-real)/real= 0.0007778836237169647\n",
      "epoch= 937 iteration= 87142 loss= 0.002146342536434531\n",
      "epoch= 937 iteration= 87162 loss= 0.0005316901952028275\n",
      "epoch= 937 iteration= 87182 loss= 0.00039301131619140506\n",
      "epoch= 937 iteration= 87202 loss= 0.0003202296793460846\n",
      "epoch= 937 iteration= 87222 loss= 0.0020697452127933502\n",
      "test_data MSELoss:(pred-real)/real= 0.0007764809003371435\n",
      "epoch= 938 iteration= 87235 loss= 0.0021202408242970705\n",
      "epoch= 938 iteration= 87255 loss= 0.0005303691141307354\n",
      "epoch= 938 iteration= 87275 loss= 0.0003915170091204345\n",
      "epoch= 938 iteration= 87295 loss= 0.0003178411861881614\n",
      "epoch= 938 iteration= 87315 loss= 0.002045956440269947\n",
      "test_data MSELoss:(pred-real)/real= 0.0007756777243533483\n",
      "epoch= 939 iteration= 87328 loss= 0.002087665256112814\n",
      "epoch= 939 iteration= 87348 loss= 0.0005292799323797226\n",
      "epoch= 939 iteration= 87368 loss= 0.0003905668272636831\n",
      "epoch= 939 iteration= 87388 loss= 0.00031609111465513706\n",
      "epoch= 939 iteration= 87408 loss= 0.0020275081042200327\n",
      "test_data MSELoss:(pred-real)/real= 0.000773508185375249\n",
      "epoch= 940 iteration= 87421 loss= 0.002069938462227583\n",
      "epoch= 940 iteration= 87441 loss= 0.0005286351079121232\n",
      "epoch= 940 iteration= 87461 loss= 0.0003890942898578942\n",
      "epoch= 940 iteration= 87481 loss= 0.0003155767044518143\n",
      "epoch= 940 iteration= 87501 loss= 0.0020085035357624292\n",
      "test_data MSELoss:(pred-real)/real= 0.0007725183333807056\n",
      "epoch= 941 iteration= 87514 loss= 0.002055908553302288\n",
      "epoch= 941 iteration= 87534 loss= 0.0005293585709296167\n",
      "epoch= 941 iteration= 87554 loss= 0.00038797943852841854\n",
      "epoch= 941 iteration= 87574 loss= 0.0003134339349344373\n",
      "epoch= 941 iteration= 87594 loss= 0.0019883199129253626\n",
      "test_data MSELoss:(pred-real)/real= 0.0007719850343871965\n",
      "epoch= 942 iteration= 87607 loss= 0.00202110200189054\n",
      "epoch= 942 iteration= 87627 loss= 0.000528343953192234\n",
      "epoch= 942 iteration= 87647 loss= 0.00038586926530115306\n",
      "epoch= 942 iteration= 87667 loss= 0.0003120938199572265\n",
      "epoch= 942 iteration= 87687 loss= 0.001968755153939128\n",
      "test_data MSELoss:(pred-real)/real= 0.0007704515891317795\n",
      "epoch= 943 iteration= 87700 loss= 0.002003305358812213\n",
      "epoch= 943 iteration= 87720 loss= 0.00052780652185902\n",
      "epoch= 943 iteration= 87740 loss= 0.0003850325010716915\n",
      "epoch= 943 iteration= 87760 loss= 0.0003108470991719514\n",
      "epoch= 943 iteration= 87780 loss= 0.0019497459288686514\n",
      "test_data MSELoss:(pred-real)/real= 0.0007691404559106255\n",
      "epoch= 944 iteration= 87793 loss= 0.00197782414034009\n",
      "epoch= 944 iteration= 87813 loss= 0.000528180506080389\n",
      "epoch= 944 iteration= 87833 loss= 0.00038374424912035465\n",
      "epoch= 944 iteration= 87853 loss= 0.0003101066395174712\n",
      "epoch= 944 iteration= 87873 loss= 0.0019355397671461105\n",
      "test_data MSELoss:(pred-real)/real= 0.0007683224620349291\n",
      "epoch= 945 iteration= 87886 loss= 0.001951433951035142\n",
      "epoch= 945 iteration= 87906 loss= 0.0005278714816085994\n",
      "epoch= 945 iteration= 87926 loss= 0.0003815086674876511\n",
      "epoch= 945 iteration= 87946 loss= 0.00030784355476498604\n",
      "epoch= 945 iteration= 87966 loss= 0.0019162071403115988\n",
      "test_data MSELoss:(pred-real)/real= 0.0007674138105357997\n",
      "epoch= 946 iteration= 87979 loss= 0.0019221275579184294\n",
      "epoch= 946 iteration= 87999 loss= 0.0005278963362798095\n",
      "epoch= 946 iteration= 88019 loss= 0.00037950955447740853\n",
      "epoch= 946 iteration= 88039 loss= 0.00030679471092298627\n",
      "epoch= 946 iteration= 88059 loss= 0.0018977767322212458\n",
      "test_data MSELoss:(pred-real)/real= 0.0007667875959466781\n",
      "epoch= 947 iteration= 88072 loss= 0.0018981442553922534\n",
      "epoch= 947 iteration= 88092 loss= 0.0005280920304358006\n",
      "epoch= 947 iteration= 88112 loss= 0.0003784906002692878\n",
      "epoch= 947 iteration= 88132 loss= 0.0003051045350730419\n",
      "epoch= 947 iteration= 88152 loss= 0.0018784168642014265\n",
      "test_data MSELoss:(pred-real)/real= 0.0007657106462930744\n",
      "epoch= 948 iteration= 88165 loss= 0.0018677765037864447\n",
      "epoch= 948 iteration= 88185 loss= 0.0005284427898004651\n",
      "epoch= 948 iteration= 88205 loss= 0.00037649471778422594\n",
      "epoch= 948 iteration= 88225 loss= 0.00030339858494699\n",
      "epoch= 948 iteration= 88245 loss= 0.0018642509821802378\n",
      "test_data MSELoss:(pred-real)/real= 0.0007641944624386573\n",
      "epoch= 949 iteration= 88258 loss= 0.0018539691809564829\n",
      "epoch= 949 iteration= 88278 loss= 0.0005284646176733077\n",
      "epoch= 949 iteration= 88298 loss= 0.00037482462357729673\n",
      "epoch= 949 iteration= 88318 loss= 0.0003024464822374284\n",
      "epoch= 949 iteration= 88338 loss= 0.001846356550231576\n",
      "test_data MSELoss:(pred-real)/real= 0.0007633674047408729\n",
      "epoch= 950 iteration= 88351 loss= 0.0018263463862240314\n",
      "epoch= 950 iteration= 88371 loss= 0.0005277997115626931\n",
      "epoch= 950 iteration= 88391 loss= 0.00037315956433303654\n",
      "epoch= 950 iteration= 88411 loss= 0.00030134880216792226\n",
      "epoch= 950 iteration= 88431 loss= 0.0018293344182893634\n",
      "test_data MSELoss:(pred-real)/real= 0.0007618362364559693\n",
      "epoch= 951 iteration= 88444 loss= 0.0018118377774953842\n",
      "epoch= 951 iteration= 88464 loss= 0.0005285050137899816\n",
      "epoch= 951 iteration= 88484 loss= 0.0003712266334332526\n",
      "epoch= 951 iteration= 88504 loss= 0.0003001124714501202\n",
      "epoch= 951 iteration= 88524 loss= 0.0018104136688634753\n",
      "test_data MSELoss:(pred-real)/real= 0.0007610253681680964\n",
      "epoch= 952 iteration= 88537 loss= 0.0018004016019403934\n",
      "epoch= 952 iteration= 88557 loss= 0.0005297604948282242\n",
      "epoch= 952 iteration= 88577 loss= 0.0003691467281896621\n",
      "epoch= 952 iteration= 88597 loss= 0.00029865981196053326\n",
      "epoch= 952 iteration= 88617 loss= 0.0017918716184794903\n",
      "test_data MSELoss:(pred-real)/real= 0.0007602243883108409\n",
      "epoch= 953 iteration= 88630 loss= 0.0017625638283789158\n",
      "epoch= 953 iteration= 88650 loss= 0.0005301140481606126\n",
      "epoch= 953 iteration= 88670 loss= 0.0003680297522805631\n",
      "epoch= 953 iteration= 88690 loss= 0.0002975441748276353\n",
      "epoch= 953 iteration= 88710 loss= 0.001778834848664701\n",
      "test_data MSELoss:(pred-real)/real= 0.0007597800678114356\n",
      "epoch= 954 iteration= 88723 loss= 0.001733978744596243\n",
      "epoch= 954 iteration= 88743 loss= 0.0005315319867804646\n",
      "epoch= 954 iteration= 88763 loss= 0.00036620660102926195\n",
      "epoch= 954 iteration= 88783 loss= 0.0002968406770378351\n",
      "epoch= 954 iteration= 88803 loss= 0.0017638244898989797\n",
      "test_data MSELoss:(pred-real)/real= 0.0007587587016233657\n",
      "epoch= 955 iteration= 88816 loss= 0.0017076260410249233\n",
      "epoch= 955 iteration= 88836 loss= 0.0005320548661984503\n",
      "epoch= 955 iteration= 88856 loss= 0.00036485237069427967\n",
      "epoch= 955 iteration= 88876 loss= 0.0002955451491288841\n",
      "epoch= 955 iteration= 88896 loss= 0.0017468426376581192\n",
      "test_data MSELoss:(pred-real)/real= 0.0007585443519120519\n",
      "epoch= 956 iteration= 88909 loss= 0.0016810252564027905\n",
      "epoch= 956 iteration= 88929 loss= 0.0005334309535101056\n",
      "epoch= 956 iteration= 88949 loss= 0.00036229105899110436\n",
      "epoch= 956 iteration= 88969 loss= 0.0002939634141512215\n",
      "epoch= 956 iteration= 88989 loss= 0.0017263066256418824\n",
      "test_data MSELoss:(pred-real)/real= 0.000757407848545376\n",
      "epoch= 957 iteration= 89002 loss= 0.0016507237451151013\n",
      "epoch= 957 iteration= 89022 loss= 0.0005346281104721129\n",
      "epoch= 957 iteration= 89042 loss= 0.00036034698132425547\n",
      "epoch= 957 iteration= 89062 loss= 0.00029314716812223196\n",
      "epoch= 957 iteration= 89082 loss= 0.0017131324857473373\n",
      "test_data MSELoss:(pred-real)/real= 0.0007567572857725383\n",
      "epoch= 958 iteration= 89095 loss= 0.0016227161977440119\n",
      "epoch= 958 iteration= 89115 loss= 0.0005350590217858553\n",
      "epoch= 958 iteration= 89135 loss= 0.00035850098356604576\n",
      "epoch= 958 iteration= 89155 loss= 0.0002912433410529047\n",
      "epoch= 958 iteration= 89175 loss= 0.0016989357536658645\n",
      "test_data MSELoss:(pred-real)/real= 0.0007569361364908724\n",
      "epoch= 959 iteration= 89188 loss= 0.0015929547371342778\n",
      "epoch= 959 iteration= 89208 loss= 0.0005363378440961242\n",
      "epoch= 959 iteration= 89228 loss= 0.00035665452014654875\n",
      "epoch= 959 iteration= 89248 loss= 0.000290107709588483\n",
      "epoch= 959 iteration= 89268 loss= 0.001684557180851698\n",
      "test_data MSELoss:(pred-real)/real= 0.0007565357110353135\n",
      "epoch= 960 iteration= 89281 loss= 0.0015603485517203808\n",
      "epoch= 960 iteration= 89301 loss= 0.0005368563579395413\n",
      "epoch= 960 iteration= 89321 loss= 0.0003551942645572126\n",
      "epoch= 960 iteration= 89341 loss= 0.0002892295888159424\n",
      "epoch= 960 iteration= 89361 loss= 0.0016674501821398735\n",
      "test_data MSELoss:(pred-real)/real= 0.0007557811764450485\n",
      "epoch= 961 iteration= 89374 loss= 0.0015436295652762055\n",
      "epoch= 961 iteration= 89394 loss= 0.0005378068890422583\n",
      "epoch= 961 iteration= 89414 loss= 0.0003534650313667953\n",
      "epoch= 961 iteration= 89434 loss= 0.0002877126098610461\n",
      "epoch= 961 iteration= 89454 loss= 0.0016526171239092946\n",
      "test_data MSELoss:(pred-real)/real= 0.0007551372715776475\n",
      "epoch= 962 iteration= 89467 loss= 0.0015302607789635658\n",
      "epoch= 962 iteration= 89487 loss= 0.0005395750631578267\n",
      "epoch= 962 iteration= 89507 loss= 0.0003511108225211501\n",
      "epoch= 962 iteration= 89527 loss= 0.00028643064433708787\n",
      "epoch= 962 iteration= 89547 loss= 0.001637946581467986\n",
      "test_data MSELoss:(pred-real)/real= 0.0007549254991722086\n",
      "epoch= 963 iteration= 89560 loss= 0.001508674817159772\n",
      "epoch= 963 iteration= 89580 loss= 0.0005400035879574716\n",
      "epoch= 963 iteration= 89600 loss= 0.00034891304676420987\n",
      "epoch= 963 iteration= 89620 loss= 0.00028548104455694556\n",
      "epoch= 963 iteration= 89640 loss= 0.0016225685831159353\n",
      "test_data MSELoss:(pred-real)/real= 0.0007542562901411051\n",
      "epoch= 964 iteration= 89653 loss= 0.0014858057256788015\n",
      "epoch= 964 iteration= 89673 loss= 0.0005404572002589703\n",
      "epoch= 964 iteration= 89693 loss= 0.00034780133864842355\n",
      "epoch= 964 iteration= 89713 loss= 0.0002849574957508594\n",
      "epoch= 964 iteration= 89733 loss= 0.001610815292224288\n",
      "test_data MSELoss:(pred-real)/real= 0.0007541427484183158\n",
      "epoch= 965 iteration= 89746 loss= 0.0014717939775437117\n",
      "epoch= 965 iteration= 89766 loss= 0.0005425709532573819\n",
      "epoch= 965 iteration= 89786 loss= 0.0003454681427683681\n",
      "epoch= 965 iteration= 89806 loss= 0.00028335920069366693\n",
      "epoch= 965 iteration= 89826 loss= 0.0015959611628204584\n",
      "test_data MSELoss:(pred-real)/real= 0.000753924845614367\n",
      "epoch= 966 iteration= 89839 loss= 0.0014573769876733422\n",
      "epoch= 966 iteration= 89859 loss= 0.0005428962758742273\n",
      "epoch= 966 iteration= 89879 loss= 0.00034329318441450596\n",
      "epoch= 966 iteration= 89899 loss= 0.0002823040122166276\n",
      "epoch= 966 iteration= 89919 loss= 0.0015836091479286551\n",
      "test_data MSELoss:(pred-real)/real= 0.0007524719391464411\n",
      "epoch= 967 iteration= 89932 loss= 0.0014479460660368204\n",
      "epoch= 967 iteration= 89952 loss= 0.000544349430128932\n",
      "epoch= 967 iteration= 89972 loss= 0.0003414080711081624\n",
      "epoch= 967 iteration= 89992 loss= 0.00028209388256073\n",
      "epoch= 967 iteration= 90012 loss= 0.0015690462896600366\n",
      "test_data MSELoss:(pred-real)/real= 0.0007535269057067732\n",
      "epoch= 968 iteration= 90025 loss= 0.0014336735475808382\n",
      "epoch= 968 iteration= 90045 loss= 0.0005449543241411448\n",
      "epoch= 968 iteration= 90065 loss= 0.00033991102827712893\n",
      "epoch= 968 iteration= 90085 loss= 0.0002799955545924604\n",
      "epoch= 968 iteration= 90105 loss= 0.0015553950797766447\n",
      "test_data MSELoss:(pred-real)/real= 0.0007520827518116372\n",
      "epoch= 969 iteration= 90118 loss= 0.0014241726603358984\n",
      "epoch= 969 iteration= 90138 loss= 0.0005457389634102583\n",
      "epoch= 969 iteration= 90158 loss= 0.0003380299895070493\n",
      "epoch= 969 iteration= 90178 loss= 0.00027912663063034415\n",
      "epoch= 969 iteration= 90198 loss= 0.0015401903074234724\n",
      "test_data MSELoss:(pred-real)/real= 0.0007518842107351196\n",
      "epoch= 970 iteration= 90211 loss= 0.0014163873856887221\n",
      "epoch= 970 iteration= 90231 loss= 0.0005453784833662212\n",
      "epoch= 970 iteration= 90251 loss= 0.0003367051249369979\n",
      "epoch= 970 iteration= 90271 loss= 0.00027797179063782096\n",
      "epoch= 970 iteration= 90291 loss= 0.0015264467801898718\n",
      "test_data MSELoss:(pred-real)/real= 0.0007507423086078941\n",
      "epoch= 971 iteration= 90304 loss= 0.0014075313229113817\n",
      "epoch= 971 iteration= 90324 loss= 0.0005455248756334186\n",
      "epoch= 971 iteration= 90344 loss= 0.00033491477370262146\n",
      "epoch= 971 iteration= 90364 loss= 0.0002776698092930019\n",
      "epoch= 971 iteration= 90384 loss= 0.0015139346942305565\n",
      "test_data MSELoss:(pred-real)/real= 0.000750606927694106\n",
      "epoch= 972 iteration= 90397 loss= 0.0014045158168300986\n",
      "epoch= 972 iteration= 90417 loss= 0.0005461690598167479\n",
      "epoch= 972 iteration= 90437 loss= 0.00033300131326541305\n",
      "epoch= 972 iteration= 90457 loss= 0.000275965197943151\n",
      "epoch= 972 iteration= 90477 loss= 0.0015021099243313074\n",
      "test_data MSELoss:(pred-real)/real= 0.0007501114309383815\n",
      "epoch= 973 iteration= 90490 loss= 0.0013896615710109472\n",
      "epoch= 973 iteration= 90510 loss= 0.0005478733219206333\n",
      "epoch= 973 iteration= 90530 loss= 0.00033141678432002664\n",
      "epoch= 973 iteration= 90550 loss= 0.00027510710060596466\n",
      "epoch= 973 iteration= 90570 loss= 0.0014896648935973644\n",
      "test_data MSELoss:(pred-real)/real= 0.0007500396245126871\n",
      "epoch= 974 iteration= 90583 loss= 0.0013752719387412071\n",
      "epoch= 974 iteration= 90603 loss= 0.0005480445106513798\n",
      "epoch= 974 iteration= 90623 loss= 0.0003298908704891801\n",
      "epoch= 974 iteration= 90643 loss= 0.0002741499920375645\n",
      "epoch= 974 iteration= 90663 loss= 0.001474976073950529\n",
      "test_data MSELoss:(pred-real)/real= 0.000748624616891094\n",
      "epoch= 975 iteration= 90676 loss= 0.0013684118166565895\n",
      "epoch= 975 iteration= 90696 loss= 0.0005488520255312324\n",
      "epoch= 975 iteration= 90716 loss= 0.0003281201934441924\n",
      "epoch= 975 iteration= 90736 loss= 0.0002729085390456021\n",
      "epoch= 975 iteration= 90756 loss= 0.0014649253571406007\n",
      "test_data MSELoss:(pred-real)/real= 0.0007489074968260764\n",
      "epoch= 976 iteration= 90769 loss= 0.0013535907492041588\n",
      "epoch= 976 iteration= 90789 loss= 0.0005485399160534143\n",
      "epoch= 976 iteration= 90809 loss= 0.0003271594177931547\n",
      "epoch= 976 iteration= 90829 loss= 0.0002723696525208652\n",
      "epoch= 976 iteration= 90849 loss= 0.0014506198931485415\n",
      "test_data MSELoss:(pred-real)/real= 0.0007484044678373417\n",
      "epoch= 977 iteration= 90862 loss= 0.0013348613865673542\n",
      "epoch= 977 iteration= 90882 loss= 0.0005480493418872356\n",
      "epoch= 977 iteration= 90902 loss= 0.000325231027090922\n",
      "epoch= 977 iteration= 90922 loss= 0.00027101446175947785\n",
      "epoch= 977 iteration= 90942 loss= 0.0014382791705429554\n",
      "test_data MSELoss:(pred-real)/real= 0.0007483136125503936\n",
      "epoch= 978 iteration= 90955 loss= 0.0013250678312033415\n",
      "epoch= 978 iteration= 90975 loss= 0.0005488285678438842\n",
      "epoch= 978 iteration= 90995 loss= 0.00032388060935772955\n",
      "epoch= 978 iteration= 91015 loss= 0.000270116695901379\n",
      "epoch= 978 iteration= 91035 loss= 0.0014248955994844437\n",
      "test_data MSELoss:(pred-real)/real= 0.0007477641576972221\n",
      "epoch= 979 iteration= 91048 loss= 0.0013130962615832686\n",
      "epoch= 979 iteration= 91068 loss= 0.0005479978863149881\n",
      "epoch= 979 iteration= 91088 loss= 0.00032260449370369315\n",
      "epoch= 979 iteration= 91108 loss= 0.00026906351558864117\n",
      "epoch= 979 iteration= 91128 loss= 0.0014136310201138258\n",
      "test_data MSELoss:(pred-real)/real= 0.0007476858202911293\n",
      "epoch= 980 iteration= 91141 loss= 0.0012942953035235405\n",
      "epoch= 980 iteration= 91161 loss= 0.0005468081217259169\n",
      "epoch= 980 iteration= 91181 loss= 0.00032078821095637977\n",
      "epoch= 980 iteration= 91201 loss= 0.00026770285330712795\n",
      "epoch= 980 iteration= 91221 loss= 0.0014077131636440754\n",
      "test_data MSELoss:(pred-real)/real= 0.0007472824310955022\n",
      "epoch= 981 iteration= 91234 loss= 0.0012849641498178244\n",
      "epoch= 981 iteration= 91254 loss= 0.0005467849550768733\n",
      "epoch= 981 iteration= 91274 loss= 0.00031948601827025414\n",
      "epoch= 981 iteration= 91294 loss= 0.00026679260190576315\n",
      "epoch= 981 iteration= 91314 loss= 0.0013910948764532804\n",
      "test_data MSELoss:(pred-real)/real= 0.0007469830036118057\n",
      "epoch= 982 iteration= 91327 loss= 0.0012712576426565647\n",
      "epoch= 982 iteration= 91347 loss= 0.000547497533261776\n",
      "epoch= 982 iteration= 91367 loss= 0.0003179946215823293\n",
      "epoch= 982 iteration= 91387 loss= 0.000265922280959785\n",
      "epoch= 982 iteration= 91407 loss= 0.0013811951503157616\n",
      "test_data MSELoss:(pred-real)/real= 0.0007469333678374016\n",
      "epoch= 983 iteration= 91420 loss= 0.0012610475532710552\n",
      "epoch= 983 iteration= 91440 loss= 0.0005467928131110966\n",
      "epoch= 983 iteration= 91460 loss= 0.00031604216201230884\n",
      "epoch= 983 iteration= 91480 loss= 0.0002645041677169502\n",
      "epoch= 983 iteration= 91500 loss= 0.001368574914522469\n",
      "test_data MSELoss:(pred-real)/real= 0.0007471942792310276\n",
      "epoch= 984 iteration= 91513 loss= 0.001251920242793858\n",
      "epoch= 984 iteration= 91533 loss= 0.0005460138199850917\n",
      "epoch= 984 iteration= 91553 loss= 0.0003147891547996551\n",
      "epoch= 984 iteration= 91573 loss= 0.0002644235792104155\n",
      "epoch= 984 iteration= 91593 loss= 0.0013594881165772676\n",
      "test_data MSELoss:(pred-real)/real= 0.0007465003650900649\n",
      "epoch= 985 iteration= 91606 loss= 0.0012425095774233341\n",
      "epoch= 985 iteration= 91626 loss= 0.0005454708589240909\n",
      "epoch= 985 iteration= 91646 loss= 0.00031346530886366963\n",
      "epoch= 985 iteration= 91666 loss= 0.0002629971713759005\n",
      "epoch= 985 iteration= 91686 loss= 0.0013465863885357976\n",
      "test_data MSELoss:(pred-real)/real= 0.0007451165919418498\n",
      "epoch= 986 iteration= 91699 loss= 0.001242626691237092\n",
      "epoch= 986 iteration= 91719 loss= 0.000545934890396893\n",
      "epoch= 986 iteration= 91739 loss= 0.0003119566827081144\n",
      "epoch= 986 iteration= 91759 loss= 0.0002623375621624291\n",
      "epoch= 986 iteration= 91779 loss= 0.0013351119123399258\n",
      "test_data MSELoss:(pred-real)/real= 0.0007452674225431918\n",
      "epoch= 987 iteration= 91792 loss= 0.0012429774506017566\n",
      "epoch= 987 iteration= 91812 loss= 0.0005460423417389393\n",
      "epoch= 987 iteration= 91832 loss= 0.00031110289273783565\n",
      "epoch= 987 iteration= 91852 loss= 0.00026113842613995075\n",
      "epoch= 987 iteration= 91872 loss= 0.0013243189314380288\n",
      "test_data MSELoss:(pred-real)/real= 0.0007454472547073641\n",
      "epoch= 988 iteration= 91885 loss= 0.0012414352968335152\n",
      "epoch= 988 iteration= 91905 loss= 0.0005455670761875808\n",
      "epoch= 988 iteration= 91925 loss= 0.00030872231582179666\n",
      "epoch= 988 iteration= 91945 loss= 0.000260871194768697\n",
      "epoch= 988 iteration= 91965 loss= 0.0013163359835743904\n",
      "test_data MSELoss:(pred-real)/real= 0.0007446303287805575\n",
      "epoch= 989 iteration= 91978 loss= 0.0012443020241335034\n",
      "epoch= 989 iteration= 91998 loss= 0.0005453394260257483\n",
      "epoch= 989 iteration= 92018 loss= 0.00030740449437871575\n",
      "epoch= 989 iteration= 92038 loss= 0.000259310967521742\n",
      "epoch= 989 iteration= 92058 loss= 0.0013053887523710728\n",
      "test_data MSELoss:(pred-real)/real= 0.0007443679666418271\n",
      "epoch= 990 iteration= 92071 loss= 0.0012447899207472801\n",
      "epoch= 990 iteration= 92091 loss= 0.0005450070020742714\n",
      "epoch= 990 iteration= 92111 loss= 0.0003052498504985124\n",
      "epoch= 990 iteration= 92131 loss= 0.00025940441992133856\n",
      "epoch= 990 iteration= 92151 loss= 0.0012956117279827595\n",
      "test_data MSELoss:(pred-real)/real= 0.000744029182290736\n",
      "epoch= 991 iteration= 92164 loss= 0.001245723688043654\n",
      "epoch= 991 iteration= 92184 loss= 0.0005446728318929672\n",
      "epoch= 991 iteration= 92204 loss= 0.0003042382886633277\n",
      "epoch= 991 iteration= 92224 loss= 0.0002579250722192228\n",
      "epoch= 991 iteration= 92244 loss= 0.0012841415591537952\n",
      "test_data MSELoss:(pred-real)/real= 0.000743201537665704\n",
      "epoch= 992 iteration= 92257 loss= 0.001243848935700953\n",
      "epoch= 992 iteration= 92277 loss= 0.000544210197404027\n",
      "epoch= 992 iteration= 92297 loss= 0.0003031785599887371\n",
      "epoch= 992 iteration= 92317 loss= 0.000256476690992713\n",
      "epoch= 992 iteration= 92337 loss= 0.0012731965398415923\n",
      "test_data MSELoss:(pred-real)/real= 0.0007429275028698612\n",
      "epoch= 993 iteration= 92350 loss= 0.001244251849129796\n",
      "epoch= 993 iteration= 92370 loss= 0.0005437336512841284\n",
      "epoch= 993 iteration= 92390 loss= 0.00030174601124599576\n",
      "epoch= 993 iteration= 92410 loss= 0.0002553500817157328\n",
      "epoch= 993 iteration= 92430 loss= 0.0012650631833821535\n",
      "test_data MSELoss:(pred-real)/real= 0.0007426791319934031\n",
      "epoch= 994 iteration= 92443 loss= 0.0012404581066220999\n",
      "epoch= 994 iteration= 92463 loss= 0.000542356981895864\n",
      "epoch= 994 iteration= 92483 loss= 0.00030074617825448513\n",
      "epoch= 994 iteration= 92503 loss= 0.0002552238875068724\n",
      "epoch= 994 iteration= 92523 loss= 0.0012561301700770855\n",
      "test_data MSELoss:(pred-real)/real= 0.000742315465079931\n",
      "epoch= 995 iteration= 92536 loss= 0.001233480405062437\n",
      "epoch= 995 iteration= 92556 loss= 0.0005418770015239716\n",
      "epoch= 995 iteration= 92576 loss= 0.00029960955725982785\n",
      "epoch= 995 iteration= 92596 loss= 0.0002544191956985742\n",
      "epoch= 995 iteration= 92616 loss= 0.0012502343161031604\n",
      "test_data MSELoss:(pred-real)/real= 0.0007415807655585619\n",
      "epoch= 996 iteration= 92629 loss= 0.0012259215582162142\n",
      "epoch= 996 iteration= 92649 loss= 0.0005415959749370813\n",
      "epoch= 996 iteration= 92669 loss= 0.00029803276993334293\n",
      "epoch= 996 iteration= 92689 loss= 0.0002531912177801132\n",
      "epoch= 996 iteration= 92709 loss= 0.0012366613373160362\n",
      "test_data MSELoss:(pred-real)/real= 0.000740988817900264\n",
      "epoch= 997 iteration= 92722 loss= 0.001221157144755125\n",
      "epoch= 997 iteration= 92742 loss= 0.0005396059714257717\n",
      "epoch= 997 iteration= 92762 loss= 0.0002969021734315902\n",
      "epoch= 997 iteration= 92782 loss= 0.0002528815530240536\n",
      "epoch= 997 iteration= 92802 loss= 0.0012290466111153364\n",
      "test_data MSELoss:(pred-real)/real= 0.0007410572697052783\n",
      "epoch= 998 iteration= 92815 loss= 0.0012130201794207096\n",
      "epoch= 998 iteration= 92835 loss= 0.0005372918676584959\n",
      "epoch= 998 iteration= 92855 loss= 0.0002958910772576928\n",
      "epoch= 998 iteration= 92875 loss= 0.00025104416999965906\n",
      "epoch= 998 iteration= 92895 loss= 0.0012192139402031898\n",
      "test_data MSELoss:(pred-real)/real= 0.0007402274317832456\n",
      "epoch= 999 iteration= 92908 loss= 0.0012057903222739697\n",
      "epoch= 999 iteration= 92928 loss= 0.0005359752103686333\n",
      "epoch= 999 iteration= 92948 loss= 0.00029448801069520414\n",
      "epoch= 999 iteration= 92968 loss= 0.00025134923635050654\n",
      "epoch= 999 iteration= 92988 loss= 0.0012119763996452093\n",
      "test_data MSELoss:(pred-real)/real= 0.0007398833056666061\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":   \n",
    "    period = 100  \n",
    "    iteration=0\n",
    "    loss_list=[]\n",
    "    #开始训练神经网络\n",
    "    for epoch in range(1000):         \n",
    "        predict_list=[]\n",
    "        accuracy_list=[]\n",
    "        train(epoch)\n",
    "        test()\n",
    "    #绘制损失函数下降曲线    \n",
    "    loss_curve(loss_list)\n",
    "    #绘制测试集pred-real对比曲线\n",
    "    contrast_lines(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
